{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T14:20:17.136815Z",
     "start_time": "2024-08-11T14:20:15.786593Z"
    }
   },
   "source": [
    "import time\n",
    "import json\n",
    "import multiprocessing\n",
    "\n",
    "import llama_cpp\n",
    "from huggingface_hub import hf_hub_download\n",
    "import numpy as np\n",
    "\n",
    "np.int = int\n",
    "\n",
    "from skopt.space import Integer, Categorical\n",
    "\n",
    "\n",
    "new_model_name = \"lmstudio-community/gemma-2-27b-it-GGUF\"\n",
    "new_model_file = \"gemma-2-27b-it-Q6_K.gguf\"\n",
    "MODEL_PATH = hf_hub_download(new_model_name, filename=new_model_file)\n",
    "\n",
    "# Hyperparameters\n",
    "space = [\n",
    "    Categorical([True, False], name=\"f16_kv\"),\n",
    "    Categorical([True, False], name=\"use_mlock\"),\n",
    "    Integer(1, multiprocessing.cpu_count(), name=\"n_threads\"),\n",
    "    Integer(0, 100, name=\"n_gpu_layers\"),\n",
    "]\n",
    "\n",
    "# TODO: Make this a random prompt to avoid any cache related inconsistencies\n",
    "PROMPT = \"\"\"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\n",
    "    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\n",
    "    You should generate only the question. The language should be {lang_dict[lang]}. \n",
    "\"\"\"\n",
    "\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    f16_kv = params[\"f16_kv\"]\n",
    "    use_mlock = params[\"use_mlock\"]\n",
    "    n_threads = params[\"n_threads\"]\n",
    "    n_batch = params[\"n_gpu_layers\"]\n",
    "    llm = llama_cpp.Llama(\n",
    "        model_path=MODEL_PATH,\n",
    "        f16_kv=f16_kv,\n",
    "        use_mlock=use_mlock,\n",
    "        n_threads=n_threads,\n",
    "        n_gpu_layers=n_batch,\n",
    "    )\n",
    "\n",
    "    t1 = time.time()\n",
    "    output = llm(\n",
    "        PROMPT,\n",
    "        max_tokens=4096,  # Only optimize prompt processing\n",
    "        stop=[\"###\", \"\\n\"],\n",
    "        echo=True,\n",
    "    )\n",
    "    t2 = time.time()\n",
    "\n",
    "    print(json.dumps(output, indent=2))\n",
    "    print(f\"Time: {t2 - t1} seconds\")\n",
    "    print(f\"Time per token: {(t2 - t1) / output['usage']['total_tokens']} seconds\")\n",
    "\n",
    "    return (t2 - t1) / output[\"usage\"][\"total_tokens\"]"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T14:20:17.139180Z",
     "start_time": "2024-08-11T14:20:17.137769Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T14:53:39.833973Z",
     "start_time": "2024-08-11T14:20:17.139856Z"
    }
   },
   "source": [
    "from skopt import gp_minimize\n",
    "\n",
    "res = gp_minimize(objective, space)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17862.55 ms\n",
      "llama_print_timings:      sample time =       0.13 ms /     1 runs   (    0.13 ms per token,  7462.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17862.47 ms /    95 tokens (  188.03 ms per token,     5.32 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17866.66 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-10cedc90-f0ba-45ac-b077-2df7d8bd2730\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386023,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.869253158569336 seconds\n",
      "Time per token: 0.1880974016691509 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   18203.35 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7352.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18203.28 ms /    95 tokens (  191.61 ms per token,     5.22 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   18205.16 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-58ce8d2e-c8ae-4c4e-aba7-c3b346fadf75\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386043,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 18.208917140960693 seconds\n",
      "Time per token: 0.19167281201011258 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16937.68 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7246.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16937.61 ms /    95 tokens (  178.29 ms per token,     5.61 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16939.69 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-526fdd6b-12da-45f3-8656-3104f99f4627\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386064,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.942920446395874 seconds\n",
      "Time per token: 0.17834653101469342 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17422.56 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7042.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17422.49 ms /    95 tokens (  183.39 ms per token,     5.45 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17424.29 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-dbfcfddc-60c1-47b7-81b9-b26725a89f55\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386083,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.429821252822876 seconds\n",
      "Time per token: 0.18347180266129343 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16564.47 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7352.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16564.40 ms /    95 tokens (  174.36 ms per token,     5.74 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16565.33 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-3a152d1f-6aa9-440d-94ec-5dedda02df2a\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386103,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n    \",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 16.568613529205322 seconds\n",
      "Time per token: 0.17258972426255545 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16466.94 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7042.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16466.87 ms /    95 tokens (  173.34 ms per token,     5.77 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16468.42 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-56c4e09a-5bc4-4cfc-96f3-52620fc01289\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386122,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.47110342979431 seconds\n",
      "Time per token: 0.17338003610309802 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16760.99 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7407.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16760.91 ms /    95 tokens (  176.43 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16763.02 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-bd195a87-e76b-4328-98c7-8ec7a7aa09b8\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386141,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.765340566635132 seconds\n",
      "Time per token: 0.17647726912247508 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16540.00 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7246.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16539.93 ms /    95 tokens (  174.10 ms per token,     5.74 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16540.75 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-cb930256-00b4-48b0-b4b8-2e1e682a5831\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386161,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n{\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 16.544031143188477 seconds\n",
      "Time per token: 0.17233365774154663 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16802.64 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7299.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16802.58 ms /    95 tokens (  176.87 ms per token,     5.65 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16803.88 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-eef89955-22a9-4ec3-a65e-3adb45a81616\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386180,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.80823040008545 seconds\n",
      "Time per token: 0.17692874105353104 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17037.24 ms\n",
      "llama_print_timings:      sample time =       0.15 ms /     1 runs   (    0.15 ms per token,  6451.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17037.18 ms /    95 tokens (  179.34 ms per token,     5.58 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17038.62 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-858d22de-f9b5-4c40-8f48-9f9a773f01b6\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386199,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.041451930999756 seconds\n",
      "Time per token: 0.17938370453683952 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16999.47 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7299.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16999.41 ms /    95 tokens (  178.94 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17001.71 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-27cb5865-a382-4227-82bd-2accee5eed9d\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386219,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.004688501358032 seconds\n",
      "Time per token: 0.17899672106692666 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16492.02 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7352.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16491.96 ms /    95 tokens (  173.60 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16492.80 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-63b32f16-59b1-48bf-ad00-e18f0232844e\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386239,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n    \",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 16.496050596237183 seconds\n",
      "Time per token: 0.17183386037747064 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16535.04 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  6896.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16534.97 ms /    95 tokens (  174.05 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16537.15 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-3b3f919a-7754-4305-af4e-5b4e07ed3b1b\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386258,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.539721250534058 seconds\n",
      "Time per token: 0.17410232895299008 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17507.90 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7042.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17507.82 ms /    95 tokens (  184.29 ms per token,     5.43 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17509.79 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-429d40e0-db76-4688-94c4-530f01358ec6\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386277,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.513822317123413 seconds\n",
      "Time per token: 0.18435602439077278 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17822.58 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7299.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17822.51 ms /    95 tokens (  187.61 ms per token,     5.33 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17823.36 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-2f82ad72-db78-4a2c-b855-d2fa6ca5e3c4\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386298,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.82685875892639 seconds\n",
      "Time per token: 0.1876511448308041 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16490.76 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7299.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16490.69 ms /    95 tokens (  173.59 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16491.68 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-d3d78109-d809-4cd7-9b42-85fc1e6d7787\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386318,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.494736433029175 seconds\n",
      "Time per token: 0.17362880455820184 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16509.61 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  6944.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16509.54 ms /    95 tokens (  173.78 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16510.75 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-35ff4f7a-2797-4ca9-8002-967284c9e205\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386338,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.513564348220825 seconds\n",
      "Time per token: 0.17382699313916658 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17018.32 ms\n",
      "llama_print_timings:      sample time =       0.20 ms /     1 runs   (    0.20 ms per token,  5050.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17018.25 ms /    95 tokens (  179.14 ms per token,     5.58 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17020.41 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-172f926a-734e-4d9e-ae08-200aa90c0fea\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386357,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.022932291030884 seconds\n",
      "Time per token: 0.17918876095821984 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   15942.51 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7142.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15942.44 ms /    95 tokens (  167.82 ms per token,     5.96 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   15943.69 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-5ee06e8f-efe7-4d9a-8583-f8d26f16db43\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386377,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 15.946713924407959 seconds\n",
      "Time per token: 0.16786014657271536 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16609.30 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7194.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16609.23 ms /    95 tokens (  174.83 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16610.11 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-feb48149-883c-48cb-b827-cfb35f758f42\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386395,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.61321496963501 seconds\n",
      "Time per token: 0.1748759470487896 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   15811.09 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7407.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15811.03 ms /    95 tokens (  166.43 ms per token,     6.01 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   15812.10 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-9bfd643c-f80f-4197-9ca4-b4dba1d9ff59\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386415,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 15.81565546989441 seconds\n",
      "Time per token: 0.16648058389362536 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16707.21 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7407.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16707.14 ms /    95 tokens (  175.86 ms per token,     5.69 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16707.92 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-a9eae9d3-3903-4248-bc2c-9d5d64b8e5d7\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386433,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n    \",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 16.711182594299316 seconds\n",
      "Time per token: 0.1740748186906179 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   15844.62 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7246.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15844.55 ms /    95 tokens (  166.78 ms per token,     6.00 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   15845.04 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-a2c009a5-153b-40c5-b8c6-31760c5b44fb\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386453,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n{\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 15.84836459159851 seconds\n",
      "Time per token: 0.1650871311624845 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16044.81 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7352.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16044.75 ms /    95 tokens (  168.89 ms per token,     5.92 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16045.76 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-9f48ba76-435b-47ac-b61e-789552000f5a\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386472,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n    \",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 16.048725605010986 seconds\n",
      "Time per token: 0.16717422505219778 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   15854.69 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7246.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15854.62 ms /    95 tokens (  166.89 ms per token,     5.99 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   15855.54 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-00471472-2a30-4fb8-9601-52b2870d8bf2\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386491,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 15.861162185668945 seconds\n",
      "Time per token: 0.16695960195440995 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   15953.89 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7194.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15953.82 ms /    95 tokens (  167.93 ms per token,     5.95 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   15954.88 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-f9a9d656-73e4-4c06-aa74-75767b7f2e9f\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386509,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 15.957979679107666 seconds\n",
      "Time per token: 0.1679787334642912 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16456.08 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7299.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16456.01 ms /    95 tokens (  173.22 ms per token,     5.77 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16457.08 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-b24dd37d-7182-45db-9e41-f5bdb48bf584\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386528,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n{\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 16.460307598114014 seconds\n",
      "Time per token: 0.1714615374803543 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   19339.58 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7407.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19339.51 ms /    95 tokens (  203.57 ms per token,     4.91 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   19340.35 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-509277a2-1b49-4a5a-8606-62efa3061cfa\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386547,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 19.34366726875305 seconds\n",
      "Time per token: 0.20361755019740055 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16952.06 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7246.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16951.99 ms /    95 tokens (  178.44 ms per token,     5.60 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16952.92 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-0ca1f8b0-0366-4db7-b582-5c5ce43113de\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386570,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.95613169670105 seconds\n",
      "Time per token: 0.17848559680737947 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16606.85 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7407.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16606.78 ms /    95 tokens (  174.81 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16608.82 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-3a629c98-1eaa-410b-accb-a62a2816cbae\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386589,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.61188554763794 seconds\n",
      "Time per token: 0.17486195313303093 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17599.09 ms\n",
      "llama_print_timings:      sample time =       0.13 ms /     1 runs   (    0.13 ms per token,  7633.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17599.03 ms /    95 tokens (  185.25 ms per token,     5.40 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17600.70 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-49b5a05d-f0cb-4dfb-86f1-cbf2ac557eb3\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386609,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n    \",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 17.605708837509155 seconds\n",
      "Time per token: 0.18339280039072037 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16883.60 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7352.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16883.54 ms /    95 tokens (  177.72 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16884.75 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-f0e5d3d5-9bca-416b-be4c-6d2dfcc7011d\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386629,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.888418674468994 seconds\n",
      "Time per token: 0.1777728281523052 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16357.95 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7194.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16357.87 ms /    95 tokens (  172.19 ms per token,     5.81 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16359.52 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-8db4a892-83d8-4d4c-b8bc-4a45b29c7347\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386649,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.363523960113525 seconds\n",
      "Time per token: 0.17224762063277396 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16159.87 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7299.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16159.80 ms /    95 tokens (  170.10 ms per token,     5.88 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16160.96 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-79ca9163-6e58-4622-b384-e8a2868f2c0e\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386668,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n    \",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 16.16413402557373 seconds\n",
      "Time per token: 0.16837639609972635 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16320.25 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7352.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16320.18 ms /    95 tokens (  171.79 ms per token,     5.82 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16321.43 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-c7dbd7df-27da-4706-ba92-f3ca93ddf551\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386688,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.32562780380249 seconds\n",
      "Time per token: 0.17184871372423674 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16223.28 ms\n",
      "llama_print_timings:      sample time =       0.19 ms /     1 runs   (    0.19 ms per token,  5208.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16223.21 ms /    95 tokens (  170.77 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16226.01 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-0f83e621-1277-4f41-af1d-5a1f20543cee\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386707,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n    \",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 16.22873067855835 seconds\n",
      "Time per token: 0.16904927790164948 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16658.10 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7299.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16658.03 ms /    95 tokens (  175.35 ms per token,     5.70 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16659.19 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-0bcaad5d-661b-484e-bbc3-04c3807e8254\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386726,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.6651132106781 seconds\n",
      "Time per token: 0.17542224432292738 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16783.12 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7299.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16783.05 ms /    95 tokens (  176.66 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16785.02 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-ed6ab6cb-ea6e-4ae5-827e-1ac6423d61d1\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386746,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.787624835968018 seconds\n",
      "Time per token: 0.1767118403786107 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16828.30 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7092.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16828.23 ms /    95 tokens (  177.14 ms per token,     5.65 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16830.10 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-9bb973c2-7f9c-406f-816f-26c9bbb34e4b\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386765,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.83564853668213 seconds\n",
      "Time per token: 0.1772173530177066 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16436.68 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7194.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16436.61 ms /    95 tokens (  173.02 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16437.87 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-ce1d5038-91f9-4568-8dcc-d1d5a1878537\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386785,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.44368290901184 seconds\n",
      "Time per token: 0.1730913990422299 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16867.47 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7352.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16867.40 ms /    95 tokens (  177.55 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16868.98 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-d3a7963d-6bb3-4e99-b542-566ff57d8747\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386805,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.874085664749146 seconds\n",
      "Time per token: 0.1776219543657805 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16984.76 ms\n",
      "llama_print_timings:      sample time =       0.24 ms /     1 runs   (    0.24 ms per token,  4201.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16984.69 ms /    95 tokens (  178.79 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16986.89 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-1ab44f9a-2fa3-4b45-9469-6ff02329e633\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386824,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.99185061454773 seconds\n",
      "Time per token: 0.17886158541629188 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17211.52 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7299.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17211.45 ms /    95 tokens (  181.17 ms per token,     5.52 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17212.63 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-f66f7347-a43e-43fa-a3ff-f1b217f171c5\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386845,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.217299222946167 seconds\n",
      "Time per token: 0.18123472866259124 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17114.42 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7352.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17114.35 ms /    95 tokens (  180.15 ms per token,     5.55 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17115.41 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-85c19bc9-6804-4a8d-882a-a39a98ce819d\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386865,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.120941877365112 seconds\n",
      "Time per token: 0.1802204408143696 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16706.15 ms\n",
      "llama_print_timings:      sample time =       0.19 ms /     1 runs   (    0.19 ms per token,  5376.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16706.06 ms /    95 tokens (  175.85 ms per token,     5.69 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16708.02 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-2f27c75c-1fd6-48f2-97e3-f3ec0e5e010b\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386885,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.71285128593445 seconds\n",
      "Time per token: 0.17592475037825736 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16245.75 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7194.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16245.67 ms /    95 tokens (  171.01 ms per token,     5.85 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16246.89 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-d2ee55df-e120-4cb9-815e-bf0e57f65f72\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386905,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.251502752304077 seconds\n",
      "Time per token: 0.17106845002425344 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16414.73 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7299.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16414.65 ms /    95 tokens (  172.79 ms per token,     5.79 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16416.99 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-a00790c0-02db-4110-b54c-262222d68add\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386924,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n    \",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 16.419822454452515 seconds\n",
      "Time per token: 0.17103981723388037 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17881.17 ms\n",
      "llama_print_timings:      sample time =       0.26 ms /     1 runs   (    0.26 ms per token,  3816.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17881.10 ms /    95 tokens (  188.22 ms per token,     5.31 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17884.64 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-e9d4a26b-4156-42a1-9454-e279ae3b6dee\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386943,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n    \",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 17.887340784072876 seconds\n",
      "Time per token: 0.18632646650075912 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17083.74 ms\n",
      "llama_print_timings:      sample time =       0.16 ms /     1 runs   (    0.16 ms per token,  6172.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17083.68 ms /    95 tokens (  179.83 ms per token,     5.56 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17086.54 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-88a072f4-9231-4052-86d2-5e2a4cace65c\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386964,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.090564966201782 seconds\n",
      "Time per token: 0.1799006838547556 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   18204.28 ms\n",
      "llama_print_timings:      sample time =       0.16 ms /     1 runs   (    0.16 ms per token,  6289.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18204.22 ms /    95 tokens (  191.62 ms per token,     5.22 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   18207.31 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-98712d2e-c91d-48b4-a5ca-75abac6078d5\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723386985,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 18.212040662765503 seconds\n",
      "Time per token: 0.19170569118700528 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   18519.35 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     1 runs   (    0.30 ms per token,  3322.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18519.29 ms /    95 tokens (  194.94 ms per token,     5.13 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   18524.96 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-2c8deb4f-1f05-4cce-9517-6e547d91bf42\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387006,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 18.527761459350586 seconds\n",
      "Time per token: 0.19502906799316405 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17473.95 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7299.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17473.87 ms /    95 tokens (  183.94 ms per token,     5.44 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17476.30 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-807376db-dab2-4012-a182-23ed235a2f0f\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387028,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.479220628738403 seconds\n",
      "Time per token: 0.18399179609198318 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16881.13 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7407.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16881.06 ms /    95 tokens (  177.70 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16883.29 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-b7da0d09-43a5-466e-8e85-7e9c16b2a668\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387049,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.888970375061035 seconds\n",
      "Time per token: 0.17777863552695827 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16310.95 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7246.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16310.88 ms /    95 tokens (  171.69 ms per token,     5.82 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16313.30 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-72c67c5f-b5ae-46b4-8a17-d5e7f4f11c41\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387068,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n{\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 16.316274166107178 seconds\n",
      "Time per token: 0.1699611892302831 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   18857.37 ms\n",
      "llama_print_timings:      sample time =       0.19 ms /     1 runs   (    0.19 ms per token,  5263.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18857.31 ms /    95 tokens (  198.50 ms per token,     5.04 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   18860.20 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-ba8b35bc-b055-4ec8-b9ef-72a4802acbd1\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387088,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 18.864640951156616 seconds\n",
      "Time per token: 0.19857516790691174 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16386.63 ms\n",
      "llama_print_timings:      sample time =       0.13 ms /     1 runs   (    0.13 ms per token,  7462.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16386.56 ms /    95 tokens (  172.49 ms per token,     5.80 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16388.90 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-65799274-b9dd-4607-80b3-c9a4e279d4c1\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387110,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.391558170318604 seconds\n",
      "Time per token: 0.17254271758230108 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17568.69 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  6896.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17568.62 ms /    95 tokens (  184.93 ms per token,     5.41 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17571.22 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-faca2897-978e-4cb8-8348-dc5c2950724c\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387129,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.574123859405518 seconds\n",
      "Time per token: 0.1849907774674265 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16605.45 ms\n",
      "llama_print_timings:      sample time =       0.15 ms /     1 runs   (    0.15 ms per token,  6802.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16605.32 ms /    95 tokens (  174.79 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16607.66 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-f3b8b7c9-edb7-4edb-acb2-28b84a0a19f3\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387150,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n    \",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 16.61147141456604 seconds\n",
      "Time per token: 0.17303616056839624 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16496.29 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7246.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16496.22 ms /    95 tokens (  173.64 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16498.62 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-b1401cc2-68f7-4eda-8efe-896b0a3b5fbf\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387169,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.501762866973877 seconds\n",
      "Time per token: 0.17370276702077767 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17997.62 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7142.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17997.56 ms /    95 tokens (  189.45 ms per token,     5.28 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   18000.17 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-82c8377d-fd0b-4866-8ff5-e18cb6559449\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387189,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 18.00332999229431 seconds\n",
      "Time per token: 0.18950873676099275 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17319.10 ms\n",
      "llama_print_timings:      sample time =       0.16 ms /     1 runs   (    0.16 ms per token,  6329.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17318.96 ms /    95 tokens (  182.30 ms per token,     5.49 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17321.92 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-e8dd2c17-b575-496a-8cae-47a642b3e379\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387210,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.32512140274048 seconds\n",
      "Time per token: 0.18236969897621555 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   18556.86 ms\n",
      "llama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 11627.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18556.78 ms /    95 tokens (  195.33 ms per token,     5.12 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   18562.03 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-f22d8939-95c1-4a6e-be70-57547cd19a53\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387230,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 18.565471649169922 seconds\n",
      "Time per token: 0.1954260173596834 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17519.01 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7299.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17518.94 ms /    95 tokens (  184.41 ms per token,     5.42 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17521.43 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-3ff766ca-90b8-4ae1-b421-5c1ad8ab27ff\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387252,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n    \",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 17.525375843048096 seconds\n",
      "Time per token: 0.18255599836508432 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17820.96 ms\n",
      "llama_print_timings:      sample time =       0.13 ms /     1 runs   (    0.13 ms per token,  7692.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17820.89 ms /    95 tokens (  187.59 ms per token,     5.33 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17822.80 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-3679e61b-d6f8-4c4a-b8a3-b47736c459b3\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387273,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.826493978500366 seconds\n",
      "Time per token: 0.18764730503684596 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17096.99 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7352.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17096.92 ms /    95 tokens (  179.97 ms per token,     5.56 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17099.08 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-dd619faa-b8dd-420a-ba90-06eee6385fd9\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387294,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.10205841064453 seconds\n",
      "Time per token: 0.18002166748046874 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16687.74 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7092.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16687.67 ms /    95 tokens (  175.66 ms per token,     5.69 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16690.62 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-c07499b7-8ccf-486e-a1f6-257f3437ee0a\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387314,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.694769382476807 seconds\n",
      "Time per token: 0.17573441455238745 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17113.07 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7246.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17113.00 ms /    95 tokens (  180.14 ms per token,     5.55 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17115.27 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-663be591-ca80-4316-b0aa-94601a9ccd26\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387333,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.118582248687744 seconds\n",
      "Time per token: 0.18019560261776574 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17381.42 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7352.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17381.34 ms /    95 tokens (  182.96 ms per token,     5.47 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17383.20 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-dafefd00-ae47-4d6f-bb1d-acc0d2cfb4d5\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387354,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.38741421699524 seconds\n",
      "Time per token: 0.1830254128104762 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16815.59 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7194.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16815.52 ms /    95 tokens (  177.01 ms per token,     5.65 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16817.58 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-1bc306b1-78db-4f2b-bd3a-95227b37ee1e\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387374,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.821226358413696 seconds\n",
      "Time per token: 0.177065540614881 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16738.30 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7299.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16738.23 ms /    95 tokens (  176.19 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16740.95 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-37e8fe3b-b959-4db4-8e2f-54f684b38aa1\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387394,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.745174407958984 seconds\n",
      "Time per token: 0.17626499376798932 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   18227.30 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  6993.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18227.23 ms /    95 tokens (  191.87 ms per token,     5.21 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   18230.12 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-2c5b6409-5aa5-476e-bff5-9fd4fe9db35d\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387414,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 18.233450174331665 seconds\n",
      "Time per token: 0.1919310544666491 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17817.66 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7246.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17817.60 ms /    95 tokens (  187.55 ms per token,     5.33 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17819.77 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-252e8b99-adef-4bef-ba9c-1c0b1ff27677\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387435,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.823384284973145 seconds\n",
      "Time per token: 0.18761457142076995 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16840.08 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7352.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16839.98 ms /    95 tokens (  177.26 ms per token,     5.64 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16842.97 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-1976e5ca-024b-4674-9183-0ea113cc5458\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387456,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n```\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 16.846423149108887 seconds\n",
      "Time per token: 0.17548357446988425 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16618.44 ms\n",
      "llama_print_timings:      sample time =       0.36 ms /     1 runs   (    0.36 ms per token,  2770.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16618.37 ms /    95 tokens (  174.93 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16620.71 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-f2929861-4e5d-4781-a6dd-1e5861f0b37e\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387476,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.624976634979248 seconds\n",
      "Time per token: 0.17499975405241314 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17398.61 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7194.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17398.54 ms /    95 tokens (  183.14 ms per token,     5.46 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17401.32 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-d4589f76-4ff1-416c-bc58-e31ab759ff6d\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387496,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.404369592666626 seconds\n",
      "Time per token: 0.1832038904491224 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16612.69 ms\n",
      "llama_print_timings:      sample time =       0.13 ms /     1 runs   (    0.13 ms per token,  7462.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16612.62 ms /    95 tokens (  174.87 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16615.10 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-408a8b74-8053-4e6e-b76b-f20da4d18fe9\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387516,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.619630813598633 seconds\n",
      "Time per token: 0.17494348224840667 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17175.22 ms\n",
      "llama_print_timings:      sample time =       0.18 ms /     1 runs   (    0.18 ms per token,  5681.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17175.15 ms /    95 tokens (  180.79 ms per token,     5.53 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17180.32 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-a0903178-0f01-493a-97c9-5fd529c80882\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387536,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.18335795402527 seconds\n",
      "Time per token: 0.1808774521476344 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17523.15 ms\n",
      "llama_print_timings:      sample time =       0.19 ms /     1 runs   (    0.19 ms per token,  5235.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17523.08 ms /    95 tokens (  184.45 ms per token,     5.42 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17527.65 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-2bd6cfac-921e-4508-83a5-c6f7e3e2b78d\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387556,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n    \",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 17.530291080474854 seconds\n",
      "Time per token: 0.18260719875494638 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17296.63 ms\n",
      "llama_print_timings:      sample time =       1.48 ms /     1 runs   (    1.48 ms per token,   674.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17296.56 ms /    95 tokens (  182.07 ms per token,     5.49 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17300.81 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-9706aa03-0251-40c1-b3c2-7478e42d5de0\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387577,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.303733348846436 seconds\n",
      "Time per token: 0.18214456156680459 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17654.80 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7246.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17654.72 ms /    95 tokens (  185.84 ms per token,     5.38 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17657.73 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-b4d7569d-efc9-417a-b914-9a9277863a91\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387598,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.662243366241455 seconds\n",
      "Time per token: 0.18591835122359426 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17702.19 ms\n",
      "llama_print_timings:      sample time =       0.17 ms /     1 runs   (    0.17 ms per token,  6060.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17702.13 ms /    95 tokens (  186.34 ms per token,     5.37 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17707.12 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-7ce4e343-3ba2-4f7e-b033-98d825372cf4\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387619,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n    \",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 17.710264444351196 seconds\n",
      "Time per token: 0.18448192129532495 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17769.77 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7299.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17769.70 ms /    95 tokens (  187.05 ms per token,     5.35 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17772.86 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-de8e4093-b48b-4223-bd71-d7a204638bc6\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387640,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.776880264282227 seconds\n",
      "Time per token: 0.1871250554134971 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17829.12 ms\n",
      "llama_print_timings:      sample time =       0.23 ms /     1 runs   (    0.23 ms per token,  4405.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17829.05 ms /    95 tokens (  187.67 ms per token,     5.33 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17832.95 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-f2bea60a-6607-4139-b3e1-f1a23e5414fc\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387660,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.836820125579834 seconds\n",
      "Time per token: 0.187756001321893 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17274.64 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7299.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17274.57 ms /    95 tokens (  181.84 ms per token,     5.50 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17277.40 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-4dd2a21b-4c56-4f8e-879d-fb62819bbcbe\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387682,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.282302379608154 seconds\n",
      "Time per token: 0.18191897241692795 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17265.50 ms\n",
      "llama_print_timings:      sample time =       0.17 ms /     1 runs   (    0.17 ms per token,  5917.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17265.43 ms /    95 tokens (  181.74 ms per token,     5.50 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17268.52 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-261aafb0-7382-44ee-9470-63fd55fdf631\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387702,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.273083686828613 seconds\n",
      "Time per token: 0.18182193354556436 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   17050.55 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7407.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17050.46 ms /    95 tokens (  179.48 ms per token,     5.57 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   17053.34 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-e0694744-98ca-49c1-9d7b-52bffd9d6830\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387723,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 17.056190490722656 seconds\n",
      "Time per token: 0.1795388472707648 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16853.68 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7194.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16853.61 ms /    95 tokens (  177.41 ms per token,     5.64 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16856.52 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-fba81044-843a-4047-8605-cf36e2fd3247\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387743,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.86140203475952 seconds\n",
      "Time per token: 0.17748844247115286 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16234.68 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7142.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16234.62 ms /    95 tokens (  170.89 ms per token,     5.85 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16237.41 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-8a013023-a58f-4e09-bc0c-867d8f7d2d69\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387763,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.240634202957153 seconds\n",
      "Time per token: 0.17095404424165425 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16751.63 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7246.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16751.57 ms /    95 tokens (  176.33 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16754.22 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-53e9a652-ff31-4719-9e21-b65e5ec0c136\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387783,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n```\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 16.757706880569458 seconds\n",
      "Time per token: 0.1745594466725985 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16762.22 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7042.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16762.14 ms /    95 tokens (  176.44 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16765.29 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-db052802-752a-4818-b61a-41b182d1828c\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387803,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n    \",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 16.76793122291565 seconds\n",
      "Time per token: 0.17466595023870468 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16256.26 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7092.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16256.18 ms /    95 tokens (  171.12 ms per token,     5.84 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16259.40 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-8728daca-5fb3-4899-9e3d-5585425c78eb\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387822,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.2621910572052 seconds\n",
      "Time per token: 0.17118095849689685 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16250.50 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7194.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16250.43 ms /    95 tokens (  171.06 ms per token,     5.85 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16253.32 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-561ed21f-5d63-4e58-9f27-81240b294139\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387842,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.255987882614136 seconds\n",
      "Time per token: 0.17111566192225405 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16667.10 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7352.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16667.04 ms /    95 tokens (  175.44 ms per token,     5.70 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16669.60 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-ade16138-028c-475c-86c9-d9e022e13317\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387862,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n{\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 16.67336082458496 seconds\n",
      "Time per token: 0.17368084192276 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   18263.79 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7407.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18263.71 ms /    95 tokens (  192.25 ms per token,     5.20 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   18266.00 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-0b784a73-b712-41c8-b9c1-166900cd8d10\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387882,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 18.269050359725952 seconds\n",
      "Time per token: 0.19230579326027317 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16138.47 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7299.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16138.39 ms /    95 tokens (  169.88 ms per token,     5.89 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16141.00 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-5673011b-ebc0-4f76-b069-ff954f179983\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387903,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.144578218460083 seconds\n",
      "Time per token: 0.1699429286153693 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16160.65 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7142.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16160.58 ms /    95 tokens (  170.11 ms per token,     5.88 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16163.41 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-0b2bd1eb-05e7-446e-a9ce-53a8cf04159c\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387923,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.165908813476562 seconds\n",
      "Time per token: 0.17016746119449014 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16294.74 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  6896.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16294.67 ms /    95 tokens (  171.52 ms per token,     5.83 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16297.46 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-00b51fd3-e8e6-4f17-847e-7fa63c1f730c\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387942,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.301360607147217 seconds\n",
      "Time per token: 0.17159326954891807 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16746.48 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  6944.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16746.41 ms /    95 tokens (  176.28 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16749.49 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-24619843-ebb9-44b8-86d6-0a2b2aa37e72\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387962,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.753884077072144 seconds\n",
      "Time per token: 0.17635667449549625 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hellstrom/folder/autoencoders/venv/lib/python3.11/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [False, True, 1, 100] before, using random point [False, False, 11, 57]\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16025.66 ms\n",
      "llama_print_timings:      sample time =       0.15 ms /     1 runs   (    0.15 ms per token,  6849.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16025.59 ms /    95 tokens (  168.69 ms per token,     5.93 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16027.96 ms /    96 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-9ff29a69-fccb-43e5-a636-5042dc012bdf\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723387982,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 0,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "Time: 16.030988693237305 seconds\n",
      "Time per token: 0.16874724940249794 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 508 tensors from /home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-27b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 4608\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 46\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 36864\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models/gemma-2-27b-it-GGUF/gemma-2-2...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 322\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  185 tensors\n",
      "llama_model_loader: - type q6_K:  323 tensors\n",
      "llm_load_vocab: special tokens cache size = 108\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4608\n",
      "llm_load_print_meta: n_layer          = 46\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 36864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 27B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 27.23 B\n",
      "llm_load_print_meta: model size       = 20.80 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-27b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.23 MiB\n",
      "llm_load_tensors:        CPU buffer size = 21302.67 MiB\n",
      ".warning: failed to mlock 21369786368-byte buffer (after previously locking 973742080 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      ".............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   184.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  184.00 MiB, K (f16):   92.00 MiB, V (f16):   92.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   509.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1850\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '128', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.attention.value_length': '128', 'gemma2.attention.sliding_window': '4096', 'gemma2.attention.head_count': '32', 'gemma2.feed_forward_length': '36864', 'gemma2.block_count': '46', 'tokenizer.ggml.pre': 'default', 'gemma2.embedding_length': '4608', 'general.file_type': '18', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '16', 'tokenizer.ggml.add_eos_token': 'false', 'quantize.imatrix.file': '/models/gemma-2-27b-it-GGUF/gemma-2-27b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'general.name': 'gemma-2-27b-it', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'gemma2.attention.key_length': '128', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.entries_count': '322'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "\n",
      "llama_print_timings:        load time =   16074.70 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     1 runs   (    0.14 ms per token,  7299.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16074.63 ms /    95 tokens (  169.21 ms per token,     5.91 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   16077.28 ms /    96 tokens\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-3718cf9d-8b7a-4736-b340-8cf82853aa5f\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1723388002,\n",
      "  \"model\": \"/home/hellstrom/.cache/huggingface/hub/models--lmstudio-community--gemma-2-27b-it-GGUF/snapshots/8f63ea1cfc5ce646813257d8f5d5a1afa82af58f/gemma-2-27b-it-Q6_K.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"You are conducting an interview for the position of {role}. Job description: {requirements}. {history_text}\\n    Your task is to generate the next question so that it best fits the job description and assesses the interviewee's competencies in the most critical areas. The question should have a clear answer and focus solely on algorithms and data structures.\\n    You should generate only the question. The language should be {lang_dict[lang]}. \\n{\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 1,\n",
      "    \"total_tokens\": 96\n",
      "  }\n",
      "}\n",
      "Time: 16.081121921539307 seconds\n",
      "Time per token: 0.1675116866827011 seconds\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T14:53:41.578688Z",
     "start_time": "2024-08-11T14:53:39.835101Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skopt.plots import plot_objective\n",
    "\n",
    "plot_objective(res)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 17 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvwAAALYCAYAAAAAZSavAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd1yT1/4H8E8IeytTEEXEPXBbR6u2WLVeraOOFhW19XpbvQ5a168Caquo1WrdtSpqtVVvHbWOtkBdqG0tijhRUQGRoSJbCCT5/RGJRsIIBB6SfN6vV14POTk5zzfkEb85OUMkl8vlICIiIiIivWQkdABERERERFR9mPATEREREekxJvxERERERHqMCT8RERERkR5jwk9EREREpMeY8BMRERER6TEm/EREREREeowJPxERERGRHmPCT0RERESkx5jw1wJyuRz//ve/UbduXYhEIkRHR1f7OUUiEQ4dOlTt5yEiIiIiYTHhrwV+/fVXbN++HUeOHEFycjKysrIwaNAguLm5lZmY37hxA4MHD4adnR2srKzQuXNnJCQk1GzwRERERFSrMeGvBeLi4lCvXj10794drq6uyM3NhY+PD9avX1/mc3r27InmzZvj5MmTiImJQWBgIMzNzWswciIiIiKq7YyFDsDQjR8/Hjt27ACgGGbTsGFD3L9/HwMGDCjzeZ9//jneeecdLF++XFnWuHHjSscRHByMzZs347fffsOePXsQERGBv/76S6WOj48Phg8fjqCgoEqfh4iIiIhqFnv4BfbNN99g0aJFqF+/PpKTk3HhwoVynyOTyXD06FE0bdoU/fr1g7OzM7p27VqpMflyuRz//e9/sXPnTpw5cwZt27aFn58f/v77b8TFxSnrXbt2DTExMfjggw80PgcRERERCYcJv8Ds7OxgY2MDsVgMV1dXODk5lfuctLQ05OTkYOnSpejfvz9+//13DB06FMOGDcOpU6cqfO6ioiKMGTMGERERiIyMhLe3NwCgVatW8PHxwQ8//KCsu3v3bnTt2lVZh4iIiIh0AxN+HSSTyQAA7777LmbOnIl27dph7ty5+Ne//oVNmzZVuJ2ZM2fir7/+wunTp+Hu7q7ymJ+fnzLhl8vl+PHHH+Hn56e9F0FERERENYIJvw5ydHSEsbExWrZsqVLeokULjVbp6du3L5KSkvDbb7+VeOz9999HbGwsLl68iHPnziExMRGjRo2qcuxEREREVLM4aVcHmZqaonPnzoiNjVUpv3XrFho2bFjhdgYPHoxBgwbhgw8+gFgsxujRo5WP1a9fH7169cLu3bvx7Nkz9O3bF87Ozlp7DURERERUM5jw10I5OTm4c+eO8v69e/cQHR2NunXrokGDBgCAWbNmYdSoUXjjjTfQp08f/Prrr/jll19w8uRJjc41dOhQfP/99xg7diyMjY3x3nvvKR/z8/NDcHAwJBIJVq1apZXXRkREREQ1iwl/LfTPP/+gT58+yvsBAQEAAH9/f2zfvh2AIlHftGkTQkJCMG3aNDRr1gz79+9Hz549NT7fe++9B5lMhrFjx8LIyAjDhg1Tlk+dOhVisRhDhgyp8usiIiIioponksvlcqGDICIiIiKi6sFJu0REREREeowJvx7avXs3rK2t1d5atWoldHhEREREVIM4pEcPZWdnIzU1Ve1jJiYmGq3kQ0RERES6jQk/EREREZEe45AeIiIiIiI9xoSfdMr27dthb28vdBhEREREOkPnE36RSFTmbcGCBUKHSGqMHz9e7fv18oZjRERERFR1Or/xVnJysvLnvXv3IigoCLGxscoya2tr5c9yuRxSqRTGxjr/svVC//79ERoaqlLm5OQkUDRERERE+knne/hdXV2VNzs7O4hEIuX9mzdvwsbGBsePH0fHjh1hZmaGyMhIjB8/vsTOsTNmzEDv3r2V92UyGUJCQtCoUSNYWFjAx8cHP/30U82+OD1nZmam8v65urrim2++QZs2bWBlZQUPDw988sknyMnJKbWNy5cvo0+fPrCxsYGtrS06duyIf/75R/l4ZGQkXn/9dVhYWMDDwwPTpk1Dbm5uTbw8IiIiolpB5xP+ipg7dy6WLl2KGzduoG3bthV6TkhICHbu3IlNmzbh2rVrmDlzJsaMGYNTp05Vc7SGzcjICGvWrMG1a9ewY8cO/PHHH5g9e3ap9f38/FC/fn1cuHABUVFRmDt3LkxMTAAAcXFx6N+/P4YPH46YmBjs3bsXkZGRmDp1ak29HCIiIiLBGcTYlkWLFqFv374Vrl9QUIAlS5YgPDwc3bp1AwB4eXkhMjIS3377LXr16lVdoRqUI0eOqAy5GjBgAP73v/8p73t6euLLL7/Ef/7zH2zYsEFtGwkJCZg1axaaN28OAGjSpInysZCQEPj5+WHGjBnKx9asWYNevXph48aNMDc3r4ZXRURERFS7GETC36lTJ43q37lzB3l5eSU+JEgkErRv316boRm0Pn36YOPGjcr7VlZWCA8PR0hICG7evImsrCwUFRUhPz8feXl5sLS0LNFGQEAAPvroI3z//ffw9fXFiBEj0LhxYwCK4T4xMTHYvXu3sr5cLodMJsO9e/fQokWL6n+RRERERAIziITfyspK5b6RkRFe3W+ssLBQ+XPxmPGjR4/C3d1dpZ6ZmVk1RWl4rKys4O3trbx///59/Otf/8LHH3+MxYsXo27duoiMjMSHH34IiUSiNuFfsGABPvjgAxw9ehTHjx9HcHAw9uzZg6FDhyInJweTJ0/GtGnTSjyvQYMG1fraiIiIiGoLg0j4X+Xk5ISrV6+qlEVHRyvHfrds2RJmZmZISEjg8J0aFBUVBZlMhpUrV8LISDG9ZN++feU+r2nTpmjatClmzpyJ999/H6GhoRg6dCg6dOiA69evq3yoICIiIjI0BjFp91Vvvvkm/vnnH+zcuRO3b99GcHCwygcAGxsbfPbZZ5g5cyZ27NiBuLg4XLx4EWvXrsWOHTsEjFy/eXt7o7CwEGvXrsXdu3fx/fffY9OmTaXWf/bsGaZOnYqTJ08iPj4eZ8+exYULF5RDdebMmYNz585h6tSpiI6Oxu3bt/Hzzz9z0i4REREZFINM+Pv164fAwEDMnj0bnTt3RnZ2NsaNG6dS54svvkBgYCBCQkLQokUL9O/fH0ePHkWjRo0Eilr/+fj44Ouvv8ayZcvQunVr7N69GyEhIaXWF4vFePLkCcaNG4emTZti5MiRGDBgABYuXAgAaNu2LU6dOoVbt27h9ddfR/v27REUFAQ3N7eaeklEREREghPJXx3MTkREREREesMge/iJiIiIiAwFE34iIiIiIj3GhJ+IiIiISI8x4SciIiIi0mNM+ImIiIiI9BgTfiIiIiIiPWawCX9BQQEWLFiAgoICoUMhDfG9q5iioiKEh4fj22+/RXZ2NgDg4cOHyMnJETgyIiIiqkkGuw5/VlYW7OzskJmZCVtbW6HDIQ3wvStffHw8+vfvj4SEBBQUFODWrVvw8vLC9OnTUVBQUOYOxkRERKRfDLaHn0ifTZ8+HZ06dcLTp09hYWGhLB86dCgiIiIEjIyIiIhqmrHQARCR9p05cwbnzp2DqampSrmnpyeSkpIEioqIiIiEoPcJv0wmw8OHD2FjYwORSKQsz8rKUjmS7ijtvZPL5cjOzoabmxuMjAz7yyuZTAapVFqi/MGDB7CxsREgIiIiIhKK3o/hf/DgATw8PIQOg2pQYmIi6tevL3QYgho1ahTs7OywefNm2NjYICYmBk5OTnj33XfRoEEDhIaGCh0iERER1RC9T/gzMzNhb2+PxMRETvDUF9HRQK9ewKlTQLt2yuKsrCx4eHggIyMDdnZ2goVXGzx48AD9+vWDXC7H7du30alTJ9y+fRuOjo44ffo0nJ2dhQ6RiIiIaojeD+kpHsZja2vLhF9fWFu/OKp5T18eumWo6tevj8uXL2Pv3r24fPkycnJy8OGHH8LPz09lEi8RERHpP71P+IkMlbGxMfz8/ODn5yd0KERERCQgw57ZSLrJxgZ4+23FkdQKCQnBtm3bSpRv27YNy5YtEyAiIiIiEgoTftI9TZoAv/2mOJJa3377LZo3b16ivFWrVtx0i4iIyMAw4SfdI5UCWVmKI6mVkpKCevXqlSh3cnJCcnKyABERERGRUJjwk+65fBmws1McSS0PDw+cPXu2RPnZs2fh5uYmQEREREQkFE7aJdJDkyZNwowZM1BYWIg333wTABAREYHZs2fj008/FTg6IiIiqklM+In00KxZs/DkyRN88sknkEgkAABzc3PMmTMH8+bNEzg6IiIiqklM+In0kEgkwrJlyxAYGIgbN27AwsICTZo0gZmZmdChERERUQ1jwk+kx6ytrdG5c2ehwyAiIiIBMeEn3dOmDZCWBtjbCx1JrZWbm4ulS5ciIiICaWlpkMlkKo/fvXtXoMiIiIiopjHhJ91jYgI4OQkdRa320Ucf4dSpUxg7dizq1asHkUgkdEhEREQkECb8pHvi4oCZM4FVq4DGjYWOplY6fvw4jh49ih49eggdChEREQmM6/CT7snMBH75RXEkterUqYO6desKHQYRERHVAkz4ifTQF198gaCgIOTl5QkdChEREQmMQ3qI9NDKlSsRFxcHFxcXeHp6wsTEROXxixcvChQZERER1TQm/ER6aMiQIUKHQERERLUEE37SPe7uwMqVimMVrV+/Hl999RVSUlLg4+ODtWvXokuXLmrrXrt2DUFBQYiKikJ8fDxWrVqFGTNmqNTx9PREfHx8ied+8sknWL9+PQAgJSUFs2bNQlhYGLKzs9GsWTN8/vnnGD58uLL+4MGDER0djbS0NNSpUwe+vr5YtmwZ3NzcKvS6goODK/gbICIiIn3HMfyke1xcgIAAxbEK9u7di4CAAAQHB+PixYvw8fFBv379kJaWprZ+Xl4evLy8sHTpUri6uqqtc+HCBSQnJytvYWFhAIARI0Yo64wbNw6xsbE4fPgwrly5gmHDhmHkyJG4dOmSsk6fPn2wb98+xMbGYv/+/YiLi8N7772n0evLyMjAli1bMG/ePKSnpwNQDOVJSkrSqB0iIiLSbSK5XC4XOojqlJWVBTs7O2RmZsLW1lbocEgbnj4FwsMBX1+gTh1lsabvddeuXdG5c2esW7cOACCTyeDh4YH//ve/mDt3bpnP9fT0xIwZM0r08L9qxowZOHLkCG7fvq1cC9/a2hobN27E2LFjlfUcHBywbNkyfPTRR2rbOXz4MIYMGYKCgoIS4/HViYmJga+vL+zs7HD//n3ExsbCy8sL8+fPR0JCAnbu3FluG0RERKQf2MNPuufePWDkSMWxkiQSCaKiouDr66ssMzIygq+vL86fP6+NKCGRSLBr1y5MnDhRZeOr7t27Y+/evUhPT4dMJsOePXuQn5+P3r17q20nPT0du3fvRvfu3SuU7ANAQEAAxo8fj9u3b8Pc3FxZ/s477+D06dNVel1ERESkW/Qu4S8oKEBWVpbKjQzLq+9/QUFBiTqPHz+GVCqFyyvDglxcXJCSkqKVOA4dOoSMjAyMHz9epXzfvn0oLCyEg4MDzMzMMHnyZBw8eBDe3t4q9ebMmQMrKys4ODggISEBP//8c4XPfeHCBUyePLlEubu7u9ZeHxEREekGvUv4Q0JCYGdnp7x5eHgIHRLVMA8PD5VrICQkRJA4tm7digEDBpSYaBsYGIiMjAyEh4fjn3/+QUBAAEaOHIkrV66o1Js1axYuXbqE33//HWKxGOPGjUNFR+CZmZmp/bB769YtODk5Vf5FERERkc7Ru1V65s2bh4CAAOX9rKwsJv0GJjExUWUMv5mZWYk6jo6OEIvFSE1NVSlPTU0tdUKuJuLj4xEeHo4DBw6olMfFxWHdunW4evUqWrVqBQDw8fHBmTNnsH79emzatEklRkdHRzRt2hQtWrSAh4cH/vzzT3Tr1q3c8w8ePBiLFi3Cvn37AAAikQgJCQmYM2eOympAREREpP/0roffzMwMtra2KjfSMxYWQPv2iqMar77/6hJ+U1NTdOzYEREREcoymUyGiIiICiXU5QkNDYWzszMGDhyoUl68862Rkeo/PbFYDJlMVmp7xY+pG56kzsqVK5GTkwNnZ2c8e/YMvXr1gre3N2xsbLB48WJNXgoRERHpOL3r4ScD0KIFoIWdYgMCAuDv749OnTqhS5cuWL16NXJzczFhwgQAiuUz3d3dlUOCJBIJrl+/rvw5KSkJ0dHRsLa2Vhl/L5PJEBoaCn9/fxgbq/4Ta968Oby9vTF58mSsWLECDg4OOHToEMLCwnDkyBEAwF9//YULFy6gZ8+eqFOnDuLi4hAYGIjGjRtX+MOInZ0dwsLCEBkZiZiYGOTk5KBDhw4qk5SJiIjIMHBZTtIblXmv161bp9x4q127dlizZg26du0KAOjduzc8PT2xfft2AMD9+/fRqFGjEm306tULJ0+eVN7//fff0a9fP8TGxqJp06Yl6t++fRtz585FZGQkcnJy4O3tjc8++0y5TOeVK1cwffp0XL58Gbm5uahXrx769++P+fPnw10Lm40RERGRYWHCT7rn0iXgtdeAP/9UDO15ztDf6zVr1lS47rRp06oxEiIiIqpNOKSHaoxcLkd+oQy5kiLkFUgVR0kRcgukqkeJFHkFz49qHq939wY2SSS4/jADLduXf15DsWrVKpX7jx49Ql5eHuzt7QEodt61tLSEs7MzE34iIiIDwoSf1CqSypBXKH2RmL+UoOdJXiqXSJFb8MpRpb5quTa+TypKV0x8zckvqnpjeuTeSxuR/fDDD9iwYQO2bt2KZs2aAQBiY2MxadIktevzExERkf7ikB4dJ5fLIZHKKpeAl/F4QVHpK8Zog5WpGJZmxoqjqTGszF45lvG4461raPvuW8g99xesunVRtqnv77UmGjdujJ9++gnt26t+BRIVFYX33ntP5cMBERER6Tf28NcgmUyOZ4VSRQ+5BkNYXjyuPmEvklXfZzaxkQhWpmJYmRnD8uWjqXGlE3ZzYzGMjESVDyovCQBgZcbLtzTJyckoKir5DYhUKi2x9wARERHpN2ZMpSiSyiqQgJeSqJdSnieRVmvMZsZGryTkz4+lJuzlP24qNoJIVIXkvDq0aAFcvQp4eQkdSa311ltvYfLkydiyZQs6dOgAQNG7//HHH3NpTiIiIgNjkAn/6VuPcODigzITekk1DmkRiVAy0TY1hoWpuEI95IrHX0rYzcSwNBHDWKx3+6ipZ2EBPN+lltTbtm2bco8BExMTAEBRURH69euHLVu2CBwdERER1SSDTPjjn+TiUPTDCtU1NhLByqwiCXjFE3Rzk1rYa65L4uOBL74AAgOBhg2FjqZWcnJywrFjx3Dr1i3cvHkTgGLTL3X7AhAREZF+M8hJu9cfZuFc3OPSE/aXEndTYwPpNdclFy8CHTsCUVHA8+EqACftEhEREaljkD38Ld1s0dKNCSHpL6lUiu3btyMiIgJpaWmQyVSHqP3xxx8CRUZEREQ1zSATfiJ9N336dGzfvh0DBw5E69atOYSMiIjIgDHhJ9JDe/bswb59+/DOO+8IHQoREREJjAPUSfe4uABz5yqOpJapqSm8vb2FDoOIiIhqASb8pHvc3YGQEMWR1Pr000/xzTffQM/n5BMREVEFcEgP6Z7sbMUKPR07AjY2QkdTK0VGRuLEiRM4fvw4WrVqpVyLv9iBAwcEioyIiIhqGhN+0j23bwN9+pRYlpNesLe3x9ChQ4UOg4iIiGoBJvxEeig0NFToEIiIiKiW4Bh+Ij1VVFSE8PBwfPvtt8jOzgYAPHz4EDk5OQJHRkRERDWJPfxEeig+Ph79+/dHQkICCgoK0LdvX9jY2GDZsmUoKCjApk2bhA6RiIiIagh7+En3mJgoVuh5ZSIqvTB9+nR06tQJT58+hYWFhbJ86NChiIiIEDAyIiIiqmns4Sfd06YN8OCB0FHUamfOnMG5c+dgamqqUu7p6YmkpCSBoiIiIiIhsIefSA/JZDJIpdIS5Q8ePIANlzIlIiIyKEz4SfdcuQLUr684klpvv/02Vq9erbwvEomQk5OD4OBgvPPOO8IFRkRERDWOQ3pI9xQWAklJiiOptXLlSvTr1w8tW7ZEfn4+PvjgA9y+fRuOjo748ccfhQ6PiIiIahATfiI9VL9+fVy+fBl79uxBTEwMcnJy8OGHH8LPz09lEi8RERHpPyb8RHrK2NgYY8aMEToMIiIiEhgTfiI9FRsbi7Vr1+LGjRsAgBYtWmDq1Klo3ry5wJERERFRTeKkXdI9TZoAJ04ojqTW/v370bp1a0RFRcHHxwc+Pj64ePEi2rRpg/379wsdHhEREdUgkVwulwsdRHXKysqCnZ0dMjMzYWtrK3Q4VI34Xr/QuHFj+Pn5YdGiRSrlwcHB2LVrF+Li4gSKjIiIiGoae/hJ9yQlAfPmKY6kVnJyMsaNG1eifMyYMUhOThYgIiIiIhIKE37SPampwNKliiOp1bt3b5w5c6ZEeWRkJF5//XUBIiIiIiKhcNIukR4aPHgw5syZg6ioKLz22msAgD///BP/+9//sHDhQhw+fFilLhEREekvjuEn3XPxItCxIxAVBXTooCzme/2CkVHFvrwTiUSQSqXVHA0REREJiT38RHpIJpMJHQIRERHVEhzDT7rHwQH48EPFkcqVn58vdAhEREQkICb8pHsaNgS2bFEcSS2pVIovvvgC7u7usLa2xt27dwEAgYGB2Lp1q8DRERERUU1iwk+659kz4No1xZHUWrx4MbZv347ly5fD1NRUWd66dWts2bJFwMiIiIiopjHhJ91z4wbQurXiSGrt3LkTmzdvhp+fH8RisbLcx8cHN2/eFDAyIiIiqmlM+MmgrV+/Hp6enjA3N0fXrl3x999/l1r32rVrGD58ODw9PSESibB69eoSdYofe/U2ZcoUZZ2UlBSMHTsWrq6usLKyQocOHbB//37l4/fv38eHH36IRo0awcLCAo0bN0ZwcDAkEkmFX1dSUhK8vb1LlMtkMhQWFla4HSIiItJ9TPjJYO3duxcBAQEIDg7GxYsX4ePjg379+iEtLU1t/by8PHh5eWHp0qVwdXVVW+fChQtITk5W3sLCwgAAI0aMUNYZN24cYmNjcfjwYVy5cgXDhg3DyJEjcenSJQDAzZs3IZPJ8O233+LatWtYtWoVNm3ahP/7v/+r8Gtr2bKl2o23fvrpJ7Rv377C7RAREZEekOu5zMxMOQB5Zmam0KGQtkRFyeWA4vgSTd/rLl26yKdMmaK8L5VK5W5ubvKQkJByn9uwYUP5qlWryq03ffp0eePGjeUymUxZZmVlJd+5c6dKvbp168q/++67UttZvny5vFGjRuWer9ihQ4fkdnZ28qVLl8otLS3lX331lfyjjz6Sm5qayn///fcKt0NERES6jz38pHtEIsDUVHGsJIlEgqioKPj6+irLjIyM4Ovri/Pnz2sjSkgkEuzatQsTJ06E6KVYu3fvjr179yI9PR0ymQx79uxBfn4+evfuXWpbmZmZqFu3boXP/e677+KXX35BeHg4rKysEBQUhBs3buCXX35B3759q/KyiIiISMfo3cZbBQUFKCgoUN7PysoSMBqqFu3bAy+9x6969T03MzODmZmZStnjx48hlUrh4uKiUu7i4qK1Sa2HDh1CRkYGxo8fr1K+b98+jBo1Cg4ODjA2NoalpSUOHjyodsw9ANy5cwdr167FihUrNDr/66+/rhxSRERERIZL73r4Q0JCYGdnp7x5eHgIHRLVMA8PD5VrICQkRJA4tm7digEDBsDNzU2lPDAwEBkZGQgPD8c///yDgIAAjBw5EleuXCnRRlJSEvr3748RI0Zg0qRJNRU6ERER6RG96+GfN28eAgIClPezsrKY9OubGzcAPz9g926gRYsSDycmJsLW1lZ5/9XefQBwdHSEWCxGamqqSnlqamqpE3I1ER8fj/DwcBw4cEClPC4uDuvWrcPVq1fRqlUrAIqlMs+cOYP169dj06ZNyroPHz5Enz590L17d2zevLncc9apU0dl6FBZ0tPTNXg1REREpMv0LuFXN3yD9MyzZ8ClS6VuvGVra6uS8KtjamqKjh07IiIiAkOGDAGgWLIyIiICU6dOrXKIoaGhcHZ2xsCBA1XK8/LyACjmC7xMLBZDJpMp7yclJaFPnz7o2LEjQkNDS9RX5+VlQp88eYIvv/wS/fr1Q7du3QAA58+fx2+//YbAwMDKviwiIiLSQXqX8BNVVEBAAPz9/dGpUyd06dIFq1evRm5uLiZMmABAsXymu7u7ckiQRCLB9evXlT8nJSUhOjoa1tbWKuPvZTIZQkND4e/vD2Nj1X9izZs3h7e3NyZPnowVK1bAwcEBhw4dQlhYGI4cOQJAkez37t0bDRs2xIoVK/Do0SPl88v69sHf31/58/Dhw7Fo0SKVDy/Tpk3DunXrEB4ejpkzZ1b210ZEREQ6hgk/GaxRo0bh0aNHCAoKQkpKCtq1a4dff/1VOZE3ISFBpWf94cOHKmvYr1ixAitWrECvXr1w8uRJZXl4eDgSEhIwceLEEuc0MTHBsWPHMHfuXAwaNAg5OTnw9vbGjh078M477wAAwsLCcOfOHdy5cwf169dXeb5cLq/Qa/vtt9+wbNmyEuX9+/fH3LlzK9QGERER6QeRvKIZhI7KysqCnZ0dMjMzyx3mQTri4kWgY0cgKgro0EFZzPf6hYYNG2LatGn49NNPVcpXrlyJNWvWID4+XqDIiIiIqKaxh590T6NGwL59iiOptXDhQnz00Uc4efIkunbtCgD466+/8Ouvv+K7774TODoiIiKqSezhJ73B91rVX3/9hTVr1uDGjRsAgBYtWmDatGnKDwBERERkGJjwk+5JTVUsyennB7y0cRbfayIiIqKS9G7jLTIASUnAp58qjkRERERUJib8RERERER6jAk/EREREZEeY8JPRERERKTHmPCT7rGzAwYNUhyJiIiIqExch590T+PGwOHDQkdR6wwbNqzCdQ8cOFCNkRAREVFtwoSfdE9hIZCRAdjbAyYmQkdTa9jxGw8iIiJSgwk/6Z4rV4COHYGoKKBDB6GjqTVCQ0OFDoGIiIhqIY7hJyIiIiLSY+zhJ9JTP/30E/bt24eEhARIJBKVxy5evChQVERERFTT2MNPpIfWrFmDCRMmwMXFBZcuXUKXLl3g4OCAu3fvYsCAAUKHR0RERDWICT+RHtqwYQM2b96MtWvXwtTUFLNnz0ZYWBimTZuGzMxMocMjIiKiGsSEn3SPjw+Qmak4kloJCQno3r07AMDCwgLZ2dkAgLFjx+LHH38UMjQiIiKqYUz4SfeIxYCtreJIarm6uiI9PR0A0KBBA/z5558AgHv37kEulwsZGhEREdUwJvyke27fBvr1UxxJrTfffBOHn29ONmHCBMycORN9+/bFqFGjMHToUIGjIyIioprEVXpI92RnA7//rjiSWps3b4ZMJgMATJkyBQ4ODjh37hwGDx6MyZMnCxwdERER1SQm/ER6yMjICEZGL77AGz16NEaPHi1gRERERCQUJvxEeiImJgatW7eGkZERYmJiyqzbtm3bGoqKiIiIhMaEn0hPtGvXDikpKXB2dka7du0gEonUTtAViUSQSqUCREhERERCYMJPusfDA1i3TnEkpXv37sHJyUn5MxERERHAhJ90kZMTMGWK0FHUOg0bNlT+HB8fj+7du8PYWPWfeFFREc6dO6dSl4iIiPQbl+Uk3ZOeDuzapTiSWn369FGuw/+yzMxM9OnTR4CIiIiISChM+En33L8PjB2rOJJacrkcIpGoRPmTJ09gZWUlQEREREQkFA7pIdIjw4YNA6CYmDt+/HiYmZkpH5NKpYiJiUH37t2FCo+IiIgEwISfSI/Y2dkBUPTw29jYwMLCQvmYqakpXnvtNUyaNEmo8IiIiEgATPiJ9EhoaKhyKc61a9fC2tpa4IiIiIhIaBzDT7rHygp47TXFkUqQy+XYvXs3kpOThQ6FiIiIagEm/KR7mjUDzp9XHKkEIyMjNGnSBE+ePBE6FCIiIqoFmPAT6aGlS5di1qxZuHr1qtChEBERkcBE8uIBv3oqKysLdnZ2yMzMhK2trdDhkDZcvAh07AhERQEdOiiL+V6/UKdOHeTl5aGoqAimpqYqk3cBqF2jn4iIiPQTJ+0S6aHVq1cLHQIRERHVEkz4ifSQv7+/0CEQERFRLcGEn0jP5efnQyKRqJQZ+pAnIiIiQ8JJu0R6KDc3F1OnToWzszOsrKxQp04dlRsREREZDib8pHtatgRu31Ycq2j9+vXw9PSEubk5unbtir///rvUuteuXcPw4cPh6ekJkUikdpx88WOv3qZMmaKsk5KSgrFjx8LV1RVWVlbo0KED9u/fr9LO4sWL0b17d1haWsLe3l7j1zV79mz88ccf2LhxI8zMzLBlyxYsXLgQbm5u2Llzp8btERERke5iwk+6x9wc8PZWHKtg7969CAgIQHBwMC5evAgfHx/069cPaWlpauvn5eXBy8sLS5cuhaurq9o6Fy5cQHJysvIWFhYGABgxYoSyzrhx4xAbG4vDhw/jypUrGDZsGEaOHIlLly4p60gkEowYMQIff/xxpV7bL7/8gg0bNmD48OEwNjbG66+/jvnz52PJkiXYvXt3pdokIiIi3cSEn3TPvXvAmDGKYxV8/fXXmDRpEiZMmICWLVti06ZNsLS0xLZt29TW79y5M7766iuMHj0aZmZmaus4OTnB1dVVeTty5AgaN26MXr16KeucO3cO//3vf9GlSxd4eXlh/vz5sLe3R1RUlLLOwoULMXPmTLRp06ZSry09PR1eXl4AFOP1i5fh7NmzJ06fPl2pNomIiEg36V3CX1BQgKysLJUb6ZmnT4HduxVHNV59/wsKCkrUkUgkiIqKgq+vr7LMyMgIvr6+OH/+vFbClEgk2LVrFyZOnAiRSKQs7969O/bu3Yv09HTIZDLs2bMH+fn56N27t1bOCwBeXl649/wDUfPmzbFv3z4Aip7/ygwRIiIiIt2ldwl/SEgI7OzslDcPDw+hQ6Ia5uHhoXINhISElKjz+PFjSKVSuLi4qJS7uLggJSVFK3EcOnQIGRkZGD9+vEr5vn37UFhYCAcHB5iZmWHy5Mk4ePAgvL29tXJeAJgwYQIuX74MAJg7dy7Wr18Pc3NzzJw5E7NmzdLaeYiIiKj207tlOefNm4eAgADl/aysLCb9BiYxMVFl2cnSht9Ut61bt2LAgAFwc3NTKQ8MDERGRgbCw8Ph6OiIQ4cOYeTIkThz5kylh/C8aubMmcqffX19cfPmTURFRcHb2xtt27bVyjmIiIhIN+hdwm9mZqaS4MnlcgDg0B59kpPz4vjS+1r8HtvY2JS7zryjoyPEYjFSU1NVylNTU0udkKuJ+Ph4hIeH48CBAyrlcXFxWLduHa5evYpWrVoBAHx8fHDmzBmsX78emzZtqtJ5ZTIZvvrqKxw+fBgSiQRvvfUWgoOD0bBhQzRs2LBKbRMREZFu0ruE/1XZ2dkAwF5+ffTSRNiXZWdnw87OrsynmpqaomPHjoiIiMCQIUMAKJLliIgITJ06tcqhhYaGwtnZGQMHDlQpz8vLA6CYL/AysVgMmUxW5fMuXrwYCxYsgK+vLywsLPDNN98gLS2t1InIREREpP/0PuF3c3NDYmIibGxsVCZOkv6Ry+XIzs4uMYSmNAEBAfD390enTp3QpUsXrF69Grm5uZgwYQIAxfKZ7u7uyjkAEokE169fV/6clJSE6OhoWFtbq4y/l8lkCA0Nhb+/P4yNVf+JNW/eHN7e3pg8eTJWrFgBBwcHHDp0CGFhYThy5IiyXkJCAtLT05GQkACpVIro6GgAgLe3N6ytrUt9TTt37sSGDRswefJkAEB4eDgGDhyILVu2lPiQQURERIZBJC8e80JkgNatW4evvvoKKSkpaNeuHdasWYOuXbsCAHr37g1PT09s374dAHD//n00atSoRBu9evXCyZMnlfd///139OvXD7GxsWjatGmJ+rdv38bcuXMRGRmJnJwceHt747PPPsPYsWOVdcaPH48dO3aUeO6JEyfKXM3HzMwMd+7cUflGy9zcHHfu3EH9+vXL+3UQERGRHmLCT6RHxGIxUlJS4OTkpCyzsbFBTEyM2g8rtZGnpydmzJiBGTNm1Mr2iIiIdI3eD+khMiRyuRzjx49Xmbien5+P//znP7CyslKWvTqZmIiIiPQXE34iPeLv71+ibMyYMQJEQkRERLUFZ/ER6ZHQ0NAStxMnTsDHx0elrF27dliwYAHkcjkWLFiABg0awMzMDG5ubpg2bZqyvYKCAnz22Wdwd3eHlZUVunbtqjJfoSzbt2+Hvb09jhw5gmbNmsHS0hLvvfce8vLysGPHDnh6eqJOnTqYNm0apFJpqe0kJCTg3XffhbW1NWxtbTFy5MgSy6n+8ssv6Ny5M8zNzeHo6IihQ4eW2t6WLVtgb2+PiIiICr0OIiIiXccefiIDtn//fqxatQp79uxBq1atkJKSotyhFwCmTp2K69evY8+ePXBzc8PBgwfRv39/XLlyBU2aNCm3/by8PKxZswZ79uxBdnY2hg0bhqFDh8Le3h7Hjh3D3bt3MXz4cPTo0QOjRo0q8XyZTKZM9k+dOoWioiJMmTIFo0aNUn7wOHr0KIYOHYrPP/8cO3fuhEQiwbFjx9TGs3z5cixfvhy///47unTpUrlfGhERkY5hwk9kwBISEuDq6gpfX1+YmJigQYMGykQ4ISEBoaGhSEhIUC51+tlnn+HXX39FaGgolixZUm77hYWF2LhxIxo3bgwAeO+99/D9998jNTUV1tbWaNmyJfr06YMTJ06oTfgjIiJw5coV3Lt3T7ny0M6dO9GqVStcuHABnTt3xuLFizF69GgsXLhQ+TwfH58Sbc2ZMwfff/89Tp06pdz0jIiIyBBwSA+RARsxYgSePXsGLy8vTJo0CQcPHkRRUREA4MqVK5BKpWjatCmsra2Vt1OnTiEuLq5C7VtaWiqTfQBwcXGBp6enyl4CLi4uSEtLU/v8GzduwMPDQ2WZ0ZYtW8Le3h43btwAAERHR+Ott94qM46VK1fiu+++Q2RkJJN9IiIyOEz4ifSckZERXl19t7CwEIBiB+rY2Fhs2LABFhYW+OSTT/DGG2+gsLAQOTk5EIvFiIqKQnR0tPJ248YNfPPNNxU6t4mJicp9kUiktqwquwxbWFiUW+f111+HVCrFvn37Kn0eIiIiXcWEn0jPOTk5ITk5WXk/KysL9+7dU963sLDAoEGDsGbNGpw8eRLnz5/HlStX0L59e0ilUqSlpcHb21vl5urqWiOxt2jRAomJiUhMTFSWXb9+HRkZGWjZsiUAoG3btuVOwO3SpQuOHz+OJUuWYMWKFdUaMxERUW3DMfxEeu7NN9/E9u3bMWjQINjb2yMoKAhisRiAYiUdqVSKrl27wtLSErt27YKFhQUaNmwIBwcH+Pn5Ydy4cVi5ciXat2+PR48eISIiAm3btsXAgQOrPXZfX1+0adMGfn5+WL16NYqKivDJJ5+gV69e6NSpEwAgODgYb731Fho3bozRo0ejqKgIx44dw5w5c1Ta6t69O44dO4YBAwbA2NiYG3EREZHBYA8/kZ6bN28eevXqhX/9618YOHAghgwZohxXb29vj++++w49evRA27ZtER4ejl9++QUODg4AFMt8jhs3Dp9++imaNWuGIUOG4MKFC2jQoEGNxC4SifDzzz+jTp06eOONN+Dr6wsvLy/s3btXWad379743//+h8OHD6Ndu3Z488038ffff6ttr2fPnjh69Cjmz5+PtWvX1shrICIiEppI/urgXiIiIiIi0hvs4SciIiIi0mNM+ImoUgYMGKCyXOfLt4qs0U9EREQ1g0N6iKhSkpKS8OzZM7WP1a1bF3Xr1q3hiIiIiEgdJvxERERERHqMQ3qIiKpo+/btsLe3FzoMIiIitZjwE1UDkUhU5m3BggVCh0hqjB8/Xu37defOHaFDIyIiqjRuvEVUDV7e2Xbv3r0ICgpCbGysssza2lr5s1wuh1QqhbEx/znWBv3790doaKhKmZOTk0DREBERVR17+Imqgaurq/JmZ2cHkUikvH/z5k3Y2Njg+PHj6NixI8zMzBAZGYnx48djyJAhKu3MmDEDvXv3Vt6XyWQICQlBo0aNYGFhAR8fH/z00081++L0nJmZmcr75+rqim+++QZt2rSBlZUVPDw88MknnyAnJ6fUNi5fvow+ffrAxsYGtra26NixI/755x/l45GRkXj99ddhYWEBDw8PTJs2Dbm5uTXx8oiIyAAx4ScSyNy5c7F06VLcuHEDbdu2rdBzQkJCsHPnTmzatAnXrl3DzJkzMWbMGJw6daqaozVsRkZGWLNmDa5du4YdO3bgjz/+wOzZs0ut7+fnh/r16+PChQuIiorC3LlzYWJiAgCIi4tD//79MXz4cMTExGDv3r2IjIzE1KlTa+rlEBGRgeEYAiKBLFq0CH379q1w/YKCAixZsgTh4eHo1q0bAMDLywuRkZH49ttv0atXr+oK1aAcOXJEZcjVgAED8L///U9539PTE19++SX+85//YMOGDWrbSEhIwKxZs9C8eXMAQJMmTZSPhYSEwM/PDzNmzFA+tmbNGvTq1QsbN26Eubl5NbwqIiIyZEz4iQTSqVMnjerfuXMHeXl5JT4kSCQStG/fXpuhGbQ+ffpg48aNyvtWVlYIDw9HSEgIbt68iaysLBQVFSE/Px95eXmwtLQs0UZAQAA++ugjfP/99/D19cWIESPQuHFjAIrhPjExMdi9e7eyvlwuh0wmw71799CiRYvqf5FERGRQmPATCcTKykrlvpGREV7dFqOwsFD5c/GY8aNHj8Ld3V2lnpmZWTVFaXisrKzg7e2tvH///n3861//wscff4zFixejbt26iIyMxIcffgiJRKI24V+wYAE++OADHD16FMePH0dwcDD27NmDoUOHIicnB5MnT8a0adNKPK9BgwbV+tqIiMgwMeEnqiWcnJxw9epVlbLo6Gjl2O+WLVvCzMwMCQkJHL5Tg6KioiCTybBy5UoYGSmmPe3bt6/c5zVt2hRNmzbFzJkz8f777yM0NBRDhw5Fhw4dcP36dZUPFURERNWJk3aJaok333wT//zzD3bu3Inbt28jODhY5QOAjY0NPvvsM8ycORM7duxAXFwcLl68iLVr12LHjh0CRq7fvL29UVhYiLVr1+Lu3bv4/vvvsWnTplLrP3v2DFOnTsXJkycRHx+Ps2fP4sKFC8qhOnPmzMG5c+cwdepUREdH4/bt2/j55585aZeIiKoNE36iWqJfv34IDAzE7Nmz0blzZ2RnZ2PcuHEqdb744gsEBgYiJCQELVq0QP/+/XH06FE0atRIoKj1n4+PD77++mssW7YMrVu3xu7duxESElJqfbFYjCdPnmDcuHFo2rQpRo4ciQEDBmDhwoUAgLZt2+LUqVO4desWXn/9dbRv3x5BQUFwc3OrqZdEREQGRiR/ddAwERERERHpDfbwExERERHpMSb8RERERER6jAk/EREREZEeY8JPRERERKTHmPATEREREekxJvxERERERHqMCT8RERERkR5jwk9USxQUFGDBggUoKCgQOhSqBL5/5SsqKkJ4eDi+/fZbZGdnAwAePnyInJwcgSMjItJv3HiLqJbIysqCnZ0dMjMzYWtrK3Q4pCG+f2WLj49H//79kZCQgIKCAty6dQteXl6YPn06CgoKsGnTJqFDJCLSW+zhJyKiajd9+nR06tQJT58+hYWFhbJ86NChiIiIEDAyIiL9Zyx0AEREpP/OnDmDc+fOwdTUVKXc09MTSUlJAkVFRGQY9D7hl8lkePjwIWxsbCASiYQOh6qRXC5HdnY23NzcYGRUu7+8UnddZmVlqRxJt5T2/unSdVmdZDIZpFJpifIHDx7AxsZGgIiIiAyH3o/hf/DgATw8PIQOg2pQYmIi6tevL3QYZeJ1aXh04bqsTqNGjYKdnR02b94MGxsbxMTEwMnJCe+++y4aNGiA0NBQoUMkItJbep/wZ2Zmwt7eXnnf99jkSrVz/0ndSj2vIN2i/EpqmDzRzpcvZk8q/1zLx5W7NCweFVbqeWbJ2ZV6HgBE3Fip/DkjIwN2dnaVbqsmFF+XiYmJLyZ4RkcDvXoBp04B7doJGR5pUVZWFjw8PHTiuqxODx48QL9+/SCXy3H79m106tQJt2/fhqOjI06fPg1nZ2ehQyQi0lt6P6Tn5WE8nQ/4wdjKTOM27j12gNhS83PnP7GAUeXyfRiZa+etKXQHzB9X7rkF7oDlI82Tfom7OSxTNU/6pR7mMHtYueEs7Z0/wqW0LQCgE0O3imO0tbV9kfBbW784cpUXvaML12V1ql+/Pi5fvoy9e/fi8uXLyMnJwYcffgg/Pz+VSbxERKR9et/DX7xUnu+xyZVO9isj/0nl/wMzeVw9n8Mqm/hXJukHUKmkv1hlEv8iaQEibqzUiWUR1S7hePEi0LEjEBUFdOggbICkNVyuk4iIhGa4M8gqQJ+SfQDId6zc8/KcKtczmedigjwXk0o9t8DNABMjGxvg7bcVRyI9ExISgm3btpUo37ZtG5YtWyZAREREhoMJfyn0LdkvVtNJPwAm/RXVpAnw22+KI5Ge+fbbb9G8efMS5a1ateKmW0RE1YwJvxr6muwXq0rSX5Xe/sowqKRfKgWyshRHIj2TkpKCevXqlSh3cnJCcnKyABERERkOJvyv0Pdkv1i+ozBDfCqjwM3WMBL/y5cBOzvFkUjPeHh44OzZsyXKz549Czc3NwEiIiIyHHq/So8mDCXZf1m+Y+Um8+Y5iSo1mbc46a/MhN4CN9tKr+JDRMKaNGkSZsyYgcLCQrz55psAgIiICMyePRuffvqpwNEREek3JvzPVTbZrwqhk/1iNZ30A4rEn0k/keGYNWsWnjx5gk8++QQSiQQAYG5ujjlz5mDevHkCR0dEpN9qR8YpsKok+5Xt3a8tyX6xqiT9QOWW7mTST2Q4RCIRli1bhsDAQNy4cQMWFhZo0qQJzMw0Xy6ZiIg0U7uyTgEw2X+hskk/ULUhPpVN+oHKrddPRMKxtrZG586dhQ6DiMig1M7Ms4Yw2S+peCJvTY/rr+wmXXrV29+mDZCWBtjbCx0Jkdbl5uZi6dKliIiIQFpaGmQymcrjd+/eFSgyIiL9V7uzz2rEZL9snMwrABMTwMlJ6CiIqsVHH32EU6dOYezYsahXrx5Eosrv7UFERJrRnQxUi5jsV4yujesXJz7S+Hm1SlwcMHMmsGoV0Lix0NEQadXx48dx9OhR9OjRQ+hQiIgMjsGtw89kXzOVXasfEGC9/no2lXperZGZCfzyi+JIpGfq1KmDunXrCh0GEZFBMriEv7IMMdkvpkubdBFR7fTFF18gKCgIeXl5QodCRGRwdD8b1UBNb6ylD8n+y3RpXD8R1S4rV65EXFwcXFxc4OnpCRMT1Q/1Fy9eFCgyIiL9p18ZaRnuP6kLsaXmz2Oyr0qXNukiotpjyJAhQodARGSw9DMr1RIm++rp0mReneLuDqxcqTiSQVu/fj2++uorpKSkwMfHB2vXrkWXLl3U1r127RqCgoIQFRWF+Ph4rFq1CjNmzFCp4+npifj4+BLP/eSTT7B+/XoAQEpKCmbNmoWwsDBkZ2ejWbNm+PzzzzF8+HBl/cGDByM6OhppaWmoU6cOfH19sWzZMri5uZX7moKDgzX4DRARkTZxDH8pmOyXjeP6q4GLCxAQoDiSwdq7dy8CAgIQHByMixcvwsfHB/369UNaWpra+nl5efDy8sLSpUvh6uqqts6FCxeQnJysvIWFhQEARowYoawzbtw4xMbG4vDhw7hy5QqGDRuGkSNH4tKlS8o6ffr0wb59+xAbG4v9+/cjLi4O7733XoVfW0ZGBrZs2YJ58+YhPT0dgGIoT1JSUoXbICIizYnkcnnlxlnoiKysLNjZ2cH7+3kQW5pX6DmVTfYBw0n4X1bZ3XkrO8QHUD+uv6goH5F/LEBmZiZsbW0r3XZNKL4uVWJ9+hQIDwd8fYE6dYQNkLRG7Xtdhq5du6Jz585Yt24dAEAmk8HDwwP//e9/MXfu3DKf6+npiRkzZpTo4X/VjBkzcOTIEdy+fVu5Hr61tTU2btyIsWPHKus5ODhg2bJl+Oijj9S2c/jwYQwZMgQFBQUlxuS/KiYmBr6+vrCzs8P9+/cRGxsLLy8vzJ8/HwkJCdi5c2eZzyciospjD/8rmOxrrqZ7+gHd6+0vKChAVlaWyq2Ee/eAkSMVR9I7r77/BQUFJepIJBJERUXB19dXWWZkZARfX1+cP39eK3FIJBLs2rULEydOVNn8qnv37ti7dy/S09Mhk8mwZ88e5Ofno3fv3mrbSU9Px+7du9G9e/dyk30ACAgIwPjx43H79m2Ym7/ofHnnnXdw+vTpKr8uIiIqHRP+lzDZr7yqJP2GMMQnJCQEdnZ2ypuHh4fQIVEN8/DwULkGQkJCStR5/PgxpFIpXF4Z1uXi4oKUlBStxHHo0CFkZGRg/PjxKuX79u1DYWEhHBwcYGZmhsmTJ+PgwYPw9vZWqTdnzhxYWVnBwcEBCQkJ+Pnnnyt03gsXLmDy5Mklyt3d3bX22oiISD0m/M8x2a86Xdqkq6bNmzcPmZmZyltiYqLQIVENS0xMVLkG5s2bJ0gcW7duxYABA0pMtA0MDERGRgbCw8Pxzz//ICAgACNHjsSVK1dU6s2aNQuXLl3C77//DrFYjHHjxqEiI0PNzMzUfrN169YtODk5Ve1FERFRmZipgsm+NhUn/TW9Xn9tX8HHzMwMZmZmQodBArK1tS13DL+joyPEYjFSU1NVylNTU0udkKuJ+Ph4hIeH48CBAyrlcXFxWLduHa5evYpWrVoBAHx8fHDmzBmsX78emzZtUonR0dERTZs2RYsWLeDh4YE///wT3bp1K/PcgwcPxqJFi7Bv3z4AgEgkQkJCAubMmaOyEhAREWmfwffwM9mvHkKs4PPMSTd6+0tlYQG0b684kkEyNTVFx44dERERoSyTyWSIiIgoN6GuiNDQUDg7O2PgwIEq5cW73xoZqf6XIBaLIZPJSm2v+DF18xFetXLlSuTk5MDZ2RnPnj1Dr1694O3tDRsbGyxevFjTl0JERBrQiYz1xIkT6NOnj9rH1q9fjylTplSqXSb71UuI9fp1WosWAHcbNXgBAQHw9/dHp06d0KVLF6xevRq5ubmYMGECAMXyme7u7so5ABKJBNevX1f+nJSUhOjoaFhbW6uMv5fJZAgNDYW/vz+MjVX/fjVv3hze3t6YPHkyVqxYAQcHBxw6dAhhYWE4cuQIAOCvv/7ChQsX0LNnT9SpUwdxcXEIDAxE48aNK/RhxM7ODmFhYYiMjERMTAxycnLQoUMHlQnKRERUPXQiax02bBjCw8PRsWNHlfJvvvkGgYGBlU74K4vJfsVVNukHqrY7L5GuGjVqFB49eoSgoCCkpKSgXbt2+PXXX5UTeRMSElR64h8+fIj27dsr769YsQIrVqxAr169cPLkSWV5eHg4EhISMHHixBLnNDExwbFjxzB37lwMGjQIOTk58Pb2xo4dO/DOO+8AACwtLXHgwAEEBwcjNzcX9erVQ//+/TF//nyNhqv17NkTPXv21PTXQkREVaAT6/Bv2bIF//d//4fTp0+jefPmABRfDy9atAhHjhzB66+/XupzS1uHnxtr1bzqXq9fKslH1N7PdXcd/kuXgNdeA/78UzG0h/SCpuvw65M1a9ZUuO60adOqMRIiIsOmE9nrRx99hPT0dPj6+iIyMhJ79+7FkiVLcOzYMfTo0UPj9pjsC6MqQ3wMoqdfLgckEsWRSA+sWrVK5f6jR4+Ql5cHe3t7AIqddy0tLeHs7MyEn4ioGulMBjt79mw8efIEnTp1glQqxW+//YbXXntN43aY7AuLST+R4bj30iZyP/zwAzZs2ICtW7eiWbNmAIDY2FhMmjRJ7fr8RESkPbU2i1X3VbC7uzssLS3xxhtv4O+//8bff/8NoOJfBTPZrx04mZd0lUwmx+OcAjzIeIaHyls+kjKeIenpM2Q+K0TknD4qO9iSQmBgIH766Sdlsg8AzZo1w6pVq/Dee+/Bz89PwOiIiPRbrc1kX/0quJhYLMbZs2dx9uxZAIq1nCuS8BekW8CoEvk+k/3qIcR6/UTleSaRIkklmX+GpIx8JGXk4WFGPpIzn6FQWva1l5VfBDsLHV8ithokJyejqKioRLlUKi2x7wAREWlXrc1mX/4qWChM9qsfh/i8pEUL4OpVwMtL6Ej0kkwmx+PcAjzMyH8pmVf0zD/MVPTUp+dKym3HSAS42prDzd4C7nUs4GavuLnbK8qsTMU18Gp0z1tvvYXJkydjy5Yt6NChAwAgKioKH3/8MZfmJCKqZsxoS8Fkv+Yw6X/OwgJ4vsspaS6/UKocYvMw49krw26e4WFmPiRFpW8iVczKVAz3OhZwt385mbdQJvguNmYwFhv8noUa27Ztm3J/ARMTxTcgRUVF6NevH7Zs2SJwdERE+k0nstrhw4ejS5cumDNnjkr58uXLceHCBfzvf//T6vmY7Nc8jusHEB8PfPEFEBgINGwodDS1ilwux5NciTJ5f/D0RWKv6J1/hsc5Feudd3neO6+4mcPdXjW5tzU35hj8auDk5IRjx47h1q1buHnzJgDFhl9NmzYVODIiIv2nE5nt6dOnsWDBghLlAwYMwMqVK7V6Lib7wqnquH6zJO3GU+OePAG2bgU++cTgEv78QilSMtX1zOcrx9QXVKB33tJUrJK816+jSOrd7BT3Xe3MYcLeeUE1bdqUST4RUQ3Tiew2JycHpqamJcpNTEyQlZWltfMw2a8dKt3b78he2dpILpfjaV4hkp4+U50Qm/nseVk+HucUlNuOSAQ425i9SOZfSuyLe+rtLEzYO19LSaVSbN++HREREUhLS4NMpvoB7o8//hAoMiIi/acTGW6bNm2wd+9eBAUFqZTv2bMHLVu21Mo5mOzXLpVN+qnmSYpkSM4sTuZfmRD7/Of8wvJ75y1MxIrEvY6lYgKsnWpPvYutOUyN2Tuvq6ZPn47t27dj4MCBaN26NT+YERHVIJ3IcgMDAzFs2DDExcXhzTffBABERETgxx9/1Pr4fao9mPQLTy6XIyOvUKVnPumVoTaPcgoqtDlwce+8+/MeeZXJsPYWsLdk77w+27NnD/bt24d33nlH6FCIiAyOTiT8gwYNwqFDh7BkyRL89NNPsLCwQNu2bREeHo5evXpVuX327tdeBpX0u7gAc+cqjjVEUiRDataL5L14icqkl3rq8yTSctsxNzF6kcDbWby0ZKViqI2rnTnMjLlcpSEzNTWFt7e30GEQERkkncl0Bw4ciIEDB2q9XSb7tV9VJvPqFHd3ICREa83J5XJkPSt6kcyrOaZlV6x33tHaTLnOfMnlKs1R18qUvfNUpk8//RTffPMN1q1bx2uFiKiG6VS2GxUVhRs3bgAAWrVqhfbt21epPSb7ukXve/uzs4GoKKBjR8DGptzqhVJF7/zDl3aCVRl68/QZcivQO29qbKRM3F9N5ot7581N2DtPVRMZGYkTJ07g+PHjaNWqlXIt/mIHDhwQKDIiIv2nExlvWloaRo8ejZMnT8Le3h4AkJGRgT59+mDPnj1wcnLSuE0m+7pJr5P+27eBPn0USX+HDsjKL3wxzCbjxTCb4qQ+NSsfsgr0zjtYmSqG19i9svb8811iHdg7TzXA3t4eQ4cOFToMIiKDpBNZ73//+19kZ2fj2rVraNGiBQDg+vXr8Pf3x7Rp0/Djjz9q1B6Tfd2mr0n//qhEDAcwZfdFnP75EbILisp9jqnYSDkBtuRylYpy9s5TbRAaGip0CEREBksnMt9ff/0V4eHhymQfAFq2bIn169fj7bff1qgtJvv6QR/H9d9KzQEA3H+Si2wTRbJf18pUuXGUex3V8fNu9uZwtDKDkRF750k3FBUV4eTJk4iLi8MHH3wAGxsbPHz4ELa2trC2thY6PCIivaUT2a9MJisx3hNQbLz16uYtZWGyr3/0qbe/b0vF6jxfvNsatj26wt3eAham7J0n/RAfH4/+/fsjISEBBQUF6Nu3L2xsbLBs2TIUFBRg06ZNQodIRKS3dGIXmzfffBPTp0/Hw4cPlWVJSUmYOXMm3nrrrQq1YfKEyb6+Ku7t13WdmrgA7u7o4O0Mb2drJvukV6ZPn45OnTrh6dOnsLCwUJYPHToUERERAkZGRKT/dCILXrduHQYPHgxPT094eHgAABITE9G6dWvs2rVL4OioNsh3BEyShI6iitq0AR48EDoKompx5swZnDt3Dqampirlnp6eSErS9X+8RES1m04k/B4eHrh48SLCw8Nx8+ZNAECLFi3g6+srcGRUmxQ4CB0BEZVGJpNBKi25TOyDBw9gU4FlaImIqPJ0IuEHAJFIhL59+6Jv375Ch0JUPa5cAQYMAI4fV/T2E+mRt99+G6tXr8bmzZsBKP6m5+TkIDg4GO+8847A0RER6bdam/CvWbOmwnWnTZtWjZEQ1ZDCQiApSXEk0jMrV65Ev3790LJlS+Tn5+ODDz7A7du34ejoqPHSykREpJlam/CvWrWqQvVEIhETfiKiWq5+/fq4fPky9uzZg5iYGOTk5ODDDz+En5+fyiReIiLSvlqb8N+7d0/oEIiISIuMjY0xZswYocMgIjI4tTbhJyIi/RIbG4u1a9fixo0bABSLL0ydOhXNmzcXODIiIv2mEwm/XC7HTz/9hBMnTiAtLa3EZlsHDhwQKDIiLWrSBDhxQnEk0jP79+/H6NGj0alTJ3Tr1g0A8Oeff6JNmzbYs2cPhg8fLnCERET6SycS/hkzZuDbb79Fnz594OLiApFIJHRIRNpnYwP07i10FETVYvbs2Zg3bx4WLVqkUh4cHIzZs2cz4SciqkY6kfB///33OHDgAJduI/2WlASsWwdMnQq4uwsdDZFWJScnY9y4cSXKx4wZg6+++kqAiIiIDIeR0AFUhJ2dHby8vIQOg6h6paYCS5cqjkR6pnfv3jhz5kyJ8sjISLz++usCREREZDh0ood/wYIFWLhwIbZt28bl24iIdNDgwYMxZ84cREVF4bXXXgOgGMP/v//9DwsXLsThw4dV6hIRkfboRMI/cuRI/Pjjj3B2doanpydMTExUHr948aJAkRERUUV88sknAIANGzZgw4YNah8DFHurSKXSGo2NiEjf6UTC7+/vj6ioKIwZM4aTdqlUhQ5FQodARKV4dXU1IiKqOTqR8B89ehS//fYbevbsKXQoVEsVOhYBz4SOooocHIAPP1QcifRYfn4+zM3NhQ6DiMhg6MSkXQ8PD9ja2godBtVShY560rPfsCGwZYviSKRnpFIpvvjiC7i7u8Pa2hp3794FAAQGBmLr1q0CR0dEpN90IuFfuXIlZs+ejfv37wsdCtUyepPsA8CzZ8C1a4ojkZ5ZvHgxtm/fjuXLl8PU1FRZ3rp1a2zZskXAyIiI9J9OJPxjxozBiRMn0LhxY9jY2KBu3boqt4owe1LNQVKN06tkHwBu3ABat1YcifTMzp07sXnzZvj5+UEsFivLfXx8cPPmTQEjIyLSfzqR8K9evRqbN2/Gtm3bsG7dOqxatUrlRoZH75J9opesX78enp6eMDc3R9euXfH333+XWvfatWsYPnw4PD09IRKJsHr16hJ1ih979TZlyhRlnZSUFIwdOxaurq6wsrJChw4dsH//fuXj9+/fx4cffohGjRrBwsICjRs3RnBwMCQSSYVeU1JSEry9vUuUy2QyFBYWVqgNIiKqHJ2YtOvv71+hekuXLsV//vMf2Nvbq33c/DGQ76jFwEgQTPZJn+3duxcBAQHYtGkTunbtitWrV6Nfv36IjY2Fs7Nzifp5eXnw8vLCiBEjMHPmTLVtXrhwQWWpy6tXr6Jv374YMWKEsmzcuHHIyMjA4cOH4ejoiB9++AEjR47EP//8g/bt2+PmzZuQyWT49ttv4e3tjatXr2LSpEnIzc3FihUryn1dLVu2xJkzZ9DwlTkqP/30E9q3b1/RXw8REVWCTiT8FbVkyRKMHDmy1IQfYNKv65jsk777+uuvMWnSJEyYMAEAsGnTJhw9ehTbtm3D3LlzS9Tv3LkzOnfuDABqHwcAJycnlftLly5F48aN0atXL2XZuXPnsHHjRnTp0gUAMH/+fKxatQpRUVFo3749+vfvj/79+yvre3l5ITY2Fhs3bqxQwh8UFAR/f38kJSVBJpPhwIEDiI2Nxc6dO3HkyJFyn09ERJWnE0N6Kkoul1eonvnjag6EtK7QsUink/2CggJkZWWp3EoQiQBTU8WR9M6r739BQUGJOhKJBFFRUfD19VWWGRkZwdfXF+fPn9dKHBKJBLt27cLEiRNV9jTp3r079u7di/T0dMhkMuzZswf5+fno3bt3qW1lZmZWeB7Vu+++i19++QXh4eGwsrJCUFAQbty4gV9++QV9+/at6ssiIqIy6FUPvybY0687dDnRLxYSEoKFCxeWXal9e0BNEkj6wcPDQ+V+cHAwFixYoFL2+PFjSKVSuLi4qJS7uLhobWLroUOHkJGRgfHjx6uU79u3D6NGjYKDgwOMjY1haWmJgwcPqh13DwB37tzB2rVrK9S7X+z1119HWFhYVcInIqJKMNiEH2DSrwv0IdkHgHnz5iEgIEB5Pysrq0QCSPotMTFRZT8RMzMzQeLYunUrBgwYADc3N5XywMBAZGRkIDw8HI6Ojjh06BBGjhyJM2fOoE2bNip1k5KS0L9/f4wYMQKTJk2qyfCJiKgSDDrhp9pNX5J9QJHclZvg3bgB+PkBu3cDLVrUTGBUY2xtbcvdQNDR0RFisRipqakq5ampqXB1da1yDPHx8QgPD8eBAwdUyuPi4rBu3TpcvXoVrVq1AqBYLvPMmTNYv349Nm3apKz78OFD9OnTB927d8fmzZvLPF+dOnVUhg2VJT09XcNXQ0REFWXwCT97+WunyiT7ZnV1fMOqZ8+AS5e48ZYBMzU1RceOHREREYEhQ4YAUCxbGRERgalTp1a5/dDQUDg7O2PgwIEq5Xl5eQAU8wVeJhaLIZPJlPeTkpLQp08fdOzYEaGhoSXqv+rlJUKfPHmCL7/8Ev369UO3bt0AAOfPn8dvv/2GwMDAqrwsIiIqh14l/K+//josLCw0fh6T/tqlMsm+ucMzSPOqIRiiGhYQEAB/f3906tQJXbp0werVq5Gbm6tctWfcuHFwd3dHSEgIAMUk3OvXryt/TkpKQnR0NKytrVXG38tkMoSGhsLf3x/Gxqp/+ps3bw5vb29MnjwZK1asgIODAw4dOoSwsDDlCjpJSUno3bs3GjZsiBUrVuDRo0fK55f27cPLSyoPHz4cixYtUvngMm3aNKxbtw7h4eGlLilKRERVpzMJf1xcHEJDQxEXF4dvvvkGzs7OOH78OBo0aKD8CvrYsWOVbp9Jf+2gabJv7sDecNIvo0aNwqNHjxAUFISUlBS0a9cOv/76q3Iib0JCgkrP+sOHD1XWsV+xYgVWrFiBXr164eTJk8ry8PBwJCQkYOLEiSXOaWJigmPHjmHu3LkYNGgQcnJy4O3tjR07duCdd94BAISFheHOnTu4c+cO6tevr/L8iqyQ9ttvv2HZsmUlyvv371/qcqJERKQdInlF17IU0KlTpzBgwAD06NEDp0+fxo0bN+Dl5YWlS5fin3/+wU8//VTqc7OysmBnZ4cWU5ZAbGZe7rmY9Aunqsm+NC8fd8aGIDMzs9yx0kIrvi5VYr14EejYEYiKAjp0EDZA0hq177UBatiwIaZNm4ZPP/1UpXzlypVYs2YN4uPjBYqMiEj/6UQP/9y5c/Hll18iICAANjY2yvI333wT69atEzAy0hb27ANo1AjYt09xJNIzCxcuxEcffYSTJ0+ia9euAIC//voLv/76K7777juBoyMi0m86kfBfuXIFP/zwQ4lyZ2dnPH6s3V20OLSn5jHZf65OHWDECKGjIKoW48ePR4sWLbBmzRrlKkEtWrRAZGSk8gMAERFVD51I+O3t7ZGcnIxGr/R8Xrp0Ce7u7lo/H5P+msNk/yWpqYolOf38gFc2XiLSB127dsXu3buFDoOIyOCUvaZaLTF69GjMmTMHKSkpEIlEkMlkOHv2LD777DOMGzeuWs5prt0vDkgNJvuvSEoCPv1UcSQiIiLSEp1I+JcsWYLmzZvDw8MDOTk5aNmyJd544w10794d8+fPr1Ablo81n5vMpL/6MNknIiIiqhk6MaTH1NQU3333HYKCgnDlyhXk5OSgffv2aNKkiUbtWD6SI8+pYrs+UvVhsk9ERERUc3Qi4S/m4eEBDw8PSKVSXLlyBU+fPkWdOnU0akPTpJ/j+bWLyT4RERFRzdKJIT0zZszA1q1bAQBSqRS9evVChw4d4OHhobKxTEVZPtJseA+H9mhHdSf7ng7pGtWvdezsgEGDFEciIiIiLdGJHv6ffvoJY8aMAQD88ssvuHv3Lm7evInvv/8en3/+Oc6ePatxm+zpr1nVnew3cnyColyNnlL7NG4MHD4sdBREWjNs2LAK1y1eqpOIiLRPJxL+x48fw9XVFQBw7NgxjBw5Ek2bNsXEiRPxzTff1FgcTPo1p2miD1Qu2dcLhYVARgZgbw+YmAgdDVGV2fHbKiKiWkEnEn4XFxdcv34d9erVw6+//oqNGzcCAPLy8iAWiyvdbmUm8TLprzgm+xq6cgXo2BGIigI6dBA6GqIqCw0NFToEIiKCjiT8EyZMwMiRI1GvXj2IRCL4+voCUGzL3rx58yq1zZV7qgeTfSIiIqLaQScS/gULFqBNmzZISEjAiBEjYGZmBgAQi8WYN29eldvneH7tYrJPROr89NNP2LdvHxISEiCRSFQeu3jxokBRERHpP51I+BctWqT8edu2bSqPxcfHY/DgwVU+B5N+7WCyT0TqrFmzBp9//jnGjx+Pn3/+GRMmTEBcXBwuXLiAKVOmCB0eEZFe04mE/+DBgyr3CwsLce/ePRgbG6Nx48YICgoSJC4m/aqY7BNRaTZs2IDNmzfj/fffx/bt2zF79mx4eXkhKCgI6ek6vqQuEVEtpxMJ/6VLl0qUZWVlYfz48Rg6dKjWzsNJvJVXExtq6X2y7+MDZGYCVlZCR0KkdQkJCejevTsAwMLCAtnZ2QCAsWPH4rXXXsO6deuEDI+ISK/pxMZb6tja2mLhwoUIDAzUaruabspFTPa1RiwGbG0VRyI94+rqquzJb9CgAf78808AwL179yCX8+8uEVF10tmEHwAyMzORmZmp9Xa5E2/FMdnXotu3gX79FEciPfPmm2/i8PON5SZMmICZM2eib9++GDVqlFa/qSUiopJ0YkjPmjVrVO7L5XIkJyfj+++/x4ABA6rlnJzEWz4m+1qWnQ38/rviSKRnNm/eDJlMBgCYMmUKHBwccO7cOQwePBiTJ08WODoiIv2mEwn/qlWrVO4bGRnByckJ/v7+FV6W0+JRISTu5hqdl0l/6Wpjst/SNgXhGp+FiGqCkZERjIxefKk8evRojB49WsCIiIgMh04k/Pfu3dNKO5aphchzMdFKW6UxhKS/Nib7beweoiBH49MQUTWKiYlB69atYWRkhJiYmDLrtm3btoaiIiIyPDqR8GuTpkk/d+JVVd3JfmWG8LSxe6jxc4io+rVr1w4pKSlwdnZGu3btIBKJ1E7QFYlEkEqlAkRIRGQYDC7hB6o/6dfXXn4m+9XMwwNYt05xJNID9+7dg5OTk/JnIiIShkEm/JVh6Ek/k/0a4OQEcMdR0iMNGzZU/hwfH4/u3bvD2Fj1v52ioiKcO3dOpS4REWmXTi/LWRWWqYWaP8dAl+tksl9D0tOBXbsURyI906dPH7U76mZmZqJPnz4CREREZDgMNuEHKpf0a0rXk34m+zXo/n1g7FjFkUjPyOVyiEQlvyV98uQJrLi7NBFRtTL4IT01MYlXV4f3MNknoqoaNmwYAMXE3PHjx8PMzEz5mFQqRUxMDLp37y5UeEREBsHgE36AK/e8StNEH6j+ZJ+JPpFusrOzA6Do4bexsYGFhYXyMVNTU7z22muYNGmSUOERERkEJvzPceUeBSb7RKRNoaGhyqU4165dC2tra4EjIiIyPAY9hr+q9G0SL5N9gVlZAa+9pjgS6RG5XI7du3cjOTlZ6FCIiAwSE/6XGPIkXib7tUCzZsD584ojkR4xMjJCkyZN8OSJ5vN2iIio6pjwv0LTpF/TXn6g9iX9+pLsd7C6r/FziKhmLF26FLNmzcLVq1eFDoWIyOBwDL8ahjSJt7pX4gFqJtnvbH0XedkaP612uXgR6NgRiIoCOnQQOhoirRo3bhzy8vLg4+MDU1NTlcm7ANSu0U9ERNrBhL8UhjCJVx+S/c7WdzWqT0TCWL16tdAhEBEZLINJ+M2SsyH1MK/Wc+hS0s9kn4hqkr+/v9AhEBEZLIMaw2/2MEuj+pWZxKsLK/cw2SciIeXn5yMrK0vlRkRE1cegEn6gZpJ+TdVk0s9kn4iEkJubi6lTp8LZ2RlWVlaoU6eOyo2IiKqPwSX8laEvK/cw2a/lWrYEbt9WHMmgrV+/Hp6enjA3N0fXrl3x999/l1r32rVrGD58ODw9PSESidSOlS9+7NXblClTlHVSUlIwduxYuLq6wsrKCh06dMD+/ftV2lm8eDG6d+8OS0tL2Nvba/SaZs+ejT/++AMbN26EmZkZtmzZgoULF8LNzQ07d+7UqC0iItKMQSb8mvbyAzWT9Fen6k72Gzk+YbJfVebmgLe34kgGa+/evQgICEBwcDAuXrwIHx8f9OvXD2lpaWrr5+XlwcvLC0uXLoWrq6vaOhcuXEBycrLyFhYWBgAYMWKEss64ceMQGxuLw4cP48qVKxg2bBhGjhyJS5cuKetIJBKMGDECH3/8scav65dffsGGDRswfPhwGBsb4/XXX8f8+fOxZMkS7N69W+P2iIio4gwy4Qcql/RrqraM56+JZF9T1ZHsWz0o0DiOWuXePWDMGMWRDNbXX3+NSZMmYcKECWjZsiU2bdoES0tLbNu2TW39zp0746uvvsLo0aNhZmamto6TkxNcXV2VtyNHjqBx48bo1auXss65c+fw3//+F126dIGXlxfmz58Pe3t7REVFKessXLgQM2fORJs2bTR+Xenp6fDy8gIA2NraKpfh7NmzJ06fPq1xe0REVHEGm/ADhjGJ11CSfQCoF5GpcSw1paCgoPxJik+fArt3K46kd159/wsKSn5AlUgkiIqKgq+vr7LMyMgIvr6+OH/+vFbikEgk2LVrFyZOnAiR6MWqYt27d8fevXuRnp4OmUyGPXv2ID8/H71799bKeb28vHDv+YfZ5s2bY9++fQAUPf+aDg8iIiLNGHTCD+j3JF5DSvYBwD08Q8Noak5ISAjs7OyUNw8PD6FDohrm4eGhcg2EhISUqPP48WNIpVK4uLiolLu4uCAlJUUrcRw6dAgZGRkYP368Svm+fftQWFgIBwcHmJmZYfLkyTh48CC8vb21ct4JEybg8uXLAIC5c+di/fr1MDc3x8yZMzFr1iytnIOIiNQzmHX4I26sROs6/nB3dy/xmNnDLBS42Va4rZrYibeqa/TrdbIvl6PxrsdwuJSjLMrNzoX9VYnGMdWUefPmISAgQHk/KyuLSb+BSUxMhK3ti78zpQ2/qW5bt27FgAED4ObmplIeGBiIjIwMhIeHw9HREYcOHcLIkSNx5syZSg3hedXMmTOVP/v6+uLmzZuIioqCt7c32rZtW+X2iYiodHqf8MvlL4bUXH26Ay6un6qtV1SUr1G7pkn5eOZU8aTfLAnIc9Qs6ZdpFpJSoUMRoEH+blb3GaR5Fa/v6ZCOolzNYmppm4KCnPLrFetgdR952aU/fnVQHbR4kI+m2x5BJAfqACj+rubl97y2MDMzU0nwimNUGdqTk/PiyHXJ9Ubxe2xjY6OS8Kvj6OgIsViM1NRUlfLU1NRSJ+RqIj4+HuHh4Thw4IBKeVxcHNatW4erV6+iVatWAAAfHx+cOXMG69evx6ZNmyp9TplMhq+++gqHDx+GRCLBW2+9heDgYDRs2BANGzas0ushIqKK0fuEPztbNWuMuLFSfcUbNRCMnrhTieeEaz2K0mVnZ8POzq4Gz6i54utSbS//SxMpSX9U5Lo0NTVFx44dERERgSFDhgBQJMwRERGYOnVqlWMIDQ2Fs7MzBg4cqFKel6f4xG9kpDrKUywWQyaTVemcixcvxoIFC+Dr6wsLCwt88803SEtLK3USMhERaZ/eJ/xubm5ITEyEjY2NygQ10j9yuRzZ2dklhirURrwuDYem12VAQAD8/f3RqVMndOnSBatXr0Zubi4mTJgAQLF8pru7u3IOgEQiwfXr15U/JyUlITo6GtbW1irj72UyGUJDQ+Hv7w9jY9U//c2bN4e3tzcmT56MFStWwMHBAYcOHUJYWBiOHDmirJeQkID09HQkJCRAKpUiOjoaAODt7Q1ra2u1r2fnzp3YsGEDJk+eDAAIDw/HwIEDsWXLlhIfMIiIqJrIiYioVlm7dq28QYMGclNTU3mXLl3kf/75p/KxXr16yf39/ZX37927JwdQ4tarVy+VNn/77Tc5AHlsbKzac966dUs+bNgwubOzs9zS0lLetm1b+c6dO1Xq+Pv7qz3XiRMnSn0tpqam8oSEBJUyMzMzeWJiYsV+GRoKDg6W+/j4VEvblVH8/ly6dEnoUIjIgInk8lo44JmIiPSCWCxGSkoKnJyclGU2NjaIiYlBo0aNqtS2SCTCwYMHlcOfAGDBggU4dOiQ8tsHod2/fx+NGjXCpUuX0K5dO6HDISIDpfdDeoiISDhyuRzjx49XmbSen5+P//znP7CyslKWvTqRuCZJJBKYmpoKdn4iourGAZRERFRt/P394ezsrLIHwZgxY+Dm5gY7OzucO3cON2/exOzZs1G3bl24urpiwYIF5bbr6ekJABg6dChEIpHyfrHvv/8enp6esLOzw+jRo1UWcOjduzemTp2KGTNmwNHREf369QMAXL16FQMGDIC1tTVcXFwwduxYPH78YmOUX3/9FT179oS9vT0cHBzwr3/9C3FxcSrn/fvvv9G+fXuYm5ujU6dOuHTpksrjT58+hZ+fH5ycnGBhYYEmTZogNDRUg98oEZHm2MNPRETVprxktnfv3rh06RKsrKzw119/4fz58xg/fjx69OiBvn37lvq8CxcuwNnZGaGhoejfvz/EYrHysbi4OBw6dAhHjhzB06dPMXLkSCxduhSLFy9W1tmxYwc+/vhjnD17FgCQkZGBN998Ex999BFWrVqFZ8+eYc6cORg5ciT++OMPAEBubi4CAgLQtm1b5OTkICgoCEOHDkV0dDSMjIyQk5ODf/3rX+jbty927dqFe/fuYfr06SpxBwYG4vr16zh+/DgcHR1x584dPHum2T4oRESaYsJPRESCatu2LYKDgwEATZo0wbp16xAREVFmwl88J8De3r7EHgUymQzbt2+HjY0NAGDs2LGIiIhQSfibNGmC5cuXK+9/+eWXaN++PZYsWaIs27ZtGzw8PHDr1i00bdoUw4cPVznPtm3b4OTkhOvXr6N169b44YcfIJPJsHXrVpibm6NVq1Z48OABPv74Y+VzEhIS0L59e3Tq1AkASnwzQURUHTikh4iIBPXqTrv16tVDWlpapdvz9PRUJvultdexY0eV+5cvX8aJEydgbW2tvDVv3hwAlMN2bt++jffffx9eXl6wtbVVJusJCQkAgBs3bqBt27YwNzdXttutWzeV83z88cfYs2cP2rVrh9mzZ+PcuXOVfp1ERBXFHn4iIhKUiYnqruUikahKG35VpL2XJwwDQE5ODgYNGoRly5aVaK9evXoAgEGDBqFhw4b47rvv4ObmBplMhtatW0MikVQ4tgEDBiA+Ph7Hjh1DWFgY3nrrLUyZMgUrVqyocBtERJpiDz8REekkExMTSKVSrbTVoUMHXLt2DZ6envD29la5WVlZ4cmTJ4iNjcX8+fPx1ltvoUWLFnj69KlKGy1atEBMTAzy8/OVZX/++WeJczk5OcHf3x+7du3C6tWrsXnzZq28BiKi0jDhJyIineTp6YmIiAikpKSUSL41NWXKFKSnp+P999/HhQsXEBcXh99++w0TJkyAVCpFnTp14ODggM2bN+POnTv4448/EBAQoNLGBx98AJFIhEmTJuH69es4duxYiZ77oKAg/Pzzz7hz5w6uXbuGI0eOoEWLFlWKnYioPEz4iYhIJ61cuRJhYWHw8PBA+/btq9SWm5sbzp49C6lUirfffhtt2rTBjBkzYG9vDyMjIxgZGWHPnj2IiopC69atMXPmTHz11VcqbVhbW+OXX37BlStX0L59e3z++eclhgiZmppi3rx5aNu2Ld544w2IxWLs2bOnSrETEZWHO+0SEREREekx9vATEREREekxJvxERFTr7N69W2WJzJdvrVq1Ejo8IiKdwiE9RERU62RnZyM1NVXtYyYmJmjYsGENR0REpLuY8BMRERER6TEO6SEiIiIi0mNM+ImIiIiI9BgTfiIiqpCQkBB07twZNjY2cHZ2xpAhQxAbG6v18yxduhQikQgzZszQSntJSUkYM2YMHBwcYGFhgTZt2uCff/6pUptSqRSBgYFo1KgRLCws0LhxY3zxxReozCjZ06dPY9CgQXBzc4NIJMKhQ4dUHpfL5QgKCkK9evVgYWEBX19f3L59u0rxE5FhYcJPREQVcurUKUyZMgV//vknwsLCUFhYiLfffhu5ublaO8eFCxfw7bffom3btlpp7+nTp+jRowdMTExw/PhxXL9+HStXrkSdOnWq1O6yZcuwceNGrFu3Djdu3MCyZcuwfPlyrF27VuO2cnNz4ePjg/Xr16t9fPny5VizZg02bdqEv/76C1ZWVujXrx/y8/Or9BqIyHBw0i4REVXKo0eP4OzsjFOnTuGNN96ocns5OTno0KEDNmzYgC+//BLt2rXD6tWrq9Tm3LlzcfbsWZw5c6bK8b3sX//6F1xcXLB161Zl2fDhw2FhYYFdu3ZVul2RSISDBw9iyJAhABS9+25ubvj000/x2WefAQAyMzPh4uKC7du3Y/To0VV6HURkGNjDT0RElZKZmQkAqFu3rlbamzJlCgYOHAhfX1+ttAcAhw8fRqdOnTBixAg4Ozujffv2+O6776rcbvfu3REREYFbt24BAC5fvozIyEgMGDCgym2/7N69e0hJSVH5ndjZ2aFr1644f/68Vs9FRPrLWOgAiIhI98hkMsyYMQM9evRA69atq9zenj17cPHiRVy4cEEL0b1w9+5dbNy4EQEBAfi///s/XLhwAdOmTYOpqSn8/f0r3e7cuXORlZWF5s2bQywWQyqVYvHixfDz89Ni9EBKSgoAwMXFRaXcxcVF+RgRUXmY8BMRkcamTJmCq1evIjIyssptJSYmYvr06QgLC4O5ubkWontBJpOhU6dOWLJkCQCgffv2uHr1KjZt2lSlhH/fvn3YvXs3fvjhB7Rq1QrR0dGYMWMG3NzcqtQuEVF14JAeIiLSyNSpU3HkyBGcOHEC9evXr3J7UVFRSEtLQ4cOHWBsbAxjY2OcOnUKa9asgbGxMaRSaaXbrlevHlq2bKlS1qJFCyQkJFQp5lmzZmHu3LkYPXo02rRpg7Fjx2LmzJkICQmpUruvcnV1BYASuw6npqYqHyMiKg8TfiIiqhC5XI6pU6fi4MGD+OOPP9CoUSOttPvWW2/hypUriI6OVt46deoEPz8/REdHQywWV7rtHj16lFg69NatW2jYsGGVYs7Ly4ORkep/oWKxGDKZrErtvqpRo0ZwdXVFRESEsiwrKwt//fUXunXrptVzEZH+4pAeIiKqkClTpuCHH37Azz//DBsbG+UYcjs7O1hYWFS6XRsbmxLzAKysrODg4FDl+QEzZ85E9+7dsWTJEowcORJ///03Nm/ejM2bN1ep3UGDBmHx4sVo0KABWrVqhUuXLuHrr7/GxIkTNW4rJycHd+7cUd6/d+8eoqOjUbduXTRo0AAzZszAl19+iSZNmqBRo0YIDAyEm5ubciUfIqLycFlOIiKqEJFIpLY8NDQU48eP1+q5evfurZVlOQHgyJEjmDdvHm7fvo1GjRohICAAkyZNqlKb2dnZCAwMxMGDB5GWlgY3Nze8//77CAoKgqmpqUZtnTx5En369ClR7u/vj+3bt0MulyM4OBibN29GRkYGevbsiQ0bNqBp06ZVeg1EZDiY8BMRERER6TGO4SciIiIi0mNM+ImIiIiI9BgTfiIiIiIiPcaEn4iIiIhIjzHhJyIiIiLSY0z4iYiIiIj0GBN+IiIiIiI9xoSfiIg0VlBQgAULFqCgoMCg263OtqszZnqhqKgI4eHh+Pbbb5GdnQ0AePjwIXJycgSOjEh7uPEWERFpLCsrC3Z2dsjMzIStra3BtludbVdnzKQQHx+P/v37IyEhAQUFBbh16xa8vLwwffp0FBQUYNOmTUKHSKQV7OEnIiIigzR9+nR06tQJT58+hYWFhbJ86NChiIiIEDAyIu0yFjoAIiIiIiGcOXMG586dg6mpqUq5p6cnkpKSBIqKSPv0PuGXyWR4+PAhbGxsIBKJhA6HqpFcLkd2djbc3NxgZFS7v7zidWk49PW6zMrKUjlqi661W51tV1e7unRNVjeZTAapVFqi/MGDB7CxsREgIqLqofdj+B88eAAPDw+hw6AalJiYiPr16wsdRpl4XRoeXpdU2+jCNVndRo0aBTs7O2zevBk2NjaIiYmBk5MT3n33XTRo0AChoaFCh0ikFXqf8GdmZsLe3l55/9oNZ5XHzz3T7D+3P7Mba1T/6lNXjeqnPLbTqD4A4LG5RtXNHmnWo2zxSKPqsEot0qz9pEqshPAgReVuRMZO5c8ZGRmws6vE77EGvXpdTot8V+M2rmdpdm0BwP0ndTV+TkG6RfmVXmHyRPMvD80ea/wUAIDlE83/hFk8KtT4OWYpml+nEddXKH/Wpesy8dgx2PboIXQ4VE2ysrLg4eGhE9dkdXvw4AH69esHuVyO27dvo1OnTrh9+zYcHR1x+vRpODs7l98IkQ7Q+yE9L38tvTm6FaKNrVQetyzlG7vIrCZqy02tSz9XTLpbibJXTqfi4SP7EmVGlqXXR1opiX05+b552isJvlnZ9YtZpD3/wbTMaiUYm2iW8BuLNU++IFINqrvxKJwr2qt4SAeGyLwc479P9YWZtYnaelcyS15TxUq7tu49dij1OeJSrq/8J6Un9UalPGTyuIw/H5p9BgUAiCt4XZZ4nqnmCb+xsVjz51TiOn3D7AOcLvgBgG5dl7ZWVlwVxgDowjVZ3erXr4/Lly9j7969uHz5MnJycvDhhx/Cz89PZRIvka7T+x7+4mXNPjw1EqavJFXqEvSyqEvQy1Vakq6hEkm7FikTey2yTtEs6S9mkZhd6XMWSQsQcXW5TixhV3xden8/D2LL0q+RshLx0pSZiOsocw2/ZQIAy8ea/2mzTK1Ez39y2ddskbQAEddX6NR1mXnqFGzfeEPocKiacLlPIsOjf5lBKcJvN4NRGYmVkpYS9NJUZ+JenupI7EuT46q4tDRN/J95lD5JqiofBmor2W0bwLz0a059v7/hyXeqzHNK/7dW2geIPMfSv84q7QNEnkvpw6QsUwtRVGQCXC+1Su3EyYq12uXEDEz54SKy88v/+9rD2wHrP+jA3vxShISEwMXFBRMnTlQp37ZtGx49eoQ5c+YIFBmRdhlMwo/H5mUmVpUlZAJflppM7stS2cRfHX38MGD2uPJDWQxBZRL96mq3tA8QZX37kOdoCmmBDPhD8/MJqrFmc5WoZoVdT8WDp88qVPfYlRQ8zStEXSsNx2YaiG+//RY//PBDifJWrVph9OjRTPhJbxhOwk+C0mbir84zDxsUFZoAV6uleaJSlffhQZZfM3FolZplCqn2eJonAQD4d2uIcd09S603ZN1ZZBcU4WmehAl/KVJSUlCvXr0S5U5OTkhOThYgIqLqwYSfiIhUXbkCcAx/rVWc8Hs5WaOxU+krSdSxMlUk/LkSoJq+LdN1Hh4eOHv2LBo1aqRSfvbsWbi5aTbPj6g2Y8JPRESkQ57mKiaX21uWPcunjpUpEtLz8DSvEiuhGYhJkyZhxowZKCwsxJtvvgkAiIiIwOzZs/Hpp58KHB2R9hhMwp+wOAT1gwNhbGwwL9mgFBUV4e8Ty4UOQ2OxoSFo+hGvS31VVFSEhK9DhA6D9ExxD38dy7KH6dR5/oHgaa6k2mPSVbNmzcKTJ0/wySefQCJR/J7Mzc0xZ84czJs3T+DoiLTHYPbUlmfnIGnRl0KHQdXkwokQFEkqsYGXwGTPsnF7K69LfZX41ZeQ5ejmhHKqvYoT/vLG5dd9/oGguD6VJBKJsGzZMjx69Ah//vknLl++jPT0dAQFBQkdGpFWGVS3oixb9xJCqpjCAt19b6XPmBDqKymTfdIyuVxe4SE99s8T/nQm/OWytrZG586dhQ6DqNoYVMJvZFPGNrmk00zMrFFYoJvJldiCa57rK7G1DaTZWUKHoblWrYSOgEqRJ5FCIpUBqEAPv5XiA0FGLsfwlyY3NxdLly5FREQE0tLSIJPJVB6/e/euQJERaZfBJPwiG2u4B80XOgyqJp37zMPffyzRuWE9RhY2aPIhr0t95TFrPhKWf6F7w3pMuOVbbZX+fDy+qbERLEzEZdZlD3/5PvroI5w6dQpjx45FvXr1uEEZ6S2DSfgbfD4PRpwYqbeMjY3Rpc9snPtNt8ZdNpswD2Jel3rL2NgYngHzcHfR/wkdimbu3gXatRM6Cp2XmJ6Hlb/HIldS/r4GbzR1wtjXGpZbL+P5ijt1LU3LTU6LvwHIYMJfquPHj+Po0aPo0aOH0KEQVStmGkREpCpLB4ch1ULbz93HoeiHFaobcSMVIzrWh3k5vfbFvfXljd9/uU46V+kpVZ06dVC3bl2hwyCqdkz4qyjfWS7Iec3Tyu7ZeeZcQ4G8wiKt7MeLd9ytDlIJL2dtKG/nWCKqmBvJig9Oozt7oG19+1LrLfjlGiRFMjzKLoBHXcsy28yo4Ao9L9fJ4Dr8pfriiy8QFBSEHTt2wNKy7N89kS4TNEM6ffo0vvrqK0RFRSE5ORkHDx7EkCFDlI/L5XIEBwfju+++Q0ZGBnr06IGNGzeiSZMmwgVdSwj1QaMs5mkiwT5oAIC0QLhzV1aBI2BkLnQURPrlZkoW4tJyy63nameGDg3qVMu4bblcjpspirkbfl0bok19u1LrbjoVh4T0PKRm5Zeb8Bf31pe3Bv/LdTKeFUImk8PIiOPTX7Vy5UrExcXBxcUFnp6eMHll/srFixcFioxIuwRN+HNzc+Hj44OJEydi2LBhJR5fvnw51qxZgx07dqBRo0YIDAxEv379cP36dZibM0uqbYT+ECLLr30fgqhmFToWCR1CCbJntS8mffYouwCD1kaiUFqxvwcBfZti2lva70R6lFOA9FwJjERAE5eyV4hzsTV7nvCX32tRvGtuHauKD+mRyuTIzi+CXQWGARmalzsZifSZoAn/gAEDMGDAALWPyeVyrF69GvPnz8e7774LANi5cydcXFxw6NAhjB49uiZDJaIaUBsTdoPk5iZ0BJV2Oy0bhVI5LE3FaO1eeq96oVSGSwkZWBV+Cx0a1EHPJo5ajeNmsqJ339PRqtxx+c62ig6s1Kz8ctt9qkEPv5mxGFamYuRKpHiaJ6lywr9+/Xp89dVXSElJgY+PD9auXYsuXbqorXvt2jUEBQUhKioK8fHxWLVqFWbMmKFSx9PTE/Hx8SWe+8knn2D9+vUAgJSUFMyaNQthYWHIzs5Gs2bN8Pnnn2P48OHK+oMHD0Z0dDTS0tJQp04d+Pr6YtmyZXCrwHUcHByswW+ASHfV2p127927h5SUFPj6+irL7Ozs0LVrV5w/f17j9tJ27iqxvi4RUXWSyWR4tPVHocPQnLOAY/Oq6EH6MwBAx4Z1sG9yt1JvBz/pgdGdPSCXA9P2XEJy5jOtxnEzRTF+v7lr+ftsuNg8T/izK5Dw51U84Qe0tzTn3r17ERAQgODgYFy8eBE+Pj7o168f0tLUT9zKy8uDl5cXli5dCldXV7V1Lly4gOTkZOUtLCwMADBixAhlnXHjxiE2NhaHDx/GlStXMGzYMIwcORKXLl1S1unTpw/27duH2NhY7N+/H3FxcXjvvfcq/NoyMjKwZcsWzJs3D+np6QAUQ3mSkpIq3AZRbVdrZzmmpKQAAFxcXFTKXVxclI+pU1BQgIKCF1+LZj1fbeLZ9Rt4FLoDLh9OqIZoicpW2nVJ+u3xt7uQH3ND6DA09/QpYGsrdBSV8uBpHgCUOxYeABYMboWYB5m4npyFqT9cwucDW6C8Ue6Nna1ha15+T3nx+P3mruX/Hl1szQAAaRUa0vM84a/AkB5AMXE3KeNZlZfm/PrrrzFp0iRMmKD4P3TTpk04evQotm3bhrlz55ao37lzZ+XOteoeBwAnJ9UVApYuXYrGjRujV69eyrJz585h48aNym8S5s+fj1WrViEqKgrt27cHAMycOVNZv2HDhpg7dy6GDBmCwsLCEmPyXxUTEwNfX1/Y2dnh/v37mDRpEurWrYsDBw4gISEBO3fuLO9XQ6QTam3CX1khISFYuHCh2scKEhNrOBoihbKuS9Jfkvu1+29OqR9E4+OBhuWvCV8bPXiq6KmvX8ei3LrmJmJs8OuAQWsjERX/FMM2nCv3OQ0dLPHHp70hLmcCbPGQngr18D8f0pNWkR7+57vmVryHv3hpzpIr9bza8WBmZgYzM7MS9SQSCaKiojBv3jxlmZGREXx9fSv1jbs6EokEu3btQkBAgMok6u7du2Pv3r0YOHAg7O3tsW/fPuTn56N3795q20lPT8fu3bvRvXv3cpN9AAgICMD48eOxfPly2Ni8eK/eeecdfPDBB1V+XUS1Ra0d0lP8FWBqaqpKeWpqaqlfDwLAvHnzkJmZqbwlvpTkm3l4VE+wROUo67ok/WXqWbv/5oSEhMDOzk5589CDv5EvEv6KLbHo6WiFDWM6oEU9W9SvY1HmzUQsQvyTPFxKeFpmm4VSGe6kKXb9rkgPv/PzHv6KTdrVbEhPWZtveXh4qLz/ISEhatt4/PgxpFKpxt+4a+LQoUPIyMjA+PHjVcr37duHwsJCODg4wMzMDJMnT8bBgwfh7e2tUm/OnDmwsrKCg4MDEhIS8PPPP1fovBcuXMDkyZNLlLu7u2vttRHVBrW2h79Ro0ZwdXVFREQE2j3f8TErKwt//fUXPv7441KfV1oPhUXLFnCa4F9d4RKVqbTrklSZPC79T5IuTuh1nDwGjzbsqLXDeubNm4eAgADl/aysLJ1P+hOLh/RUoIe/2OtNnHB8evkbUEzfcwk/Rz9E2PVUdPIsfbOm+49zIZHKYGUqrtA3DS6aTNrVYB1+4MUHA3WbbyUmJsL2paFbQv6N2rp1KwYMGFBiom1gYCAyMjIQHh4OR0dHHDp0CCNHjsSZM2fQpk0bZb1Zs2bhww8/RHx8PBYuXIhx48bhyJEj5S65amZmpnaI5a1bt0oMOSLSZYIm/Dk5Obhz547y/r179xAdHY26deuiQYMGmDFjBr788ks0adJEuSynm5tbpZbR8hgwFuLHZa+UoA1CL01JusXsMSCuwv+xhrRJVlkfBipCiA8MRkZGcPrwfSROD6rxc1eEvn0QlRTJkPI8aa5oD78m+rZ0USb8895pUWq9G8/H7zdztanQ2vfONor3IDu/CHmSIliaqr/Wn0mkyC9ULD5RkZ12gRcJ/1M1m2/Z2tqqJPylcXR0hFgs1vgb94qKj49HeHg4Dhw4oFIeFxeHdevW4erVq2jVqhUAwMfHB2fOnMH69euxadMmlRgdHR3RtGlTtGjRAh4eHvjzzz/RrVu3Ms89ePBgLFq0CPv27QMAiEQiJCQkYM6cOSorARHpOkET/n/++Qd9+vRR3i/uafL398f27dsxe/Zs5Obm4t///jcyMjLQs2dP/Prrr5Vag9/xahGMTVT/w6+OXV8taukOuPwgUjtZPpFDbFr6e5PnWPb1ZP5I2xFVXm3/8FHVDwyAbn7LUCkWFe8dr02SM59BLgfMjI3gaF2xHnBN9GrqBBOxCHcf5+JOWg68ndWvrx/7fIWeZhUYzgMA1mbGsDQVI08iRVpWATwd1V+rxb37JmIRrM0qdj0XT+59qqaHv6JMTU3RsWNHREREKDvcZDIZIiIiMHXq1Eq3Wyw0NBTOzs4YOHCgSnlenuLbGiMj1dHHYrG4zFX3ih97eX5KaVauXIn33nsPzs7OePbsGXr16oWUlBR069YNixcv1vSlENVagib8vXv3hlxeerIjEomwaNEiLFq0qMrnskjKgbFY0cPxzEMxMcc6peb+8y7+cGGhfgWzalP8AcO8nA8i2sQPF9pj+bj6f5flfaioqJr48CH0hwpNPzTo7AeEZs2EjqBSEtNfTNitjt1zbcxN0K2xI07feoSw66mlJvzFE3Zb1Ct/wi6g+L/OxdYc9x7nIjUrH56OVmrrFQ/Lsbc0rfDre9HDX7VVegICAuDv749OnTqhS5cuWL16NXJzc5Wr9owbNw7u7u7KeQASiQTXr19X/pyUlITo6GhYW1urjL+XyWQIDQ2Fv78/jI1V/301b94c3t7emDx5MlasWAEHBwccOnQIYWFhOHLkCADgr7/+woULF9CzZ0/UqVMHcXFxCAwMROPGjcvt3QcUy32HhYUhMjISMTExyMnJQYcOHVSWBCfSB7V2DD+RRpzzgbzyx78Skf7SZEnOyurb0uV5wp+Cj3s3VluneEnOZi4VS/gBxbCee49zkZpdeq90xvNhOXUrOGEX0F7CP2rUKDx69AhBQUFISUlBu3bt8Ouvvyon8iYkJKj0xD98+FC5bCYArFixAitWrECvXr1w8uRJZXl4eDgSEhIwceLEEuc0MTHBsWPHMHfuXAwaNAg5OTnw9vbGjh078M477wAALC0tceDAAQQHByM3Nxf16tVD//79MX/+fI2Gq/Xs2RM9e/bU9NdCpDOY8BMRkarLl4HXXxc6Co1psiRnZfVt4YLAQ1dxKTEDj7IL4GSjmlRm5RciKUMRR0VW6CmmXJqzjIm7xZtnVXT8PvDSkB41Y/g1NXXq1FKH8LycxAOKXXTL+ga/2Ntvv11mvSZNmmD//v2lPt6mTRv88ccf5Z7nZWvWrKlw3WnTpmnUNlFtVamEf8eOHXB0dFSOt5s9ezY2b96Mli1b4scff0RDHV2/mYiIAFQgUauNinv4q2PCbjFXO3O0rW+HmAeZiLiRitFdGqg8Hvu8d9/Nzhx2GiTmLsqlOUtP+DM0XKEHeNHDn5EngVwur5ahTrpm1apVKvcfPXqEvLw82NvbA1DsvGtpaQlnZ2cm/KQ3KrUO/5IlS2DxfFLX+fPnsX79eixfvhyOjo4qO97VJpfjD5Q5yYd0m1wuR1bYX0KHobE7kbt4XeoxuVyO7FPa2ZiIypdYAz38gKKXHwDCrqdCKpOr3G4kF0/YrfhwHuDlpTlLH9Lz8hj+iipO+AulcuQU6OicEi27d++e8rZ48WK0a9cON27cQHp6OtLT03Hjxg106NABX3zxhdChEmlNpXr4ExMTlZNuDh06hOHDh+Pf//43evToUerud0J7nH0Hl+N/QvtGI4UOhapB1q/n8XTv70KHobHMpBu4c3oHmvaeIHQoVA2y/ziLjAPHhQ7DYCjH8FdjDz8A9G3lgpVhtxBxMw2N/++Y2jrN61V8OA8AOFdgLX7lGH6rin9zYGEqhrmJEfILZcjIK4SNecWfawgCAwPx008/odlLE9WbNWuGVatW4b333oOfn5+A0RFpT6V6+K2trfHkyRMAwO+//46+ffsCAMzNzfHs2TPtRadlWc+ShQ6BqknBrXihQ6i0vCfcdVdfFdy5L3QIBqOgSKrsHa/uHv5mLjboUsbGW6bGRniruWZrMLs8nwuQVsak3eIe/oruslusrM23DF1ycjKKikp+8yGVSkvsO0CkyyrVw9+3b1989NFHaN++PW7duqWcLX/t2jV4enpqMz6tsrWoJ3QIVE3MmjZE7l9XhQ6jUiwddHtnUyqdmbcn8qJihA5Dczq4LOfDDEXPuIWJWKMx7pUhEomwd/JryHymfiKsuYkY5iaabfT48m67pY21L15ppzIJf3JmfpVX6tFHb731FiZPnowtW7agQ4cOAICoqCh8/PHHXJqT9EqlevjXr1+Pbt264dGjR9i/fz8cHBwAKP6RvP/++1oNUFscbbzh0/A9ocOgamLbvxvqjHpb6DA0ZufeAt5v+AsdBlUTmzd7wH7YAKHD0JwObryVmF68JGf1rMH/KpFIBHtLU7U3TZN9AHB+Pmk3TyItday9MuHXYEjPy/WZ8Je0bds2uLq6olOnTsqdp7t06QIXFxds2bJF6PCItKZSPfz29vZYt25difKFCxdWOaDq4tNwWInd+kh/iEQi2PbtqnPj+L17juF1qcdEIhFsenXTvXH8CQlA69ZCR6GRF0tyVu/4/epiaWoMG3NjZOcXIS27QO1Y+6e5im8UKjukp/j59IKTkxOOHTuGW7du4ebNmwAUG341bdpU4MiItKvCCX9MTMW/lm7btm2lgiGqtDRzgPtu6T1t7eZbUzv2mjw2hixfB7c7SU8XOgKNvViSU/e+nSjmYmuO7PwcpGblo7FTyV18qzKk5+XnU0lNmzZlkk96rcL/E7Vr1w4ikahC6/hKpdIqB6Ztz9ytYWxirrX2clyF/U/8mWbzwapVvrNurtldG+Q5iCA247rYNU1bHxyAmvvwQGWrqSU5q5OLrRnupOUgTc3SnAVFUuRJFP+31tFwjkJxfSb8JUmlUmzfvh0RERFIS0srsUyyppt6EdVWFc5a7927p/z50qVL+OyzzzBr1ix069YNgGI9/pUrV2L58uVaC04qlWLBggXYtWsXUlJS4ObmhvHjx2P+/PlaG6MpdOL+qtqUyJeGCT7RC+V9eJCWvugKaVFNLclZnZxtSl+as3hJTrGRCLbmmv2/Vef5BmAc0lPS9OnTsX37dgwcOBCtW7fmxmSktyr8V+Pl3XNHjBiBNWvWKFfnARTDeDw8PBAYGIghQ4ZoJbhly5Zh48aN2LFjB1q1aoV//vkHEyZMgJ2dnca73+W6GENsKlxyrwuJfGmY4FefAkfASHtfPOk8bfa8k2HR9TH8wIuJu+o233qxJKeJxklpXfbwl2rPnj3Yt2+fSj5DpI8qlQFfuXIFjRo1KlHeqFEjXL9+vcpBFTt37hzeffddDBw4EADg6emJH3/8EX///bfWzlETmOwTVYy2hsfwg0MVOevWH638QikeZdfMGvzVyaW4hz+7ZA9/cbKuyS67xey5Dn+pTE1NlRuJEumzSi0P0qJFC4SEhEAiefHHQyKRICQkBC1atNBacN27d0dERARu3boFALh8+TIiIyMxYIDmy9zdO7GrxNg80h9SqRRJq74ROgyNJW78plbOeSHtkMlkSDy+S+gwNOfmJnQEaj2TSPHr1WT8HJ2kcvvhrwQAgJWpGPaWuruTbPFa/GlqhvQUD8epW4mEv/g5xcOC6IVPP/0U33zzDeRydm6RfqtUD/+mTZswaNAg1K9fX7kiT0xMDEQiEX755RetBTd37lxkZWWhefPmEIvFkEqlWLx4cZlbXRcUFKCg4MXXoVlZWYpj4nXci9iBxn0naC0+qj2SV3yNotQ0ocMoVWnXZeGjVCSt+xoNps8SKjSqRolHdiDnnva+9awx2dmAra3QUZQQ9PNV/C/qQamPe9S11Okx2C5lDOl50cOv+Qea4uc8zZMwsX1FZGQkTpw4gePHj6NVq1YwMVH9/R44cECgyIi0q1IJf5cuXXD37l3s3r1buW7tqFGj8MEHH8DKykprwe3btw+7d+/GDz/8gFatWiE6OhozZsyAm5sb/P3Vb1YUEhJS6n4AeY8TtRYb1S5Fj58IHUKZyrouC9Mf13A0VFOepSYIHULlxMUB7u5CR6HiZkoWfrqoSPZf86oLo1cSe7GRCOO6eQoQmfaUtdvu0+fDcSqzi3DxcwqKZHhWyG8UX2Zvb4+hQ4cKHQZRtav0LFYrKyv8+9//1mYsJcyaNQtz587F6NGjAQBt2rRBfHw8QkJCSk34582bh4CAAOX9rKwseHh4AAAsHT2qNV4SjrGjAwqTU4QOo1RlXZcmdR2FCouqmYVLA2TnXBE6DL3w1a+xkMuBgW3qYb1fB6HDqRZONooe/oIiGf7v4BWIjV4k/BfjMwBUbgy/pakYpmIjSKQyLD1+E0ZF3LSkWGhoqNAhENWIKi1bc/36dSQkJKiM5QeAwYMHVymoYnl5eSV2IRWLxWWOxS/eGvtVth4t0egt9R8SqHTmaZX/erwmJ/zW+ywAD5evqLXDekq7Lk2cXOA+NUDNMwybvky69fiXPxIOb9PNYT0vuZqUicT0vHLrOdmYoZNnXa2f/+976Yi4mQaxkQif9Wum9fZrC3MTMerZmSM5Mx8//q3+G2l3e82X9RKJRHC1M0dCeh52no+HrKD899KQFBUV4eTJk4iLi8MHH3wAGxsbPHz4ELa2trC2LrkBGpEuqlTCf/fuXQwdOhRXrlxRbsYFQPn1o7YmIQ4aNAiLFy9GgwYN0KpVK1y6dAlff/01Jk6cqHFbjfqMKfHhQRt0eQWe6laVDwuaM4bXmBm4tfL/avCcVefx8XQYicVCh6GWviTdQjIyMoLHgDG4sUG3rsuXJabnYdC6SFR06Pfef7+Grl4OWju/XC7H0uM3AACjO3ugkaP2ho3WRhvHdMTJWPUdF3YWJhjesX6l2v16pA9+u5YCuRzIz8vB4tVVCFKPxMfHo3///khISEBBQQH69u0LGxsbLFu2DAUFBdi0aZPQIRJpRaUS/unTp6NRo0aIiIhAo0aN8Pfff+PJkyf49NNPsWLFCq0Ft3btWgQGBuKTTz5BWloa3NzcMHnyZAQFBWntHMWYuFNtw4SbBPPSxMXEp3mQywELEzHauNuV+pT49FykZhXgXNyTCif8R2OScfdRTpl1HuUU4GJCBixMxJj+VpOKxa/D2nnYo52Hvdbb7eRZV/ntS1ZWFhZr/Qy6afr06ejUqRMuX74MB4cX1+3QoUMxadIkASMj0q5KJfznz5/HH3/8AUdHRxgZGcHIyAg9e/ZESEgIpk2bhkuXLmklOBsbG6xevRqrV6+uclvPnABxyREVRIIye8zrkmqhVq2UP+YVKL6xbepqg33/6VbqU3aev4+gn6/h8oOMCp3i+sMsTPnhYoVD+rBnIzjbcpc60q4zZ87g3LlzMDVVnRvh6emJpKQkgaIi0r5KJfxSqRQ2NjYAAEdHRzx8+BDNmjVDw4YNERsbq9UAiYhIOLmSIgCKNe7L4lPfHgBwOTGjxAoz6pyLU6xO1cjRCq+V841AHUsTTOnDzZFI+2QymdphyA8ePFDmOUT6oFIJf+vWrXH58mU0atQIXbt2xfLly2FqaorNmzfDy8tL2zESEVFNunYN6Kbozc+TKJIhS9Oy/7toXs8Gpv/P3n3HVVX/Dxx/XfZGZIMIKO6B29yalqZppjnKgWb+/JamSeUoR1qmmaWZ5ihFLS1taK4sJffKwD1woSAyVGTLvOf3B3Hlsu/lXu7g83w8zgPuued+zvvCh8v7fM5nmJrwOCOHqMQMfJ3L7mv/T2QiAMPa+vC/bnU1ELQgqO75559n2bJlrF27Fsgfi5iWlsbcuXPp27evjqMTBM1RaxTrrFmzFDPlzJ8/n8jISLp06cLevXtZvny5RgMUBEEQqljO0xVZ07P+a+G3LLuF39LMlEZe+Yt1nYtOKvNYuVzizJ38hL+dv+Zn9RGEivriiy84fvw4jRs3JjMzk9dee03Rneezzz7TdXiCoDFqtfD37t1b8X1AQADXrl0jMTERJycnvV3l8HHYcZw7PKu38QmVI0kSj8OO6zoMlT06fxzXtqJeGitJknh03vDqZWFPFC385c8m1dKnBuejkzgfncxLLUpfuOvmgzQeZ+RgbW5KU6/SBwILgrbVqlWL8+fP89NPP3HhwgXS0tIYN24cI0aMwNraWtfhCYLGVGoe/ps3b3Lr1i26du1KzZo19XrJ7geH9mBiZk7Ndl11HYqgBY/PHOXBoT26DkNlCcfz66VLK1EvjdGjs0dJOG549bKw9Ap26QEI9MlP3ssbuHv6v+48rXxrYGGm+emSBUEVZmZmjBw5UtdhCIJWqfVJ++jRI3r27En9+vXp27cvsbGxAIwbN453331XowFqUkbMHV2HIGhJxr1IXYegtozYO7oOQdCSjPuGWy8LZFRw0C48Hbh7KSaZnLzSF0gs6L/fzk9z8/ULgroiIiKYNGkSPXv2pGfPnkyaNIlr167pOixB0Ci1Ev6pU6dibm5OVFQUNjY2iv3Dhg1j3759GgtO02y8/XQdgqAlNrX8dR2C2mw8/XQdgqAlNl4GWi/rPh1Em/7ftJw2luW38Ps52+JgZUZWrpyIuNQSj5EkiTORov++oB9+/fVXmjZtSlhYGIGBgQQGBhIeHk6zZs349ddfdR2eIGiMWl16/vrrL/78809q1VJe8a9evXrcvXtXI4Fpmmv3fji17aLrMAQtcWrbBXlujsF163Hr1A/nlqJeGivnlvn10uC69RSajlCVFn4TExmBPjU4euMh56KTaFrCQl3RiU+IS8nE3FRGy9o1NBayIKhj2rRpzJw5k/nz5yvtnzt3LtOmTWPw4ME6ikwQNEutFv709HSllv0CiYmJWFrq5ypCTq07iYGRRkwmk+HUupOuw1CZc6Col8ZMJpPhHGh49ZL79xXfFkzLaV2BPvyAYpXY86XM1HM68hEAzWvVwMq8/IsIQdCm2NhYRo8eXWz/yJEjFd2VBcEYqJXwd+nShU2bNikey2Qy5HI5ixcvpkePHhoLThAEQdCBhATFt6q08MPTfvylTc35j+jOI+iR7t27c/To0WL7jx07Rpcu4u6rYDzU6tKzePFievbsyb///kt2djbTpk3j8uXLJCYmcvy4YU9BJwiCIDylSh9+gMD/WvhvPkgjNTMHeytzpef/EfPvC3pkwIABTJ8+nbCwMJ555hkATp06xc8//8y8efPYuXOn0rGCYKjUXmn3+vXrrFixAnt7e9LS0hg0aBATJ07E09NT0zEKgiAIOqJqC7+rvSXeNayJSXrC4n0ReDhaKZ7LzpVz91EGJjJo7euklXgFQRVvvfUWAN988w3ffPNNic9Bfk+GvLy8Ko1NEDRJ5YQ/JyeHPn36sHr1aj788ENtxKQkJiaG6dOn88cff5CRkUFAQAAhISG0adNG6+cWBEGo7lSZh79Ay9o1iEl6wvenSp7EoYmXIw5FWv4FQRfk8tKnjxUEY6Jywm9ubs6FCxe0EUsxjx8/plOnTvTo0YM//vgDV1dXbty4gZOTaBkSjEOWC5hYgdUDXUciCIXUfNrdRpWVdgu8+3wDnGwsyMot3iJqaiJjWNvalY9REDQsMzMTKyur8g8UBAOkVpeekSNHsm7dOhYtWqTpeJR89tln+Pj4EBISotjn76/evNaPw47j3OFZMSOKkcrLy+POhq90HYbK4rb+gMeo11Fz/Lyg5+RyOdF//KDrMFRXOz8hlySJ9P+69NhYVjzh93ex5eOBTbUSmiBoUl5eHp9++imrV68mPj6e69evU6dOHWbPno2fnx/jxo3TdYiCoBFqJfy5ubmsX7+eAwcO0Lp1a2xtbZWe//LLLzUS3M6dO+nduzdDhgzh8OHDeHt789ZbbzF+/PhSX5OVlUVWVpbicUpKCgAPDu3BxMycmu26aiQ2Qb/cXf8l2Y/idR1GqUqrlxkRV4jfshH/3mN1FZqgRdG7N5IWeUXXYajuyRNwcCAzR44k5e+yVaFLjyAYigULFrBx40YWL16slFs0bdqUZcuWiYRfMBpqNSteunSJVq1aYW9vz/Xr1zl79qxiO3funMaCu337NqtWraJevXr8+eefvPnmm0yePJmNGzeW+pqFCxfi6Oio2Hx8fBTPZcTc0Vhsgn7JfvxQ1yGUqax6mRUTrcPIBG16Eh+l6xDUExEBoGjdB7AWc+ZXeytXrsTPzw8rKyvat2/PP//8U+qxly9fZvDgwfj5+SGTyVi2bFmxYwqeK7pNnDhRcUxcXByjRo3Cw8MDW1tbWrVqpbQC7p07dxg3bhz+/v5YW1tTt25d5s6dS3Z2doXe06ZNm1i7di0jRozA1PRpHQ8MDOTatWsVKkMQDIFaTTYHDx7UdBwlksvltGnThk8//RSAli1bcunSJVavXk1QUFCJr5k5cybBwcGKxykpKYrkysbbT+sxC7ph4eRC1gP9XSSlrHpp6e1T2ssEA2ftXpvUtIu6DkNtGVlP+++bmIjukNXZ1q1bCQ4OZvXq1bRv355ly5bRu3dvIiIicHNzK3Z8RkYGderUYciQIUydOrXEMs+cOaM0882lS5d47rnnGDJkiGLf6NGjSUpKYufOnbi4uLBlyxaGDh3Kv//+S8uWLbl27RpyuZw1a9YQEBDApUuXGD9+POnp6SxZsqTc9xUTE0NAQECx/XK5nJycnIr8aATBIOh1x2FPT08aN26stK9Ro0ZERZXeamZpaYmDg4PSBuDavR9ObcUiGsbK9/VgLJzddR1GqUqrlzYNGuP+WskXr4Lh83kxCDv/xuUfqKcycv7rv6/CgF3BOH355ZeMHz+esWPH0rhxY1avXo2NjQ3r168v8fi2bdvy+eefM3z4cCwtLUs8xtXVFQ8PD8W2e/du6tatS7du3RTHnDhxgrfffpt27dpRp04dZs2aRY0aNQgLCwOgT58+hISE8Pzzz1OnTh0GDBjAe++9x2+//Vah99W4ceMSF9765ZdfaNmyZYXKEARDoFYLf3p6OosWLSI0NJSEhIRi01rdvn1bI8F16tSJiP9uLRe4fv06vr6+Kpfl1LqTGLBrxExNTfEbM4XrX3yg61BU4jFsJCYmen3dLVSCiYkJPi+M5Oo3hlUvCygW3RL996u17OxswsLCmDlzpmKfiYkJvXr14uTJkxo7xw8//EBwcLDS/+qOHTuydetW+vXrR40aNdi2bRuZmZl079691LKSk5OpWbNiC7vNmTOHoKAgYmJikMvl/Pbbb0RERLBp0yZ2795d2bclCHpDrU/xN954g8OHDzNq1Cg8PT21lkhPnTqVjh078umnnzJ06FD++ecf1q5dy9q1a7VyPkEQhOqktMHk/PeZXrDolmjhN06K3/d/LC0tS2yNf/jwIXl5ebi7K99FdXd311g/9x07dpCUlMSYMWOU9m/bto1hw4bh7OyMmZkZNjY2bN++vcRuOAA3b97k66+/rlB3HoCXXnqJXbt2MX/+fGxtbZkzZw6tWrVi165dPPfcc5V9W4KgN9RK+P/44w/27NlDp06dNB2PkrZt27J9+3ZmzpzJ/Pnz8ff3Z9myZYwYMUKr5xUEQagOFi5cyLx584o/ERgIPG3ht7UULfzGqPDkAQBz587lo48+0kks69at44UXXsDLy0tp/+zZs0lKSuLAgQO4uLiwY8cOhg4dytGjR2nWrJnSsTExMfTp04chQ4aUOZtfUV26dGH//v0aeR+CoK/U+hR3cnKq8O2yynrxxRd58cUXq+RcgqArma66jkDQFnmmriMoXVmDyUG08Bu76OhoxXgioNS+9i4uLpiamhIfrzz1cXx8PB4eHpWO4+7duxw4cKBYv/tbt26xYsUKLl26RJMmTYD82XOOHj3KypUrWb16teLY+/fv06NHDzp27Ch6AQhCCdRK+D/++GPmzJnDxo0bsbGx0XRMggZkukm6DqHKyTOr33sWhMoorQsHERHQti0ZaqyyKxiOwhMIlMXCwoLWrVsTGhrKwIEDgfxZbEJDQ5k0aVKl4wgJCcHNzY1+/fop7c/IyAAoNs7J1NRUaexgTEwMPXr0oHXr1oSEhJQ7LsrJyanCXZETExMrdJwg6LsKJ/wtW7ZU+gO5efMm7u7u+Pn5YW5urnRseHi45iLUkCxXCRMrkRAKglA1clzyW8flT3LLOVIPPXkCPG3hF4tuCcHBwQQFBdGmTRvatWvHsmXLSE9PZ+zY/EUDR48ejbe3NwsXLgTyB+FeuXJF8X1MTAznzp3Dzs5Oqf+9XC4nJCSEoKAgzMyU61nDhg0JCAhgwoQJLFmyBGdnZ3bs2MH+/fsVA2pjYmLo3r07vr6+LFmyhAcPHiheX9rdh8JrAjx69IhPPvmE3r1706FDBwBOnjzJn3/+yezZsyv5UxME/VHhT/GCq3pDFbP0K7ymvae0sIZgPCRJIuXYcV2HobK4rT/gMep1MVNPIQWJsjGQy+U8WPejrsNQm2KWHkvxuVndDRs2jAcPHjBnzhzi4uJo0aIF+/btUwzkjYqKUvocu3//vtK0lkuWLGHJkiV069aNQ4cOKfYfOHCAqKgoXn/99WLnNDc3Z+/evcyYMYP+/fuTlpZGQEAAGzdupG/fvgDs37+fmzdvcvPmTWrVqqX0ekkquZGv8Do+gwcPZv78+Up3KiZPnsyKFSs4cOBAqWsICIKhkUml/UUYiZSUFBwdHQEw9/Sg1vT3dRyRoA3Jh4+QuP33/O+Tkyt0m1qXCtdL28bN8Bw5VscR6Q9jSvgTVm3iydlLgGHVy+TDh3Ho2pUFe67w7dFIJnStw8y+jXQdnqAhit+zAdRJbbOzs+PcuXPFZv25efMmLVq0IC0tTUeRCYJmqdWsWKdOHR49elRsf1JSEnXq1Kl0UNqS+7B4zIJxyIq8o+sQ1JYVE63rEAQtyb5j2L/b9GwxD79g3Jydnfn999+L7f/9999xdnbWQUSCoB1qfYrfuXNHaTnsAllZWdy7d6/SQWmLmYv44zVWlv5+pJ87r+sw1GLp7VP+QYJBsvDz4cnjZF2Hobr/Fjd8IgbtCkZu3rx5vPHGGxw6dIj27dsDcPr0afbt28e3336r4+gEQXNUSvh37typ+P7PP/9UdEkAyMvLIzQ0FH9/f81Fp0Fm7m54vhdc/oGCQXLo2gUpJ4fHu/fqOhSV2DRojPtrQeUfKBgklwkjefDNRjIvXNV1KKpxcgIgPeu/aTlFH37BSI0ZM4ZGjRqxfPlyxbSgjRo14tixY4oLAEEwBiol/AUDd2UymdKgF8gfXOPn58cXX3yhseA0yXvqFEzEgF2jJZPJcOjcyeASfo9hI8WA3SLMH5b+sWRo/ftNTExwHfcq0VPm6DoU1SQkgIODYlpOMUuPYMzat2/P5s2bdR2GIGiVSp/iBfPe+vv7c+bMGVxcXLQSlCAIQknKuhjQV/JMw4uZ+/chIIB0sfCWIAiCUVDrP1FkZGSFjmvWrBl79+4ttny3IFSIm4pLlGbo8ZKmpchxzsXE+mmrtSEmtILxyvhvWk5bS1EvBUEQDJlWP8Xv3LlDTk6ONk9RYZYPZJhaVmxlvaKq46q1Gqdq8l5NVWWXFXFxIZQnIye/PlqLFn5BEASDZlD/8RctWsTMmTOZMmWK0kp52maVIBNJf2EieTcKVd0fXlxgGB5FC7/owy8IgmDQDGa04JkzZ1izZg3NmzfXdSiCIAjG7b/FmEQffkEQBONgEM02aWlpjBgxgm+//ZZPPvlErTLu7/gB7yGvixlRjFROTg73Jn2m6zBUdm/2YrwWfYCZmUH8KQoqysvL496qr3Qdhurq1CFPLpGZkz9Rg+jDLxiTQYMGVfjYgqk6BcHQGUT2O3HiRPr160evXr3KPTYrK4uUlBSlDSDt1hXu/7ZR26EKOhLzv4WQma3rMEpVWr2UUtKInblQx9EJ2hKz4ktyHsTrOgzV5eSQkf20y5do4ReMiaOjY4U3QTAWet9s89NPPxEeHs6ZM2cqdPzChQuZN29eic89iTXsZe6F0klPsnQdQpnKqpfylLQqjkaoKjmPHuo6BPVcvsyTFu0AMJGBpZlBtA0JQoWEhIToOgRBqHJqJ/yhoaGEhoaSkJCgmJ+/wPr16wFYs2YN7u7uagcXHR3NlClT2L9/P1ZWVhV6zcyZMwkOfrqibkpKimJaUGtPMT2osZJZW+p10l9WvTRxsNNVWIKWmTu7kB0fq+sw1JJeaNEtmUy9Gc4EQRAE/aBWwj9v3jzmz59PmzZt8PT0LPWfwWuvvVap4MLCwkhISKBVq1aKfXl5eRw5coQVK1aQlZWFaZHVcy0tLbG0tCxWll3dxngNCiq2XzAO3qtncm/Cp3rbrae0eilzsMNz4UwdRCRUBe9JwdxbvsQgu/WkZ/03YNdSdOcRjNsvv/zCtm3biIqKIjtb+X9IeHi4jqISBM1SK+FfvXo1GzZsYNSoUZqOR0nPnj25ePGi0r6xY8fSsGFDpk+fXizZL4vXwJFiwK4RMzc3x3fFdO6+8bGuQ1FJrY+nYSIG7BotU1NTfN6cwu35H+g6FJVlZIspOQXjt3z5cj788EPGjBnD77//ztixY7l16xZnzpxh4sSJug5PEDRGrU/y7OxsOnbsqOlYirG3t6dp06ZK+2xtbXF2di62XxAEQdAcxZScooVfMGLffPMNa9eu5dVXX2XDhg1MmzaNOnXqMGfOHBITE3UdniBojFpN3m+88QZbtmzRdCyCIAiCPmjWjCf/tfDbmIsWfsF4RUVFKRowra2tSU1NBWDUqFH8+OOPugxNEDRKrU/yzMxM1q5dy4EDB2jevDnm5uZKz3/55ZcaCa4khw4dUut1Wa4SJlZGsFquWOVWMFBVvbJvYWKVXxWZmoo+/EK14OHhQWJiIr6+vtSuXZtTp04RGBhIZGQkkmQEOYMg/Eet/4IXLlygRYsWAFy6dEnpOb2dzcElE2x0HYSgsoSKzc4EgAFeC5k/MsPEKv/PUJcJsbHT5c9W/sQAf6+3bpGR7QSIPvyCcXv22WfZuXMnLVu2ZOzYsUydOpVffvmFf//9V6UFugRB36n1SX7w4EFNxyFUF6ok8NVMZVqhxcWC/rFyfgJAXoYBXommppIutwfEoluCcVu7dq1iavGJEyfi7OzMiRMnGDBgABMmTNBxdIKgOdWn6ebkYTyHtdPYHYj7D2popBxBMyRJIuXYcV2HIeixggS8KsnlcmKX/VLl59WEjKz/ZumxrD7/JoTqx8TERGkGv+HDhzN8+HAdRiQI2lFtPsnvrjuCiYUZXoNa6zoUQQtSjhzl8e69ug5DEJTEfr6V9LAbug5DLQXTclqLFn7ByFy4cIGmTZtiYmLChQsXyjy2efPmVRSVIGhXtUn4AVKuxIiE30hlRd7RdQiCUEzmrfu6DkFtGf9Ny2krEn7ByLRo0YK4uDjc3Nxo0aIFMpmsxAG6MpmMvLw8HUQoCJpXrRJ+h8beug5B0BJLfz/Sz53XdRiCoMSqrhdpj1J0HYbqatUiPSYZABsxaFcwMpGRkbi6uiq+F4TqoNosPes7riueL7fSdRiCljh07YLTi311HYYgKPF8fxi2revpOgzVubiQ8d+0nLZiWk7ByPj6+irG8929exdvb298fX2VNm9vb+7evavjSAVBc6pNwu/Rv0WlBuzef1BDaRNUkGD1dNMSmUyGQ+dOWitf35k/NFNsQskyH1krbVXBxMQEz3deqZJzaVRi4tOVdkULv2DEevToUeKKusnJyfTo0UMHEQmCdlSbT/K4h46YZIgpIdUmptM0GJpK+o19qk9tJv26mBFIo6KieJKd3x4kpuUUjJkkSSU2Bj569AhbW1sdRCQI2lFtEn4BkbQLKtHEhYOxXzSUpvDFhPyJni5GWI70/2bpES38gjEqWFRLJpMxZswYLC0tFc/l5eVx4cIFOnbsqKvwBEHjxCe5oRLJu2AAKnvRUF0vGPSB6MMvGDNHR0cgv4Xf3t4ea+unF+kWFhY888wzjB8/XlfhCYLG6X3Cv3DhQn777TeuXbuGtbU1HTt25LPPPqNBgwa6Dk0QBMFoiRZ+wZiFhIQopuL8+uuvsbOz03FEgqBdej9o9/Dhw0ycOJFTp06xf/9+cnJyeP7550lPT9d1aIIgCMbJ1vbpPPyihV8wUpIksXnzZmJjY3UdiiBond4n/Pv27WPMmDE0adKEwMBANmzYQFRUFGFhYSqVc/d/C8jOztZSlIKu5ebmErVgoa7DUNntT2aJemnEcnNzuTd7sa7DUFm2f11y8vJbP23MRQu/YJxMTEyoV68ejx490nUogqB1ep/wF5WcnL8YTM2aNUt8Pisri5SUFKUNgFw5MeM/qaowhSoWM/8TpNQ0XYdRqlLrpVxO1Mcf6jY4QWtiZy5EStHfelmaJ9lPVxe1FrP0CEZs0aJFvP/++1y6dEnXoQiCVhlUwi+Xy3nnnXfo1KkTTZs2LfGYhQsX4ujoqNh8fHyePpkjlsg2VnI9TvahnHoplm43WnIDTPYBcsLDAbAwNcHCzKD+TQiCSkaPHs0///xDYGAg1tbW1KxZU2kTBGNhUPdqJ06cyKVLlzh27Fipx8ycOZPg4GDF45SUlKfJlbloqTJWJvZ2yFNSdR1Gqcqsl6aiXhorEwc75Mn6Wy9Lk/lf44iN6L8vGLlly5bpOgRBqBIGk/BPmjSJ3bt3c+TIEWrVqlXqcZaWlkrz6SqYmeD97SwtRijokvecWdyb97HedusptV6amFB79oKqD0ioEp4LZ3J/xqcG160nM0cOgK2YoUcwckFBQboOQRCqhN7fq5UkiUmTJrF9+3b+/vtv/P391SrHd/WHWFhYaDg6QV+YmZlR+8OZug5DZXVmfSLqpREzMzOj1sfTdB2Gygpa+EX/faE6yczMLHmslSAYAb1P+CdOnMgPP/zAli1bsLe3Jy4ujri4OJ48MfCl6wVBEPRUWmYOALYi4RcKWblyJX5+flhZWdG+fXv++eefUo+9fPkygwcPxs/PD5lMVmLXmYLnim4TJ05UHBMXF8eoUaPw8PDA1taWVq1a8euvvyqVs2DBAjp27IiNjQ01atRQ6T2lp6czadIk3NzcsLW1xcnJSWkTBGOh9wn/qlWrSE5Opnv37nh6eiq2rVu36jo0QRAEo/RWeH6Dilh0SyiwdetWgoODmTt3LuHh4QQGBtK7d28SEhJKPD4jI4M6deqwaNEiPDw8SjzmzJkzxMbGKrb9+/cDMGTIEMUxo0ePJiIigp07d3Lx4kUGDRrE0KFDOXv2rOKY7OxshgwZwptvvqny+5o2bRp///03q1atwtLSku+++4558+bh5eXFpk2bVC5PEPSV3n+aF6yEJxThlqnrCPRPhuH9THKcczGxztV1GIKgJNvMAlMZdGvgqutQBD3x5ZdfMn78eMaOHQvA6tWr2bNnD+vXr2fGjBnFjm/bti1t27YFKPF5AFdX5fq1aNEi6tatS7du3RT7Tpw4wapVq2jXrh0As2bNYunSpYSFhdGyZUsA5s2bB8CGDRtUfl+7du1i06ZNdO/enbFjx9KlSxcCAgLw9fVl8+bNjBgxQuUyBUEf6X3Cr4+8XJN0HYJQAtfbCdzVdRAq6mx/lcd1VUuqIh86aykaZZmPrKvkPIL+OTMyAMfmzbESM5sJ5Legh4WFMXPm03FSJiYm9OrVi5MnT2rsHD/88APBwcHIZDLF/o4dO7J161b69etHjRo12LZtG5mZmXTv3l0j501MTKROnToAODg4kJiYCEDnzp3VumMgCPqq2iT8Hi7JmNkaXguwtjSveV/XIWhcx18vskPXQajo7YsHie1a8u3uAqfSApQeN3PU3u/uYrLX0wcuWjuNuGjRE1lZWWRlZSkeFwxStE5PFcl+NVB0UGpps4k9fPiQvLw83N3dlfa7u7tz7do1jcSyY8cOkpKSGDNmjNL+bdu2MWzYMJydnTEzM8PGxobt27cTEBBQckEqqlOnDpGRkdSuXZuGDRuybds22rVrx65du1QeDyAI+qzaJPz6xhgTblV0drih8TI7HLup8TK1rd7fD/F9N78161Sme4nHPGOn2fdV9AKiME1fTChdQBTi76LZpexLu4Cwctbs4H5ju4BYuHChojuEUP0oLQAIzJ07l48++kgnsaxbt44XXngBLy/lz4zZs2eTlJTEgQMHcHFxYceOHQwdOpSjR4/SrFmzSp937NixnD9/nm7dujFjxgz69+/PihUryMnJ4csvv6x0+YKgL6pNwn964HJa7HgLW1tbXYciVJYkUW9zAq7hT+c2z87KwulShg6DUo/Z+VxM34jD0sqSNvL8O1CJLW24PdIFCt3WFgzTkydPSJgyV9dhlKrMBeEEoxcdHY2Dg4PicYlrhQAuLi6YmpoSHx+vtD8+Pr7UAbmquHv3LgcOHOC3335T2n/r1i1WrFjBpUuXaNKkCQCBgYEcPXqUlStXsnr16kqfe+rUqYrve/XqxbVr1wgLCyMgIIDmzZtXunxB0BdGn/AXHvR7buA3tN8xWYfRPJVtkaPrEHQqQ5ZXqddfeNGZJvcyabg+HpP/fsUFN6cNYaB3QYxpgMM+iVwycZRlcuN1VyL6OyGlybV27qz0qqt7uelZ5R+kAXlVNGBb/kS1i7DCyb4+1suiXTgKYkxJTwcxB7nRKujKY29vr5Twl8bCwoLWrVsTGhrKwIEDAZDL5YSGhjJp0qRKxxMSEoKbmxv9+vVT2p+Rkd+IY2KiPKGgqakpcnnlPiPlcjmff/45O3fuJDs7m549ezJ37lx8fX3x9fWtVNmCoI+MPuFPTVVe1v70wOU6ikTZaV0HoGPrtFh2amoqjo6OWjxD5RXUS6W2VAlY9yB/06qLWi5fKIlB1cu+fXUciVAVVKmTwcHBBAUF0aZNG9q1a8eyZctIT09XzNozevRovL29WbhwIZA/CPfKlSuK72NiYjh37hx2dnZK/e/lcjkhISEEBQVhZqackjRs2JCAgAAmTJjAkiVLcHZ2ZseOHezfv5/du3crjouKiiIxMZGoqCjy8vI4d+4cAAEBAdjZ2ZX4fhYsWMBHH31Er169sLa25quvviIhIYH169dX7IcnCAZGJuljs5MGyeVy7t+/j729vdLIf8H4SJJEamoqXl5exVqE9I2ol9WHqJeCvlG3Tq5YsYLPP/+cuLg4WrRowfLly2nfvj0A3bt3x8/PTzE15p07d/D39y9WRrdu3Th06JDi8V9//UXv3r2JiIigfv36xY6/ceMGM2bM4NixY6SlpREQEMB7773HqFGjFMeMGTOGjRs3FnvtwYMHS53Np169erz33ntMmDABgAMHDtCvXz+ePHmi93+ngqAOo0/4BUEQBEEQCrO0tOTmzZtKY1asrKy4efMmtWrV0mFkJfvoo4/YsWOH4u6FIKhKXMYKgiAIglCt5ObmYmVlpbTP3NycnJzqPb5OMF5G34dfEARBEAShMEmSGDNmjNKg9czMTP73v/8pzeZXdOag6kySJPLy8oqNtRAMg2jhFwRBEATBqHXv3p3Jkyczbdo0atasiaWlJTExMTg6Oiq2kSNH4uXlpbQP4Nq1a3Tu3BkrKysaN27MgQMHkMlk7NixA8gfryCTyfjpp5/o2LEjVlZWNG3alMOHDyvOv2HDhmILee3YsUPtsTJnzpzhueeew8XFBUdHR7p160Z4eLji+ddff50XX3xR6TU5OTm4ubmxbl3+tBlyuZyFCxfi7++PtbU1gYGB/PLLL4rjDx06hEwm448//qB169ZYWlpy7Ngxzp8/T48ePRSzPLVu3Zp///1XrfchVB1xmSYIgiAIgtHbuHEjwcHBnD59mpMnTzJmzBg+//xznnvuuVJfk5eXx8CBA6lduzanT58mNTWVd999t8Rj33//fZYtW0bjxo358ssv6d+/P5GRkTg7a35l8dTUVIKCgvj666+RJIkvvviCvn37cuPGDezt7XnjjTfo2rUrsbGxeHp6ArB7924yMjIYNmwYkL/w3g8//MDq1aupV68eR44cYeTIkbi6utKtWzfFuWbMmMGSJUuoU6cOTk5OdO3alZYtW7Jq1SpMTU05d+4c5ubmGn+PgoZJgiAIgiAIRqxbt25S586dlfa1bdtWmj59epmv++OPPyQzMzMpNjZWsW///v0SIG3fvl2SJEmKjIyUAGnRokWKY3JycqRatWpJn332mSRJkhQSEiI5Ojoqlb19+3apomnY3LlzpcDAwFKfz8vLk+zt7aVdu3Yp9jVu3FhxfkmSpP79+0tjxoyRJEmSMjMzJRsbG+nEiRNK5YwbN0569dVXJUmSpIMHD0qAtGPHDqVj7O3tpQ0bNlQobkF/iC49giAIgiAYvaIr53p6epKQkFDmayIiIvDx8VFaUbhdu3YlHtuhQwfF92ZmZrRp04arV69WIuLSxcfHM378eOrVq4ejoyMODg6kpaURFRWlOOaNN94gJCREcfwff/zB66+/DsDNmzfJyMjgueeew87OTrFt2rSJW7duKZ2rTZs2So+Dg4N544036NWrF4sWLSp2vKCfRJceQRAEQRCMXtFuJzKZrNIr9laUiYlJsdW2KzMjUFBQEI8ePeKrr77C19cXS0tLOnToQHZ2tuKY0aNHM2PGDE6ePMmJEyfw9/enS5cuAKSlpQGwZ88evL29lcouPJAZUBrEDPlThL722mvs2bOHP/74g7lz5/LTTz/x8ssvq/1+BO0TLfyCIAiCIAglaNCgAdHR0cTHxyv2nTlzpsRjT506pfg+NzeXsLAwGjVqBICrqyupqamkp6crjqnMnPrHjx9n8uTJ9O3blyZNmmBpacnDhw+VjnF2dmbgwIGEhISwYcMGxarIAI0bN8bS0pKoqCgCAgKUtsJrE5Smfv36TJ06lb/++otBgwYp7iQI+ku08AuCIAiCIJTgueeeo27dugQFBbF48WJSU1OZNWsWQLEZdlauXEm9evVo1KgRS5cu5fHjx4ouNO3bt8fGxoYPPviAyZMnc/r0acWqxOqoV68e33//PW3atCElJYX3338fa2vrYse98cYbvPjii+Tl5REUFKTYb29vz3vvvcfUqVORy+V07tyZ5ORkjh8/joODg9KxhT158oT333+fV155BX9/f+7du8eZM2cYPHiw2u9FqBqihV8QBEEQBKEEpqam7Nixg7S0NNq2bcsbb7zBhx9+CFBs4a5FixaxaNEiAgMDOXbsGDt37sTFxQWAmjVr8sMPP7B3716aNWvGjz/+yEcffaR2XOvWrePx48e0atWKUaNGMXnyZNzc3Iod16tXLzw9PenduzdeXl5Kz3388cfMnj2bhQsX0qhRI/r06cOePXvw9/cv8+fx6NEjRo8eTf369Rk6dCgvvPAC8+bNU/u9CFVDJhXtVCYIgiAIgiCU6Pjx43Tu3JmbN29St25d7ty5g7+/P2fPnqVFixa6Dk9JWloa3t7ehISEMGjQIF2HI+iQ6NIjCIIgCIJQiu3bt2NnZ0e9evW4efMmU6ZMoVOnTtStW1fXoZVKLpfz8OFDvvjiC2rUqMGAAQN0HZKgY6JLjyAIgiAI1dLmzZuVpqUsvDVp0gTIX+Rq4sSJNGzYkDFjxtC2bVt+//13jcbRpEmTUuPYvHmzyuVFRUXh7u7Oli1bWL9+PWZmon23uhNdegRBEARBqJZSU1OVZuApzNzcHF9f3yqJ4+7du6VO0+nu7o69vX2VxCEYL5HwC4IgCIIgCIIRE116BEEQBEEQBMGIiYRfEARBEARBEIyYSPgFQRAEQTBqCxcupG3bttjb2+Pm5sbAgQOJiIhQOiYzM5OJEyfi7OyMnZ0dgwcPLrV/f3kWLVqETCbjnXfe0Wj5MTExjBw5EmdnZ6ytrWnWrBn//vuv4nlJkpgzZw6enp5YW1vTq1cvbty4UeHy8/LymD17Nv7+/lhbW1O3bl0+/vhjCvf+ruw5BN0QCb8gCIIgCEbt8OHDTJw4kVOnTrF//35ycnJ4/vnnSU9PVxwzdepUdu3axc8//8zhw4e5f/++WnPXnzlzhjVr1tC8eXOl/ZUt//Hjx3Tq1Alzc3P++OMPrly5whdffIGTk5PimMWLF7N8+XJWr17N6dOnsbW1pXfv3mRmZlboHJ999hmrVq1ixYoVXL16lc8++4zFixfz9ddfa+wcgo5IgiAIgiAI1UhCQoIESIcPH5YkSZKSkpIkc3Nz6eeff1Ycc/XqVQmQTp48WeFyU1NTpXr16kn79++XunXrJk2ZMkVj5U+fPl3q3Llzqc/L5XLJw8ND+vzzzxX7kpKSJEtLS+nHH3+s0Dn69esnvf7660r7Bg0aJI0YMUJj5xB0Q7TwC4IgCIJQrSQnJwNQs2ZNAMLCwsjJyaFXr16KYxo2bEjt2rU5efJkhcudOHEi/fr1UypHU+Xv3LmTNm3aMGTIENzc3GjZsiXffvut4vnIyEji4uKUzuHo6Ej79u0rfI6OHTsSGhrK9evXATh//jzHjh3jhRde0Ng5BN0QKzEIgiAIglBtyOVy3nnnHTp16kTTpk0BiIuLw8LCgho1aigd6+7uTlxcXIXK/emnnwgPD+fMmTPFntNE+bdv32bVqlUEBwfzwQcfcObMGSZPnoyFhQVBQUGKctzd3dU+x4wZM0hJSaFhw4aYmpqSl5fHggULGDFihOJ9VPYcgm6IhF8QBEEQhGpj4sSJXLp0iWPHjmmszOjoaKZMmcL+/fuxsrLSWLmFyeVy2rRpw6effgpAy5YtuXTpEqtXryYoKEgj59i2bRubN29my5YtNGnShHPnzvHOO+/g5eWlsXMIuiG69AiCIAiCUC1MmjSJ3bt3c/DgQWrVqqXY7+HhQXZ2NklJSUrHx8fH4+HhUW65YWFhJCQk0KpVK8zMzDAzM+Pw4cMsX74cMzMz3N3dK1U+gKenJ40bN1ba16hRI6KiohTvoaBMdc/x/vvvM2PGDIYPH06zZs0YNWoUU6dOZeHChRo7h6AbIuEXBEEQBMGoSZLEpEmT2L59O3///Tf+/v5Kz7du3Rpzc3NCQ0MV+yIiIoiKiqJDhw7llt+zZ08uXrzIuXPnFFubNm0YMWKE4vvKlA/QqVOnYlOJXr9+HV9fXwD8/f3x8PBQOkdKSgqnT5+u8DkyMjIwMVFODU1NTZHL5Ro7h6AbokuPIAiCIAhGbeLEiWzZsoXff/8de3t7RX9zR0dHrK2tcXR0ZNy4cQQHB1OzZk0cHBx4++236dChA88880y55dvb2yvGAxSwtbXF2dlZsb8y5UP+tJ4dO3bk008/ZejQofzzzz+sXbuWtWvXAijm/f/kk0+oV68e/v7+zJ49Gy8vLwYOHFihc/Tv358FCxZQu3ZtmjRpwtmzZ/nyyy95/fXXNXYOQUd0PU2QIAiCIAiCNgElbiEhIYpjnjx5Ir311luSk5OTZGNjI7388stSbGys2ucsPC2npsrftWuX1LRpU8nS0lJq2LChtHbtWqXn5XK5NHv2bMnd3V2ytLSUevbsKUVERFS4/JSUFGnKlClS7dq1JSsrK6lOnTrShx9+KGVlZWnsHIJuyCSp0PJpgiAIgiAIgiAYFdGHXxAEQRAEQRCMmEj4BUEQBEEQBMGIiYRfEARBEARBEIyYSPgFQRAEQRAEwYiJhF8QBEEQBEEQjJhI+AVBEARBEATBiImEXxAEQRCEaisrK4uPPvqIrKwsgyy/Ks5RFe9BUJabm8uBAwdYs2YNqampANy/f5+0tDS1yhPz8AuCIAiCUG2lpKTg6OhIcnIyDg4OBld+VZyjKt6D8NTdu3fp06cPUVFRZGVlcf36derUqcOUKVPIyspi9erVKpcpWvgFQRAEQRAEQU9MmTKFNm3a8PjxY6ytrRX7X375ZUJDQ9Uq00xTwQmCIAiCIAiCUDlHjx7lxIkTWFhYKO338/MjJiZGrTKNPuGXy+Xcv38fe3t7ZDKZrsMRtEiSJFJTU/Hy8sLERL9vXol6WX2IeinoG1EnlaWkpCh9NbTyq+IcVfEeDKleaptcLicvL6/Y/nv37mFvb69WmUbfh//evXv4+PjoOgyhCkVHR1OrVi1dh1EmUS+rH1EvBX0j6qSgjwyhXmrbsGHDcHR0ZO3atdjb23PhwgVcXV156aWXqF27NiEhISqXafQt/EWvhAJHf6rxczxx1XiRAGS5aulazCVT40V6uCRrvEyApk5xxXdKEh133MTv0iPFrtsHonge8KX471wfFcR4F/gLcO5aX/Hc1Yae7HyhOchkxD101MwJH1pVugjLB5VvXbN+UOkiALCNz610GdYx6s10oOReCfWzkNCkTYrvDbFe2rZojuUjsIhP45q5G39aNQKZjHPJB3iYE6V4nYt5bVo49gIg19+TVL/8PqfpnmW30tnGypUeX/5zOZkpCYrHVg5uNOk9uVhZT7wkUo4d5/HuvYp9bl374ePeIf993HmiVO7xs1+Rk/v0921uZk+nlpOVjimIuTwFZZtFxgJw6vF20vIeK563M3XiGaeXVfo5VNbD8KM8PPaH4rFL5xdwadVF8fiJV/7/ElPPdJL+/IfELQfy34sB1cno6GgxWNRYnTsH3bqRsncvPn37GkS91LYvvviC3r1707hxYzIzM3nttde4ceMGLi4u/Pjjj2qVafQJf9FbgKYWlU98CnviBqYaLREy3fI/nDX6L8KtcJKvmZ+Bl2tSoUeWGimzec37RfaYl3hc+KsN8Vpzgec3XMFELvEsUHCj0RC6IhTEWAN4A+DIdfJMZGwe1p4/B7XBzDT/t1/LNv/3dv9BjcqdsPZ/XxPU/93n/NfIZpWg/s83u1CjjXVC6ceVJ/O/WOzi1E/8c/zyfxbW0anqB+Lr+/T7qNhiT5tgipz827IGWS/PXSBPJuNn6xYctGmOmSy/XtY091JK+Guae2Fm8l9fUzMrzMytSPM2Kfez0cxcOeFv+sK7XN63lKy0R1jaOdOkz1RMTfNLMbV8+oloYiXh2PNZZObmZEXewdLfD5e6XTG7n//ZaWam3Fji69WZm1H7Cj3uhJmZ8t+CmXnF/jYKyi54v7WsGnEt/YTi+VpWjfKf++/nUDR2bciKiy722NTy6fsxsZIw804HrJCZP/23b0h10sHBQST8xsrOLv+rrS1gGPVS22rVqsX58+fZunUr58+fJy0tjXHjxjFixAilQbyqMPqEX5ueuGm2vIJEX2PcNNuSr5zga0bxBL9i5GYm7JzYgmvtPBgz+wQ1Hjwp/0V6LMHFjo+n9+Nsi9olPl/4Z1+p5L+gTlQi8S+op5VJ/OHp309lEv80j/yPsMok/k98nrYmVSr5r+2Z/7VQ4m+GJdlkqF+mjsU6OvJxm1eJvKi8PyUvochjzdy6MTU1pXm/9yp0rEwmw7FbV+jWNf9xjIw0bxl2MfJix/p6dcTExITk1Cgc7Wvj4/FMpWPNreuN2a0YfG2agQyScuKpYe6Or3WzSpetqifx0UUe3yv12MyIqFKfEwRBf5iZmTFixAhGjBihmfI0UoqhkGmmlUUk+pWjbpJfkuttPTi+sS79+l7SWJm6ELxoCHd8XSp0rJdrUuVb/DWY+EPlkv/Cf0/qJv8FiT/oQfJfkPgDNfJ8SEiJUDseXQt68/9IiXLH6eJNpf01zD2Iy7pd6LF7VYemEplMRm3PDuDZQaPl5tb1BqAWtSi4eVV8mJ32WXvUJjX1YqHHpfd5z7n/qNTnBEEn7O3h+efzvwoALFy4EHd3d15//XWl/evXr+fBgwdMnz5d5TKrVcJv4VC5zvYi0VefJpN8gM4ONxTfu+7WQF9sHQu8eK/CCT9oKOkHjST+oJ+t/gXUvQAonPyDehcAgb6vcPbOVh6m3iz/YD3U9lYkoSUk8wWt2Lpq1baJEbf8C6s1IIh7OzfyJC4aaw8fag0IKvXYvDTDveMkGKl69Yj+cTsf/XpG15HojTVr1rBly5Zi+5s0acLw4cNFwl8e14bq3catTom+PrfmF07yC/P5MxGAaxo7U9W5BrQDuh29zu8vtlDptRpL+iG/DlUy6QfNJ/5QueQftNP6DxW7ADAxMSHQdxChlxarfV5dKKiXL0ScJbRp8c9NmUyGn03zKo9LKJmJiQm1B46t0LFWAbVI/+eqliMShIrJyM5ldeh1vv/7Ko+zc3Qdjt6Ii4vD09Oz2H5XV1diY4uPF6uIapPwO/g0xrVJl/IPLESTib4+J/lgOK35RVk8zsH9n1QOtrCl77l0jZ63KnQD1jf0YMj5aOxTnpDqoNpgHI0n/QU01OJfQFNdfgpooutPYepcCBS9ACisUmMB9EA3YGegA90vReJQr3q1CEuSRHzEMVIfRmLv4o97g85GMYgwN8YWM+90PN4dRuziLWSEXdd1SEI1dzU2hTc2/kuNaxc5u/Edgt9bxVJdB6UnfHx8OH78OP7+/kr7jx8/jpeXl1plVpuEPyX6CgmXjuDRvHuZx4kkX31VmegXcDuTyvEv67IuNpNMA0z4M4G3athwZeaLtLwQzZHO9ct9TVEaTfoLaKirT4GS/hY0fRFQQJ2LgdIuBApT5aKg4GJAkiTu3fxX9YB0LBNY0duFHS0H0vLWLcLrBmjlPGne+rG4Tqr/0wvt+Ihj3A3/HYDEqAsAeDRUrbFIn8lkMqwa1BYJv6BTh68/YOLmcNKycqlvnz/L39wXG7F0iY4D0xPjx4/nnXfeIScnh2effRaA0NBQpk2bxrvvvqtWmdUm4Qd4cOlwsYS/OnXXAf1P8qFiiX6Be885gUzG0f4Xyz9YTyWH3+Xgx4OhEmvgaSXpB422+hel6YuAApq+GCigzkXB/cjjREb8UcrR+u3P9XF4zQrExgU8j1efVv7Uh5FFHt/Bg6cJf6q/NfaRhjsrWNLeU4p5+AVBF378J4pZOy6RJ5d4pk5NljdxhRViOs7C3n//fR49esRbb71FdnY2AFZWVkyfPp2ZM2eqVWa1Svjlufk/NE0n+VD9En3QTYt+Mf99QKQ+rvxCTDqT91/dqeSHndaS/gJaTP4LaLIrUEm0dTFQoOhFwaPLdytfqI7kpGblf6NivSzcWm6I7F38FS37+Y/91CpHX38OYlpOQZf2XIhl5m/5DXSDWnqzaHBzLC6c021Qekgmk/HZZ58xe/Zsrl69irW1NfXq1cPSUv01j6pVwm/tW1e/u+yAxhN9MIxkv7Lsa5rxON4wB/yY1dDPxKBMVZD8g/buApSktM+GylwI2Ln7kxR5Xv0CdMjcXjOL6em7oom5e4PO+fsf3sHexU/xWF360m2pgFWD2qSdvKzrMIRqKDb5CR9sz0/2x3byY86LjUWrfjns7Oxo27atRsqqNgm/Xd3GeA8eo5GyDCXRh+qR7AN0edmZzVdLX2xGn3m9opk/Zp3RcH//8lTlRQBU7q6AS+POpMRcJyX6imaDqgJ1h7dAEx1XJEkiMfwoGTGR2Hj7U7NVF63/k69M67pMJsOjYRelbjwllW9I3XpsYmRkeEvkxthiF9iTvBdlPN69V9dhCdWIXC7x3s/nSX6SQ/NajnzQt9HTz4FmzSAhAUz06+JYl9LT01m0aBGhoaEkJCQglysvKHj79u1SXlm6apPwew0ciUklKpNWknwwqERf246l1APU69ojwzBbCXzHdcV7cBuNlaf1bj1lKVqXq+gCAEr/+9TmhQCU3z0w9e9jBpnsN5rwDHWHtuBSeOXLSgw/StzfOwBIici/2+HcumvlCxbUIpPJcOjcSST8QpUKOXGH4zcfYWVuwtJhLTA3LZSPmZuDqyukpOguQD3zxhtvcPjwYUaNGoWnp6dGGkmqTcL/OOw4zh2erdAPTWvJfQEtJfkFDDHZL+xYSj2Vk/7r4Ya5+FbKpXt4DW2r0RZPnSb9hZVUz6vwIgBU+1vWxsVBclJk+QfpqatxTxfcqkyLeUZMZJHHd4wi4dfXPvrlkSSJlGPHdR2GUI2ci07is335K+XM6teYuq52ygfcugVTp8L8+TqITj/98ccf7Nmzh06dOmmszGpz/+TBoT08PnNUaV+mm1TipnFumcqblni5Jhl8sl/gWEo9RYt/RcjztHyRpiWPT98mYv5OjZert3Wh6N+Clv8mVFHa50FlPh9savmXf5AeurrmFEl7T6n9+sL91m28lX8GNt5+apcrqK9gdeKUI0dF675QZQ5eS+DVtafIzpXzbEM3RrSvXfyg5GTYtUu08Bfi5OREzZo1NVpmtWnhB0h7GImNWxXMp1zFCYxeJnYaUjjpL6vV38TMMLv0AKRdj9Na2XrT2l+e0v5mqviOQEVUNOm3SpDh1LYL8twcHhzao+WoNC8zIhr7FpUvp2ar/M/cjJg72Hj7KR4LupEVeUfXIQjVROHpN7vUc+Gr4S3EIN0K+vjjj5kzZw4bN27ExsZGI2VWq4Tf0t9POwXrqIVSV4n+hUQvnQzcLdriX/gCoH5LO/7543FVh6QRtvXcyz+oEgrXE4NI/gszoAuBogouDGz6dAQDTPgt69fSSDkymQzn1l2NohuPMbD09yP9nOHNHHUvMYMT5x9yJTaF5t6OdK3vir+LrUgg9dT6Y5HM350/fumV1rVYOKiZcr99oUxffPEFt27dwt3dHT8/P8zNzZWeDw9XfYBVtUn4rRs3wr5L5aZXA/Si+4Ext+irQukCQBaru0AqybG5T5Wdq2jdMbgLgAJ6MD6gIkR/aUGf2MTIsK7bFbOuuSQcMayL0D5fHcXEMr+l87fwGABqOVkzq18j+jT11GVoQhEnbz1iwd6rAEzsUZf3nm8gLsxUNHDgQI2XqdblVnh4OBcvPl3Z9Pfff2fgwIF88MEHihXB9M2TK1dJPXpMtRfpWV9jfeqXfSHRS2nTtZOnDffDJPWq7qY5LahT+lKvKkUPxwcYcn/p1MPndB2CoAUymYyaLTQ3ELCqmMjgmTo1eat7XTrWdcbC1IR7j5/w1uZwtv0brevwhP/EJWfy9o/h5MklXm7pXbFk39sbvvgCvNTLJVauXImfnx9WVla0b9+ef/75p9RjL1++zODBg/Hz80Mmk7Fs2bJixxQ8V3SbOHHi0/cZF8eoUaPw8PDA1taWVq1a8euvvyqVM2DAAGrXro2VlRWenp6MGjWK+/cr9v9+7ty5ZW7qUCvhnzBhAtevXwfy5wIdPnw4NjY2/Pzzz0ybNq3C5eTl5TF79mz8/f2xtrambt26fPzxx0jS0z6ykiQxZ84cPD09sba2plevXty4ocaKrEBW7O3SkwIdJQqFE67yNn2m6wsAj0DXKj+npjg09tZ1CEDpddHg6fDvuzr2l9a3haYE43Dove789H8dmNanIVvGP8O5uc/xajsf5BJM++UCG44b7oxYxiI7V85bm8N4mJZNQw97Pn25WcVa9t3dITgY3FRfGXXr1q0EBwczd+5cwsPDCQwMpHfv3iQklLxISkZGBnXq1GHRokV4eHiUeMyZM2eIjY1VbPv37wdgyJAhimNGjx5NREQEO3fu5OLFiwwaNIihQ4dy9uxZxTE9evRg27ZtRERE8Ouvv3Lr1i1eeeWVCr+3pKQkvvvuO2bOnEliYiKQ3+AeExNT4TIKU6tLz/Xr12nRogUAP//8M127dmXLli0cP36c4cOHl3jFVJLPPvuMVatWsXHjRpo0acK///7L2LFjcXR0ZPLkyQAsXryY5cuXs3HjRvz9/Zk9eza9e/fmypUrWFmpdgvfsl4Jo8O1xCgSpUoonPRXRX//Zq82IDcrj9Mrzmn9XJrkO64rni+30nUYZSqrLhtslyAoO+nXUPcgQ+0vDWDbop2uQxAEhZp2yis/21iY8enLzbCxMGPdsUg+2nWFpCc5TOlZT3Qf0YE8ucQH2y8SHpWEvZUZq0e2xtrCtGIvfvwYDhyAdqp/5nz55ZeMHz+esWPHArB69Wr27NnD+vXrmTFjRrHj27Ztq1i5tqTnAVxdlRsQFy1aRN26denWrZti34kTJ1i1ahXt/ot51qxZLF26lLCwMFq2bAnA1KlTFcf7+voyY8YMBg4cSE5OTrE++UVduHCBXr164ejoyJ07dxg/fjw1a9bkt99+Iyoqik2bNpX3oylGrYRfkiTFql8HDhzgxRdfBMDHx4eHDx9WuJwTJ07w0ksv0a9fPyD/NsqPP/6ouB0jSRLLli1j1qxZvPTSSwBs2rQJd3d3duzYwfDhwyt8Lqdhz+PQp0OFjy9NdU/k1aGJFv/yLhpkMhlNh9Q3uIS/12gPrmUb7j8ncTFQNoeuXZBycgyuW4/Ti31x7GaYg2zTvE2wi5GXf2AVEHc7tEsmkzGrXyNsLc1YHnqDZQducCMhjSWvBFY82RQqLTtXztRt59hzIRYTGSwd2gI/F9uKFxAZCUOHwuHDqp03O5uwsDBmzpyp2GdiYkKvXr04efKkSmWVdY4ffviB4OBgpQvJjh07snXrVvr160eNGjXYtm0bmZmZdO/evcRyEhMT2bx5Mx07diw32QcIDg5mzJgxLF68GHt7e8X+vn378tprr6n1XtRK+Nu0acMnn3xCr169OHz4MKtWrQIgMjISd/eKzzjSsWNH1q5dy/Xr16lfvz7nz5/n2LFjfPnll4ry4uLi6NWrl+I1jo6OtG/fnpMnT6qU8Nd/tSFmtskVPl7QLxW5aMhNz6qCSDSvvIsZfRgjoY6SLgYM+iKgQOGLgXKSf0Nd1dShcyelf24icRWqSlZWFllZTz/LU8qZm10mkxH8XH08Ha2Y8/sl9lyI5e6jdL4d3QZPR8NcHM2QPMnO438/hHH4+gPMTWV8NbwlvRpXbua5or9zS0tLLC0tix338OFD8vLyiuWd7u7uXLt2rVIxFNixYwdJSUmMGTNGaf+2bdsYNmwYzs7OmJmZYWNjw/bt2wkICFA6bvr06axYsYKMjAyeeeYZdu/eXaHznjlzhjVr1hTb7+3tTVycelN5q5XwL1u2jBEjRrBjxw4+/PBDxRv85Zdf6NixY4XLmTFjBikpKTRs2BBTU1Py8vJYsGABI0aMAFC8qZJ+maW94dI+LOJ2ncN7WDtxq0/QCVX/iRVmTBcERncRUJD8l5L46/ssPZWpl0LZKnuRJEkSieFHyYiJxMbbn5qtulTq/1fhux65OfpxB6QkCxcuZN68eSq/7tV2tanrasf/fgjjUkwKQ9ecZNekztSwsdBClAKAXC7xf9//y9EbD7EyN2HNqDZ0q1/58XQ+Psoz182dO5ePPvqo0uWqY926dbzwwgt4FRlQPHv2bJKSkjhw4AAuLi7s2LGDoUOHcvToUZo1a6Y47v3332fcuHHcvXuXefPmMXr0aHbv3l3u37KlpWWJn8fXr18v1uWoolRO+PPy8khKSuLIkSM4OTkpPff5559jalrx22jbtm1j8+bNbNmyhSZNmnDu3DneeecdvLy8CAoKUjU0oPQPi7vrjmBiYYbXoNZqlSsIlaHuP7GKKOuCwBAuBkrrFmRQFwKltPrr+yw92qyXABnexRcpK1jxVShbYvhR4v7eAUBKRP44kOqwlsHMmTMJDg5WPE5JSSmWAJamnX9Nfp/YiRHfnSYqMYOpW8+xLqgtJiaizmnDD6fvcvTGQ6zNTdk0rh1t/TSzMmx0dDQODg6KxyW17gO4uLhgampKfHy80v74+PhSB+Sq4u7duxw4cIDffvtNaf+tW7dYsWIFly5dokmTJgAEBgZy9OhRVq5cyerVq5VidHFxoX79+jRq1AgfHx9OnTpFhw5ldzEfMGAA8+fPZ9u2bUD+nayoqCimT5/O4MGD1Xo/KjdBmJqa8vzzz5OUlFTsOSsrqwr1TSrw/vvvM2PGDIYPH06zZs0YNWoUU6dOZeHChQCKX5gqv8yZM2eSnJys2KKjn07XZXLzJs1r3ld5E4TKKqteapMh12eDnSmoUPKv77P06KpeGos0b5NSt8rKiIks8vhOpcs0BJaWljg4OChtqvCpacOqka2wNDPhYMQDVh68qaVIq7eoRxks3JvfbWbGCw0rl+xbW0PLlvlfodjvv7SE38LCgtatWxMaGqrYJ5fLCQ0NLTehroiQkBDc3NwU40wLZGRkAPnjBQozNTVVjG8tScFzhe+qluaLL74gLS0NNzc3njx5Qrdu3QgICMDe3p4FCxao+lYANbv0NG3alNu3b+Pv76/WSQtkZGSU+QPz9/fHw8OD0NBQxaxAKSkpnD59mjfffLPEMkvr6wXg0Vy92yCaSpIMobVV0I6y6mVVK1qfDaFeFiT9BtPq75YJCVZ6P0uPNutlSa37Bfu13cqvSsKt7gBfbY9rsPH2V7Ts5z/2U7ssfRnEXFWaeDny8cCmTPvlAl8euE6L2jXoUs9wp27WN3K5xLRfz/MkJ4/2/jUZ9Yxv5Qps1AjCw0GNLoXBwcEEBQXRpk0b2rVrx7Jly0hPT1fM2jN69Gi8vb0VDcnZ2dlcuXJF8X1MTAznzp3Dzs5Oqf+9XC4nJCSEoKAgzMyUU+WGDRsSEBDAhAkTWLJkCc7OzuzYsYP9+/cr+uifPn2aM2fO0LlzZ5ycnLh16xazZ8+mbt26FboYcXR0ZP/+/Rw7dowLFy6QlpZGq1atlMa0qkqthP+TTz7hvffe4+OPP6Z169bY2iqPxq7oFXn//v1ZsGABtWvXpkmTJpw9e5Yvv/yS119/Hci/hfHOO+/wySefUK9ePcW0nF5eXiqvQubbxYumw+ur9BpNExcO2iNJEnG7zuk6DJUdmHWMPl92K3bhW5UM6QLAoBJ/t0yDnaWnOlFnVp+qGMRcs1UXIL9l38bbT/G4suRyObeOb9ZIWfpsaBsfwu8+5qcz0UzcHM43I1rTuZ6LrsMyCj+cvsup24lYm5vy+SuBOu0yNWzYMB48eMCcOXOIi4ujRYsW7Nu3TzH2MyoqSun/6/379xXTZgIsWbKEJUuW0K1bNw4dOqTYf+DAAaKiohT5aGHm5ubs3buXGTNm0L9/f9LS0ggICGDjxo307dsXABsbG3777Tfmzp1Leno6np6e9OnTh1mzZqnUyNK5c2c6d+6s6o+lRDKp8CpXFVT4h1d44IEkSchkMvLy8ipUTmpqKrNnz2b79u0kJCTg5eXFq6++ypw5c7CwsFCUOXfuXNauXUtSUhKdO3fmm2++oX79iiXvKSkpODo6AtDp3dY0f61hRd+mwdPnxE3T7v8WRuSqvwFITk5W+TZwVStcL/171KLPkm7lvEJ39L0e6XvyL8/I5O4bHxtUvfRdtACT/9Y5Ubc1vrQW/sqUKain8AXN9SMbeHzvEmBYn5XqxJqZk8fI707z793HmMjgg76NGNfZX0zeUQnRiRn0XnaEjOw85g1oQlBHv8oXevYsPPMMKQcO4Ni1q0HUS21Yvnx5hY8tWKtKFWq18B88eFCdlxVjb2/PsmXLylyoSyaTMX/+fObPn1/p8+VcuU1nh7IHFR9LqVfp8+gLQ2q5VVXRJC8+LL7kAw1A0tV4OjsUXz1aX+piSXem9KkuFe3jr08XAJIkkbL/tK7DUFnKseM49nxWJEZGKO1RlK5DqDJW5qb88EZ7Zu24xC9h9/hkz1WuxKbw2eDmmJuKqWZVJZdLvP/LeTKy82inia48BSQJsrPzv1ZjS5cuVXr84MEDMjIyqFGjBpC/8q6NjQ1ubm5Vl/AXXm3MkMhzy69MJSVe5dGXxKw8hn4BUGYiJzfcDwo7x5L/DA3pIkCf6lLhCwBdJ/8p+07yeOtfOo1BHY9370Vmbq724ltlte4LumVmYUPOk+oz/aqVuSmfv9KcJl4OfLLnKr+Fx5CdK+er4S0xFbP3qES5K09zMfuRhkVGPh2ov2XLFr755hvWrVtHgwYNAIiIiGD8+PFMmDBBrfLVSvgBjh49ypo1a7h9+zY///wz3t7efP/99/j7+2usv5GmmZppp3JW5CJBXxK1wgonbfqUsKnFgD94PP1V6M9XpK7pS73S17qk6+Q/6/rdKj+npmRG3jHY1XaF0lk5uPIkWb2FewyVTCZjbCd/fJ1tmPB9GLsvxGJlbsriwSJpraioRxks+iN/Vp7pfRrg66zCSrqCymbPns0vv/yiSPYBGjRowNKlS3nllVcU61WpQq17Wr/++iu9e/fG2tqa8PBwxRRDycnJfPrpp+oUWSXyKtDCry2dHW4oNn1kaFM2FmPALfyxkeqvEKyP9UpfpwDVxRSflvU1dMtbF/4biyVa641MNe428WxDd75+Nb9l/5ewe8zdeRk1hjFWO7l5cqb9mt+Vp71/TUZ38NN1SEYvNjaW3NzcYvvz8vKKTVVfUWol/J988gmrV6/m22+/VZp3v1OnToSHh6sVSFVws8igi7Xu+y8WTtL0KVEDA078DbiVJi25+B+1OvS5PulTnarK+f3tez+DdcsG5R+oj9ScOaroBYIkSSQfPkLChk0kHz4iEixdq+bjMvo09WTJkObIZPD9qbtM/ukcT7IrNtFIZSSmZ3MpJpmMbM183leVnDw572w9p+jKs1gbXXkaNYJLl6CBgX5WakHPnj2ZMGGCUk4dFhbGm2++qfbUnGp16YmIiKBr1+K3eh0dHUtckEtf5OXlV9KiSf/RJ7V1EY6CPnbTKEjQ9K17RmndMqwa+JHxz+WqDUhD6jbT7K3RwvVJH+pSAX2tUwW00eUn9c9TPDkbofFyq4JVncqts1Ig5chRErf/DqBYk8A2wDDHgRkDB9c6PI6+qOswdOrllrXIzZOY+dtFdp2/z62ENNaObk0tJxuNlJ+Zk8e/dx5z7OZD/r2TyM0HaSRl5ABgZW5Cz0bu9G/uRY+GrlialT2RiC5l5ebx9paz/HUlHnNTGcuGt9BOVx5ra2jSRK15+I3V+vXrFesLFDSs5+bm0rt3b7777ju1ylQr4ffw8ODmzZv4+fkp7T927Bh16tRRK5CqYF7Kuy18AaDr5B/0K2HTt77Zpc3D7tCnA1J2jsENkGzZw5EpKwLKP1BN+lSXCujrgF9tzPZjqH34nV7si0PXp/O+V6ZbT9HVhrMi74BI+HXGvUFn5Hk5RJ+v3mtDDGnjg6+zLW/+EMaV2BQGrDjOxB4BDG7lTQ0bC7XKvPMwnRUHb7Lr/H2ycouv7eBgZUZKZi57LsSy50IsXo5WvN+nAS8FeuvdWIKs3Dz+930YByMeYGFmwuqRrXi2obt2Tnb3Lnz8MbzzjnbKN0Curq7s3buX69evc+1a/tiJhg0bVnhK+pKolfCPHz+eKVOmsH79emQyGffv3+fkyZO89957zJ49W+1gtK11m/L/iPW59V/XCZs+Jf9FW2ZlMhkOz7U3uIT/rS8CqmzRrYK6pOt6VJQ+1avCSuryo+pFgGV9X9JPX9JMQFXIvU5nTO/n18vK9uEvutqwpb9fpcoTKkcmk+FWv2O1T/gB2vnXZOfbnZnw/b9ciknh491XWLzvGv2aeTK8XW3a+jmVOzWtXC5x6X4yG0/cZce5GPL+G0/m4WBFpwAXOtZ1pqGnPf4utlibm3IxJpndF2LZcTaG+8mZTN16nvXH7vDRgMa09q1ZFW+7Qhbvi+BgxAOszE1YO6oNXetrcaXiR49g3ToYPVp75zBQ9evXr1SSX5haCf+MGTOQy+X07NmTjIwMunbtiqWlJe+99x5vv/22RgLTtJ69LBj7urXKryu4ANB14g/6lbDpU5Lm5ZqEJEnEbP1Hp3Go45t3bxK8ul6VrrSrTxeRRelr638BVbsAGeqdp/JIkkTKkaNkRd7B0t8Ph65dSk2MCu4UFD4W/RnSUe1IkkTC9RO6DkNveNew5pf/deTnsHtsOR3F1dgUfjsbw29nY6jjYsuQNj6083fC1c4KF3sLUjNzufUgjdsP0jlzJ5FjNx7yKD1bUV6PBq683bMeLX1qlPg30bxWDZrXqkHwc/VZdyySVYducTEmmVe/Pc0P49rTzl/3Sf+hiATWHcufIvLrV1tpN9kXSpSXl8eGDRsIDQ0lISEBuVz5jtHff/+tcplqrbRbIDs7m5s3b5KWlkbjxo2xs7NTtyitKbyi6dyP7Bn3hnr9z/Qh4S9K35I1XSZnhrzSbpvnajD1G81cwatD3+pRUfqW9JekrOTfEFfabTj5U0wt81faLamFP/nwEUW/fICaL7+k0hSeYqXdqlV4pd24a0e5G57/uzOkOlkVsUqSxLnoJH76J5pdF+6TUcHBvHaWZnSt78KErnUJ9Kmh0jkfpmUx7ZcL/H0tAUdrc359swMBbvZqRK8ZD1KzeOGrIzxMyyaogy/zXmqq/ZOGh0Pr1qQcPoxjt24GUS+1bdKkSWzYsIF+/frh6elZ7OKx6CJdFaFWC39ISAjDhw/H2tqaxo0bq1OEToSF5TDuDfVe28U6Su+S/s4ON/QqWdPloMyUyzFVfk5NuX0pQ6fn17d6VJQ+DvYtqrSxJcbAJkZWLOkvsV++mLPfIKQ+jCz/oGpKJpPRsrYTLWs7Mad/Y/ZciGXn+ftEJWaQkJpJZo4cUxMZPk7W1HG1o7GnA13qudDK10ntlXtd7Cz5ZkQrXvv2FOFRSQStP8P2tzri5mCl4XdXPknKX0n3YVo2Ddztmdm3UZXHIOT76aef2LZtG3379tVYmWp36ZkyZQpDhgxh3LhxdOzYUWMBaVOrVqq9XX1L8Euij8maLrr7ODTx5tERw5wNpU5TzcwMURn61F2sNIaY+EuSRMr+07oLSEss/HyV+uVb+BnwegPVQJq3iaKV397Fn8SoCzqOSP/ZWpoxtK0PQ9v6APl/y+nZeViYmmBhptkumFbmpnwX1JbBq04Q+TCd0ev/IWRsWzwdVe+GrK48ucTHu69w6L9BustfbYmVeRXNIOTuDjNmgJtb1ZzPAFhYWBAQoNkJPdSqtTExMWzcuJGHDx/SvXt3GjZsyGeffUZcnH6v3lfe4BvIT/ILNkOhb3OvF1Z4HnZtzsfuMbAlTu31d4ao0mh7lh5V6XNdKqBPc/qXpmCe/5R9J4yu/z6ADFmZjwX95Va/EzW8RMutqmQyGXaWZhpP9gvUtLVg49h2uNhZcC0ulReXH+PEzYdaOVdRaVm5/N+mf9lw4g4A8wc0oYFHFXYr8vaGhQvBS38bc6rau+++y1dffaXRdUvUqrlmZma8/PLL/P7770RHRzN+/Hg2b95M7dq1GTBgAL///nuxAQb6ICwsp8T9hpjkF2UIiVqBki4CKpvExW4P5/Hp2xqKsOrUb21foQvRqmQIdUnfFvMqTdaJf3UdgloSzx0v8x9N1p07ZT4ui+i/r1sJ14+TdP+qrsMQSlDb2Ybf3uxEY08HHqVnM3LdaVYduqXVxepik58wZPVJQq8lYGlmwsrXWjG8XRXnQqmpcOhQ/lcByJ/mfvPmzdStW5f+/fszaNAgpU0danXpKczd3Z3OnTtz/fp1rl+/zsWLFwkKCsLJyYmQkBC6d+9e2VNoTOvW5gad1JdHH7v3qKKkBK6i3TcS9hvmoltbl9zDwtKEPmM8dB2KEkOpS81r3tfrLj6GuqZswpE9mJiZ49y65H75FZlqUyT2+kn04ddvtZ1t+O2tjszacYlfwu7x2b5rxCRlMH9AU43P1R+T9IRX154iKjEDFzsLvh3dhpa1nTR6jgq5cQN69IDDh6v+3HqqRo0avPzyyxotU+2EPz4+nu+//56QkBBu377NwIED2b17N7169SI9PZ358+cTFBTE3btlLzwTExPD9OnT+eOPP8jIyCAgIICQkBDatGkD5Pebmzt3Lt9++y1JSUl06tSJVatWUa+easnIsPdqUe81T3XfrsEwlEStokprxS2a5BlyanH9bBp9xug6CkEb3J5rwp1bCboOQy0ZMXdKTfhLnGqzEJHsP1V4hpzypHlrf3pe0Ydf/1mZm/L5K80JrOXInJ2X+eFUFBlZeSx+pTlmag4OLqpwsl+7pg2b32iPT03djycT8oWEhGi8TLVqTv/+/fHx8WHDhg2MHz+emJgYfvzxR3r16gWAra0t7777LtHR0WWW8/jxYzp16oS5uTl//PEHV65c4YsvvsDJ6ekV5uLFi1m+fDmrV6/m9OnT2Nra0rt3bzIzM1WKuddr7nrXdUJQX9HuQIEDa+k6JLXVb6l/09kKmuE1qDW+4wxz9hobb7+n3xdJ4GUyGY7duuI2ZjSO3bqKz1YD4t6gMz6Bmpv5Q9AOmUzGqA5+LBvWAlMTGb+djWHilnCSMrLLf3E5ohMzFMm+r7MNP/3fMyLZ10O5ubkcOHCANWvWkPpfd6f79++TlpamVnlqtfC7ublx+PBhOnToUOoxrq6uREaWfevws88+w8fHR+lKxt/fX/G9JEksW7aMWbNm8dJLLwGwadMm3N3d2bFjB8OHD69wzAe2xNP//4rPZWpsjKl1XxXNX21IXpac0yvO6ToUlbTs4cjzo8XMBOrS5+48kP9P26N/C+6uO6LrUFTi1rUfNVt1Kf/AEojW/apTkVWQxe/DsL3UwhsbCzMmbg7nz8vxHIoIpX+gFyOf8SWwlmOFcxpJkgiPesyGE3f542IsuXIJX2cbfhz/DF41qm42IKFi7t69S58+fYiKiiIrK4vnnnsOe3t7PvvsM7Kysli9erXKZarVwr9u3boyk33I/0fn61v2VG07d+6kTZs2DBkyBDc3N1q2bMm3336reD4yMpK4uDjFnQMAR0dH2rdvz8mTJ0ssMysri5SUFKUN8vtK/7kxvqJv0aAcS6mn2KormUxG0yG6W7yqPKXVy7MHk/lrk2F2+dA1fU/2IX96zriHjroOo1Sl1cuaLToZfeNIdRUfcYzo83t1HYaggucau7NpXDsaezqQlSvnl7B7DFx5nGe/OMxn+65xPjqJzJzii4RJksT56CSW/BnB80uPMHjVSXadv0+uXKK9f039SfbNzfNn6jE313UkemPKlCm0adOGx48fY2399Hf08ssvExoaqlaZavfhT09P5/Dhw0RFRZGdrXyLafLkyRUq4/bt26xatYrg4GA++OADzpw5w+TJk7GwsCAoKEgxzae7u7vS69zd3UudAnThwoXMmzevxOeMra90dU7wDU11qpdVQd+TfUNZgKuseqkO0ZpcOXYxcq334xeDdg3TM3Wc2TO5M+FRSWw+dZfdF2OJfJjOqkO3WHXoFpC/4m9NWwskJNKz8kjLyiU79+kYEgszEwa28GJ0Bz+aeutRQ0SzZnDvHvzX4CDA0aNHOXHiBBYWFkr7/fz8iIlRb6FRtRL+s2fP0rdvXzIyMkhPT6dmzZo8fPgQGxsb3NzcKpzwy+Vy2rRpw6effgpAy5YtuXTpEqtXryYoKEid0Jg5cybBwcGKxykpKfj45C+coYm+0iLJFtSh7XqpSfpcx0Wir1ll1Uuh8lQZsFtVxKBdwyWTyWjt60RrXyfmD2zKwWsJ7LsUx8GIBDKy8xP8tKxcpdfYWpjSvYEbzzV2p0cDNxxtRCu6IZDL5eTlFb9rc+/ePezt1VsjQa2Ef+rUqfTv35/Vq1fj6OjIqVOnMDc3Z+TIkUyZMqXC5Xh6etK4cWOlfY0aNeLXX38FwMMjf6rC+Ph4PD2fzrATHx9PixYtSizT0tISS0vLYvvbT2qB7cuNOZYiWqCMkSRJXPr5uq7DKFVp9XLYe7XoHeRewiuqlj4n+aD/iT6UnOzr+0q7pdXLkojW++JK60Nf1T8rSZJIOXJUadak0rpkuTfojDwvR3TrMXB2lmb0D/Sif6BX/u8/M5dHaVkkpmcrFgmztTTF1d4SS7MqWjFXXRcvwgsvwM8/6zoSvfH888+zbNky1q5dC+Rf7KWlpTF37lz69lVv0L1aCf+5c+dYs2YNJiYmmJqakpWVRZ06dVi8eDFBQUEVXhSgU6dOREREKO27fv26ou+/v78/Hh4ehIaGKhL8lJQUTp8+zZtvvqlSzE2H1Bd9Uo3YhR+vGdyAXdD97FEi0a+8slr1jXWlXUG/pBw5SuL23wEU6yM4djPM2aEE1clkMhytzXG0NqeOq66jUUNODsTE5H8VAPjiiy/o3bs3jRs3JjMzk9dee40bN27g4uLCjz/+qFaZaiX85ubmmJjk9zN0c3MjKiqKRo0a4ejoWO5UnIVNnTqVjh078umnnzJ06FD++ecf1q5dq3RF88477/DJJ59Qr149/P39mT17Nl5eXgwcOFCd0AUDV9oqsHv2GObKkd+8e5Pg1fUUf0/apu8JfgFDSPSh/C48aUfOVk0gRkKSJBLDj5IRE4mNtz81W5XeUq0vSmpdr6qVQQruJDy6ckdpf+6Vu9gElByDGLQrCPqvVq1anD9/np9++okLFy6QlpbGuHHjGDFihNIgXlWolfC3bNmSM2fOUK9ePbp168acOXN4+PAh33//PU2bNq1wOW3btmX79u3MnDmT+fPn4+/vz7JlyxgxYoTimGnTppGens7//d//kZSUROfOndm3bx9WVlbqhC7oodKSeFU8iMnSQCRV7+zBZL6adJOp32h+hiFDSe4LGEqSX6Bi/fUNda1d3UgMP0rc3zsASInIb6kubfEvfVFS67ptQDe1y1Nn4K6Nt7/i55X/2K/UY8WgXUEwDGZmZowcOVJz5anzok8//VSxCMCCBQsYPXo0b775JvXq1WP9+vUqlfXiiy/y4osvlvq8TCZj/vz5zJ8/X51QBT2niWQfMOi86valDI2UY2gJfgFDS/RVYdulFdl3RWtqRWXERBZ5XPpqv/oiK/JO8ceVSPjVUbBmQkbMHWy8/cpcQ0EM2hUEwxAREcHXX3/N1av5PRgaNWrEpEmTaNiwoVrlqZXwt2nTRvG9m5sb+/btU+vkglCQpFY28W/U3o7wUMOc0su/Sdm35ww1kS/M2JL6is7G4/hCR8jJFf34K0iVlmp9Yenvp2jZL3hc1WQyGc6tu1bo4sitfieS466TdN8wu0EKRqhePTh4EOrW1XUkeuPXX39l+PDhtGnTRrHu1alTp2jWrBk//fQTgwcPVrlMtefhNzSXHntgll2x2ShK0rzmfQ1GIxRVNKlV9QKgcXtHg034LZvXNcik3tiSeFV4uSZVKOmXyWQ4PNdeJPwVpEpLtb7I77OPch9+Pf53kXD9uEj2Bf1ibw/du4t5+AuZNm2aort7YXPnzmXatGnaTfhbtmxZ4cFT4eHhKgei7yqb3IgLBtWoegFw42y6NsPRqviLD3UdAlC9E3hBP6jSUq0vZDJZ/ow4BjIrjujDL+idmBhYsQLUXH/JGMXGxjJ69Ohi+0eOHMnnn3+uVpkVTvgNfVacuF3n8B7WTmczPqibTIkLhXwltYAXvgio38qO038kVmVIGuPezEVjZYmkvepUtJXfECWeO45Lu2f1foYcXTPEtQlEH35B78THw6JF+XPxCwB0796do0ePEhAQoLT/2LFjdOmi3p3PCif8c+fOVbnwH3/8kQEDBmBra6vyazXt7rojyCxM8R7UpvyD9UhZCVx1vxgofBHQO0giO0vO1iX3dBiReu4/cUQmEnWD5OWaBJTep1/fF94qTcKRPZiYmZfa0m6I02cK+cTCW4Kg/wYMGMD06dMJCwvjmWeeAfL78P/888/MmzePnTt3Kh1bETJJkrQ2v4mDgwPnzp2jTp062jpFuVJSUnB0dATAtq4bLVZXj1tG1fFiIDsth3XdtpGcnIyDg4OuwylT4Xrp3K0BDWdV7A9WMAwFFwDJf5wg8fs9+d8bWL10aNACnwHFbykDPAo7opg+E8Dj2YEG1Q2nKtjFyNV+rarTclZE4XhyczIJ+3mWQdVJQ4hVUFN4OLRuTcrhwzh26yZ+11DhtXlkMhl5eXkVOlarg3a1eC2hlpxc0xJb4gpa6YyJuDNgOBwae+s6BEHDCj5TUu7e1G0glVDWDDmGOH2mrthHPqnwsan+6i2oIwiCcZHL1W8wKE21maUHwLZLyxL3V7QfrrFcGIiLAf3hNOx5pM5duf9AZjT1S3jKoYk3j45E6DoMlbl17VfmDDmGOH1mdZfmbVKpuw6CoDXOzjBuHNSsqetI9FJmZqZGFputNgm/07Dn8+fErgRVBugZavJW3qBPcUGgWQ7PtVf0fTa0AaCGWserkufLrZBn53J33RFdh6KSmi06ldkn3xCnzzQmqibuBV2ECr7mZWm+y5AgqM3XF777TkzLWUheXh6ffvopq1evJj4+nuvXr1OnTh1mz56Nn58f48aNU7nMapPwF06sqoKx3jXQ17sDkiRx6efrOjt/dWQIFyiG9vdlKAxx+kxjoU4rvV2MXJHsS5JE4rnjmg5LENT35Ancvg0umpuxztAtWLCAjRs3snjxYsaPH6/Y37RpU5YtW6ZWwi8u83Xs/oMaSpshu5Dopdiq2sUfIzi94lyVn7fSHlpBQpFN0Bhd/03Fbg83uNZ9wbglhh8l4cgeXYchCE9dvQpNm0KE4XV/1JZNmzaxdu1aRowYgampqWJ/YGAg165dU6tMrSb8vr6+mJuba/MURsdYLgAKJ/9VcQEQd/6B1s9RZYpeAFRkE0qly7+jlMsxOju3oHuqDNitKkUHXAuCoVu5ciV+fn5YWVnRvn17/vnnn1KPvXz5MoMHD8bPzw+ZTMayZcuKHVPwXNFt4sSJimPi4uIYNWoUHh4e2Nra0qpVK3799VfF83fu3GHcuHH4+/tjbW1N3bp1mTt3LtnZ2RV6TzExMcXm4If8wbw5OTkVKqMorSb8ly5dwsfHR5unqLCUHWFI8Za6DkNlRS8ADPUiQNvJv0egq1bK1baUY8c1M5uVuEgok67+bhyaiBmYhKckSSIq9iQXrv9EVOxJncxkZ+PtX+XnFARt2bp1K8HBwcydO5fw8HACAwPp3bs3CQkJJR6fkZFBnTp1WLRoER4eHiUec+bMGWJjYxXb/v37ARgyZIjimNGjRxMREcHOnTu5ePEigwYNYujQoZw9exaAa9euIZfLWbNmDZcvX2bp0qWsXr2aDz74oELvq3Hjxhw9erTY/l9++YWWLUuegKY8aiX8JiYmmJqalrqpa9GiRchkMt555x3FvszMTCZOnIizszN2dnYMHjyY+Ph4lct+vHsvKUeOGkXCY0zJv6YuApq92oD2k1pUPrgqpqiXuiAuBLTO8+VW+I4T/dyFfNFxp7h+Zy8Jjy5z/c5eouNOVXkMNVt1wa1rvyo/ryBow5dffsn48eMZO3YsjRs3ZvXq1djY2LB+/foSj2/bti2ff/45w4cPx9Ky5EZgV1dXPDw8FNvu3bupW7cu3bp1Uxxz4sQJ3n77bdq1a0edOnWYNWsWNWrUICwsDIA+ffoQEhLC888/T506dRgwYADvvfcev/32W4Xe15w5c5g0aRKfffYZcrmc3377jfHjx7NgwQLmzJmj4k8pn1qDdrdv3670OCcnh7Nnz7Jx40bmzZunViBnzpxhzZo1NG/eXGn/1KlT2bNnDz///DOOjo5MmjSJQYMGcfy46oOOsiLvQLdC/3wLJzdumWrFrWtFk35DHaRYNOmvTrMBFauXulY06TfQvw1B0DdJqXeVHienRoFnBx1FIwh6QiYDC4v8ryrIzs4mLCyMmTNnKvaZmJjQq1cvTp48qZHQsrOz+eGHHwgODlaa+KVjx45s3bqVfv36UaNGDbZt20ZmZibdu3cvtazk5GRqVnDq0Zdeeoldu3Yxf/58bG1tmTNnDq1atWLXrl0899xzar0XtRL+l156qdi+V155hSZNmrB161aVRw+npaUxYsQIvv32Wz755BPF/uTkZNatW8eWLVt49tlnAQgJCaFRo0acOnVKsdxwRVn6+5X+pBEk/6B8AWCoyT+ofgFgsIN2ATsXf6wSZGS66ddCdQoltfob2N+Irv4WxKBdobAa9r4kPLqseOxoX1vtstRd0EvfB+1mZWWRlZWleJwipmo0fi1bQlaWYlrOor9zS0vLElvjHz58SF5eHu7u7kr73d3d1R7YWtSOHTtISkpizJgxSvu3bdvGsGHDcHZ2xszMDBsbG7Zv315iv3uAmzdv8vXXX7NkyZIKn7tLly6K7kSaoNE+/M888wyhoaEqv27ixIn069ePXr16Ke0PCwsjJydHaX/Dhg2pXbt2qVdvWVlZpKSkKG0ATi/2xaFr9Zor2tC7/hRWXjcgfR+0W1q9dO3eD6e2+fXSKqHqpo2tNNH1p0L0fdBuafVS0A4fj2eo79cXd+em1Pfri4+Hao1Wmlg4S98H7S5cuBBHR0fFpi/jAIWq4+Pjo1QHFi5cqLNY1q1bxwsvvICXl3LOMXv2bJKSkjhw4AD//vsvwcHBDB06lIsXLxYrIyYmhj59+jBkyBClKTarmsbm4X/y5AnLly/H21u1QWo//fQT4eHhnDlzpthzcXFxWFhYUKNGDaX97u7uxMXFlVjewoULS+xW5NC57IVkAINrtVTF/Qc1DLrFvySFk/68enXhQJQOoylbafXSqfXTeqm3LfwlMaC/FV3We/smXnq90m5p9VLQjFR/a6WWeJlMRm3PDpXuxlOZ2X+svfyUVknWNzNnziQ4OFjxOCUlRST9xu7qVRgxAtasASA6OhoHBwfF06X1tXdxccHU1LTYuM74+PhSB+Sq4u7duxw4cKBYv/tbt26xYsUKLl26RJMmTYD86TKPHj3KypUrWb16teLY+/fv06NHDzp27MjatWvLPJ+Tk1OF14tKTExU8d2omfAXDUqSJFJTU7GxseGHH36ocDnR0dFMmTKF/fv3a2TZYFDzw8KAkpfKKGjpN7bEHwA9z5XLq5cGlewLFafnv1aRXFUfhRff0meldd8QjNiTJ3D2bP5XwMHBQSnhL42FhQWtW7cmNDSUgQMHAvnTVoaGhjJp0qRKhxUSEoKbmxv9+ikPcs/IyADyxwsUZmpqilz+9C5cTEwMPXr0oHXr1oSEhBQ7vqjCU4Q+evSITz75hN69e9OhQ34DwcmTJ/nzzz+ZPXu2Wu9HrYS/6LylJiYmuLq60r59e5ycnCpcTlhYGAkJCbRq1UqxLy8vjyNHjrBixQr+/PNPsrOzSUpKUmrlL+vqrbQPi5Rjx3Hs+azy1VM1SfSLMsbW/tQr+j3It7R6+TjsOLYv9UCGAXXnMaC/G13Xc0Otl4JhkSSJ6LhTJKXepYa9Lz4ez5TaUvjk/p2qDU4QtCg4OJigoCDatGlDu3btWLZsGenp6YwdOxbInz7T29tb0S0oOzubK1euKL6PiYnh3Llz2NnZKfW/l8vlhISEEBQUhJmZcqrcsGFDAgICmDBhAkuWLMHZ2ZkdO3awf/9+du/eDeQn+927d8fX15clS5bw4MHTbsel5a9BQUGK7wcPHsz8+fOVLlwmT57MihUrOHDgAFOnTlX5Z6VWwl84qMro2bNnsf5OY8eOpWHDhkyfPh0fHx/Mzc0JDQ1l8ODBAERERBAVFaW44qmox7v3IjM3x3FIO43EbuiMLem3b6zfXSdK8+DQHnJrmFKje7fyD9Y1A0r09YVDE2+DrJeCYYmKPcmNu38AkPDoMpIk4evVscRjbbz99bpLjyCoYtiwYTx48IA5c+YQFxdHixYt2Ldvn2Igb1RUlFLL+v3795XmsV+yZAlLliyhW7duHDp0SLH/wIEDREVF8frrrxc7p7m5OXv37mXGjBn079+ftLQ0AgIC2LhxI3379gVg//793Lx5k5s3b1KrVi2l11dk/Y0///yTzz77rNj+Pn36MGPGjHJfXxK1+/A/fvyYdevWcfXqVSB/kYCxY8dWeMohAHt7e5o2baq0z9bWFmdnZ8X+cePGERwcTM2aNXFwcODtt9+mQ4cOKs/QA5AVexsQCX8Bo0r6DaiBvKj0M//qb8IvkvxK8Xy5FfLsXDFTj1CuwjPpqCr2wVmlx3EPzpaa8Nds1QV5bo5ez9QjCKqYNGlSqV14CifxkL+KbkUS7ueff77M4+rVq6e0sm5RY8aMKTazjyqcnZ35/fffeffdd5X2//777zg7O6tVploJ/5EjR+jfvz+Ojo60adMGgOXLlzN//nx27dpF166am1N86dKlmJiYMHjwYLKysujduzfffPONWmVZBIi+qcYq9bJ+d50wOCLRF8ogSRKJ4UfJiInExtufmq26VHiwWXVTmUReXVKRFhD7yCeKOGQyGTVbdBIJv6A//P1h2zbw9dV1JHpj3rx5vPHGGxw6dIj27dsDcPr0afbt28e3336rVplqJfwTJ05k2LBhrFq1SrGybl5eHm+99RYTJ04scVqiiip6NWZlZcXKlStZuXKl2mUWEP+PijOWVn5D7jph17Zt1Z9UJPRVwljn4U8MP0rc3zsAFN1DnFvr0eJx1Yyna0tFl578xy10F4wgqMrJCYYMUczDL+TfIWjUqBHLly9XzBLUqFEjjh07prgAUJVaCf/Nmzf55ZdfFMk+5I9ODg4OZtOmTWoFUhWybkTDC7qOQtAGQ+064fRiXxy6aWh9CJHEF6PrC1p9n4dfXUXncs+IuSMSfh2q7dkBmUxGcmoUjva1VZ7fXxB0Kj4eNm+GAQN0HYlead++PZs3b9ZYeWrN0dWqVStF3/3Crl69SmBgYKWD0hbLeuqvamjMjGFhLplMhkf/FroOQ2Xlrg/hllnxTSiRLuu3QxPV1iUxFDbe/kUe++kmEAF4Or9/s/rDFMm/IBiMmBh49124L7rmapNaLfyTJ09mypQp3Lx5UzF49tSpU6xcuZJFixZx4cIFxbHNmzfXTKSVZN2yAfa9RauHoF9Szh7GcUBX8Q9aywon/VXZ4m+od54Szx3HpZ3yNMYZ3k8HsFl5daamo0RW5B0s/f2w6tqZDJmETYyox4IgCPpIrYT/1VdfBWDatGklPieTyZAkCZlMRl5eXuUi1JAnZyNI/fMUji+UPHNBdafrrg/V1eOtfyGzMBf1sgpVZfJfcOfJ0BL+hCN7MDEzL7Wbjkwmw7FbV+gmuvEIgiAYArUS/sjIyPIP0kNZN6JAJFalqkjXB329KJAkibhd53Qdhloyr98VCb+OlFfn9bW+V4Wy+uVLkkTKkaOKFn6HrmKWHk0xhNVwBUEwPGol/L4GOnWS6MNfebrsD11W8mXQs6Hkycs/RtAJTdR3eYZhjq8oq19+ypGjJG7/HYD0c/mz9DiK1n5BENTh6Aj9+4ODg64jMWpqJfw7d+4scb9MJsPKyoqAgAD8/f1LPEZXRB9+w1dW8pUQnlB1gWiYZCJaRgX94ta1HzVblT57VFbkneKPRcJvECRJIvHccV2HIQhP1a0LO3dW+2k5Bw0aVOFjC6bqVIVaCf/AgQMV/fQLK9x3v3PnzuzYsQMnJyd1TqFxT85GIDv+N16D2hR7zhhmqanuLOv7kn76kq7DUItMLlr4Bf1Ss0XZs0dZ+vspWvYLHgvK0rxNsIvRv7/tR2FHxKJbgn7JyYGkJDCp3t3ZHB0dtVq+Wgn//v37+fDDD1mwYAHt2rUD4J9//mH27NnMmjULR0dHJkyYwHvvvce6des0GnBlPPjrMt4lJPyG1E9XXJyUzKFPB6TsHB5v/UvXoaju8aMqrYOiDgmV5dA1v/W/cB9+wTAkXz6j6xAEQdnFi9C6NRw+rOtIdCokJESr5auV8E+ZMoW1a9fSsePTgYY9e/bEysqK//u//+Py5cssW7aM119/XWOBaoJU/iF6ryAxFEmbMkmSyLx+V9dhqKlqa2ZFLi5E/dIMSZJI2X9a12GorKRpOZWn3JRhG9ANArrlPxTTZ2uEugN2JUkiOu4USal3qWHvi4/HM6XeoTGG/4OCIKhOrYT/1q1bOJQwuMLBwYHbt28DUK9ePR4+fFi56DTMtVdjXYegMSLxV/Zg3YI3mgAAKsBJREFU2Y88ORuh6zDUYuGmfwOVxEWBZqTsO2mQd53Km5ZT0C/Rcae4fmcvAAmPLgP5q++WxLFxGxISSh6HJwiC/vjll1/Ytm0bUVFRZGdnKz0XHh6ucnlqNSe0bt2a999/nwcPHij2PXjwgGnTptG2bVsAbty4gY+PjzrFa43MCAdHGlJ3JG3Kun1P1yGoLTshVdchqMXLNanMTYAsg73rlD8tp1A5VTXFZlKqcj1LTo1Sepzqb634XkyfKgj6b/ny5YwdOxZ3d3fOnj1Lu3btcHZ25vbt27zwwgtqlanWp9G6deuIjIykVq1aBAQEEBAQQK1atbhz5w7fffcdAGlpacyaNUutoLQl9Yq472ysLOvU0nUIahP/fo2XZX3DnMIYyp6WU6i4NG+TCm/qqmGvXM8c7UufgvrJ/Ttqn0cQhKrxzTffsHbtWr7++mssLCyYNm0a+/fvZ/LkySQnJ6tVplqfMA0aNODKlSv8/vvvTJ48mcmTJ7Nz504uX75M/fr1gfyZfEaNGlVmOQsXLqRt27bY29vj5ubGwIEDiYhQ7paRmZnJxIkTcXZ2xs7OjsGDBxMfH69O2Ng39lLrdYL+c33nVaxbNtB1GGpxfa6JrkMQtMShTwechj2v6zBUVt60nIJulHZR4OPxDPX9+uLu3JT6fn3x8Sh9CmprLz8tRScIagoMhORkaNZM15HojaioKMU4WWtra1JT83sCjBo1ih9//FGtMtVuUjAxMaFPnz6KhL93796YFJlSqVmzZkRHR5daxuHDh5k4cSKnTp1i//795OTk8Pzzz5Oenq44ZurUqezatYuff/6Zw4cPc//+fZXmKlVihKOVRD/qfCYmJrhNHKrrMIT/iHqZTyaT4fBce12HobLypuUU9ItMJqO2Zwea1R9Gbc8O4ncnGBZT0/xFt0xNdR2J3vDw8CAxMRGA2rVrc+rUKQAiIyOLTYlfUWoN2q2oO3fukJOTU+rz+/btU3q8YcMG3NzcCAsLo2vXriQnJ7Nu3Tq2bNnCs88+C+RPW9SoUSNOnTrFM8+otpBW6lXRpUfQP8ZSL0WSLwhVI9XfGvvIJ2q9VnTpEfTOjRswaRIsWqTrSPTGs88+y86dO2nZsiVjx45l6tSp/PLLL/z7779qN3prNeFXVUG/pJo1awIQFhZGTk4OvXr1UhzTsGFDateuzcmTJ1VO+B0ae2suWD0gEizjYOj1UtRDQTAcNt7+pEScL/9AQagqqanw11/w4Ye6jkRvrF27Fvl/i3IWdGs/ceIEAwYMYMKECWqVqTcJv1wu55133qFTp040bdoUgLi4OCwsLKhRo4bSse7u7sTFxZVYTlZWFllZWYrHKf8t1ew7riueL7fSTvBVrNonWAlWJe/PrNowVGFI9bLa169qpLR6WR1VdFXcqpp5R1tqtuqCPDdHrLYrCHrMxMREqZv88OHDGT58eKXK1JuEf+LEiVy6dIljx45VqpyFCxcyb968Yvs9+rcwin6N1SYZKy2pL4UkSaQcO66lYCqvqutltaknek7fF94qrV7qiiRJJIYfJSMmEhtvf2q26lIln9sVTfaLHqvL5L/wVJuqkMlk1GzRSST8gqBnLly4QNOmTTExMeHChQtlHtu8eXOVy9eLhH/SpEns3r2bI0eOUKvW0+kVPTw8yM7OJikpSamVPz4+Hg8PjxLLmjlzJsHBwYrHKSkpercegDqMLoFTMaEvT8qRozzevVejZWqSuvXS6H7v1Yy+L7ylb5+XieFHift7R34s/3U70efFv8q6UDD0OwGCIFStFi1aEBcXh5ubGy1a5DcGljRAVyaTkZeXp3L5Ok34JUni7bffZvv27Rw6dAh/f3+l51u3bo25uTmhoaEMHjwYgIiICKKioujQoeRVBC0tLbG0tNR67FXFoBM+DSf1ZcmKvFNl51JHafUy7qEjJhlV93MSqpa+L7ylb5+XGTGRRR7f0XrCr0rrviAIWuDjAytWQC3DXU9HEyIjI3F1dVV8r2lqJ/yhoaGEhoaSkJCgGFhQYP369QCsWbMGd3f3UsuYOHEiW7Zs4ffff8fe3l7RL9/R0RFra2scHR0ZN24cwcHB1KxZEwcHB95++206dOig8oBdQ2NQiX4VJvalsfT3I/2cGIgm6BfL+r6kn76k6zAMRtEBpWLxL0GoBlxdYeJEqMZjiAB8fZ8uoHf37l06duyImZlymp6bm8uJEyeUjq0otRL+efPmMX/+fNq0aYOnp2epfSxfe+21MstZtWoVAN27d1faHxISwpgxYwBYunQpJiYmDB48mKysLHr37s0333yjcsxNneKwsDPnQqJ+L76l14m+HiT2JbFKkGHRoAvZdW+QduuKrsNRScr+0zgO6GoU40uE4hz6dEDKztHrbj0lSTx3HJd2z1Z5vSxY7Csj5g423n4GvfiXXYxcK9160rxNxF0JwbgkJsLevdC5s64j0Rs9evQgNjYWNzc3pf3Jycn06NGj6rr0rF69mg0bNpS7km55KrJ4gJWVFStXrmTlypWVOleB5jXz5zzXx8Rfr5J9PUvurRLKTjyS/j1mcMk+wOOtfyGzMMfxhY66DkXQgoKFtwwt4U84sgcTM/Mq7z8vk8lwbt21ys4rEmdB0AN37sCoUXD4sK4j0RuSJJXY4PLo0SNsbW3VKlOthD87O1ux5K+hal7zvl4l/XqR7OtJkl9ecl+SjHua7+9WVbJuRIFI+AU9UxX954Wqpa27DoIgaEbBoloymYwxY8YojbHKy8vjwoULauffaiX8b7zxBlu2bGH27NlqnVRf6EPSr/NEX4dJvjqJfWlsavmTes0w+/Bb1qut6xAEoRjRf9442cXIyc0RdzYEQR85OjoC+S389vb2WFs/nX7XwsKCZ555hvHjx6tVtloJf2ZmJmvXruXAgQM0b94cc3Nzpee//PJLtYLRBV0m/TpL9nWQ5GsyuS+JU9v8xWQeHDKsuaWdXuyLQ6seUN7Px02PVxUTjI5PYF883TshM9AWYWPvqiP68QuCcQoJCVF0d//666+xs7PTWNlqJfwXLlygRYsWAFy6pDwDhSEOPtRF0l+lyX4VJvjaTuwLWCcU3SPDsnYnHmBYCb9D504V+5vR5O9QXDwI5XCr39EgP8sFQTBAtrbwzDP5XwUkSWLz5s188MEH1KtXT2PlqpXwHzx4UGMB6IuqTPqrJNnXcpKv7cS+eEJfNkmSeHBFf1fa1SuaqhviwqFc+r7SbkWIft/6qaK/k6J3AiRJIuH6CW2EJAjqadAATp6s9tNyFjAxMaFevXo8evRI9wm/saqKpF+ryb6WknxtJveqJvaleXD5KPf/NazWfYOnbn2rRhcK+r7SbkWJpN9wFe3+Ex9xjOjz+rsquSAIsGjRIt5//31WrVpF06ZNNVKmSPiL0GbSr7Vk34ASfU0l+IXZxeUSdfe25guuApYPZJha5v+cM93Kn6bWKBStr0Z8AaDvK+0aI9G3vbjCSX/qQ8Od0UwwUuHh0Lq1mJazkNGjR5ORkUFgYCAWFhZKg3cBEhMTVS5TJPwl0IfZeypEC4m+ppN8bSX4RTk6+fEw9oLmT1aFKvKzN8qLgsL12MiSf2NaabeiibS4E1Bchnf5f7c2MVUzZsLexZ/EKMP+rBQEY7ds2TKNlykSfkOlwWRfn5P8kpL7knj5d0Kel0NkxB+aO7keMvqLAiNL/g11pV3BeLk36Iw8L0d06xEEPRYUFKTxMkVTTCkKVuTVFI1259FQsm+VINNIsm+doLypyy4ut9hWUTKZDE+/Duqf3IgU/F6LbgYnwUpvFoNTV8FKu9WJ6FKjnorcBdAEmUyGW32x0J8gGIrMzExSUlKUNnWIFv4y6F3XHg0kP5pK/DTRiq9KQl9iDNGpSo9z87IqVZ6xK+t3r9d3BQrqvYG1+Hu5JgGQm56F6Mmv/+wjn1T42FR/6/IPEgRBUFN6ejrTp09n27ZtPHr0qNjzeXl5KpdZbVr4D8w6hlyuesuTplv61VbJZL+yrbyVbcWvTOs95Cf3RTdjkHb4OFbxUqXvjlSWQdwVMIAWfy/XJMUG+VMgxu06p9OYjJVcLidqRwgRq+cRtSNErc93ddlHPil3U4c2W/nF2ApBbzVuDDduQMOGar185cqV+Pn5YWVlRfv27fnnn39KPfby5csMHjwYPz8/ZDJZiX3lC54ruk2cOFFxTFxcHKNGjcLDwwNbW1tatWrFr7/+qlTOggUL6NixIzY2NtSoUUOl9zRt2jT+/vtvVq1ahaWlJd999x3z5s3Dy8uLTZs2qVRWgWrzCXD36H3+mnZUrdfqTdKvpsombpropqP2uY0ouS/J/X/38ODy03qpia5RmqSXib+eKZrkFxa7PZy7645UfVDVwL2dG0m9cZHc1GRSb1zk3s6Nug6pRFXVVaci0rxNSPM2Id2z2vzrFwyBlRUEBOR/VdHWrVsJDg5m7ty5hIeHExgYSO/evUlIKPmfaEZGBnXq1GHRokV4eHiUeMyZM2eIjY1VbPv37wdgyJAhimNGjx5NREQEO3fu5OLFiwwaNIihQ4dy9uxZxTHZ2dkMGTKEN998U+X3tWvXLr755hsGDx6MmZkZXbp0YdasWXz66ads3rxZ5fKgGiX8AAlXVJ/GqIChJ/3qqmyyr/Z5jTzRLywt/k6J+/Up+RdJf8lKSvILS7kcUzWBGCi7GHmFt6KexEUVeRytdhzqtshrgz5dIAhClYiMhJEj4c4dlV/65ZdfMn78eMaOHUvjxo1ZvXo1NjY2rF+/vsTj27Zty+eff87w4cOxtLQs8RhXV1c8PDwU2+7du6lbty7dunVTHHPixAnefvtt2rVrR506dZg1axY1atQgLCxMccy8efOYOnUqzZo1U/l9JSYmUqdOHQAcHBwU03B27tyZI0fUa0SqVgl/o+YWlXp9ZZL+8hKDMlWiG0Nlu/GoozKt+uom+nK5nPPXflTrnLpm6+Zb7jH6kPiL1v6nSmvRL8q+sR6NAVLBreObq7SLjDqsPWoXeeyjo0g0L8NbKnGrDrKysjQyQFEwII8fw+bNkJQEUOz3n5VV8vi87OxswsLC6NWrl2KfiYkJvXr14uTJkxoJLTs7mx9++IHXX38dmezp/7+OHTuydetWEhMTkcvl/PTTT2RmZtK9e3eNnLdOnTpERuavmdGwYUO2bdsG5Lf8q9o9qEC1Sfhb9nBkyooAOjvcoLPDDV2HI2jY2SubeJirfgufodB10i+odvEul/Q7aS5N0v2r3Diqn11kCtQaEIR9vWaY2dfAvl4zag3QzDR2kiQRFXuSC9d/Iir2JJJUPRJtfbFw4UIcHR0Vm4+P8VzICRXj4+OjVAcWLlxY4nEPHz4kLy8Pd3d3pf3u7u7ExcVpJJYdO3aQlJTEmDFjlPZv27aNnJwcnJ2dsbS0ZMKECWzfvp2AgACNnHfs2LGcP38egBkzZrBy5UqsrKyYOnUq77//vlplVptZem5fTEcul2Nikn+N09nhBsdS6qlcjk5m7nHL1PvBipqgdheeqFge5hlusp8UcxP3Zt3KP1AP6Hw2Hx3O1KPOXbqoH45pPpAqkhJ/s9Tn5HI5N49tIj0xGtuaPgR0Hq34bIX8rjrqDBItr9zCZDIZtj51kZmYYOPtr9T6VpQkScRHHCP1YST2Lv64N+hc6vHRcae4fid/jvqER5cBqO1Z+pS/crmcizd+IiUtBgc7b5rVG15qzHK5nAchG8mKjsbSxwfXsUGlHitJEilHjpIVeQdLfz8cunYBSo5ZkiQSw4+SEROJjbc/NVt1KfX9SZJE4rnjpb4fXZs5cybBwcGKxykpKSLpr2aio6NxcHBQPC6t601VWLduHS+88AJeXsp53+zZs0lKSuLAgQO4uLiwY8cOhg4dytGjR9XqwlPU1KlTFd/36tWLa9euERYWRkBAAM2bN1erTKNP+AtaZ5If5jK5ywWW/BWoeK6V7BqnUuuqXGZDi7tcelzyYI+yuNnEE/fQUeXXAaBmnpPhAJYP1OuGkZet3jlzc9TrzqP2tJqScqCG0CJXOMb0mKvkZVf8F6zL2UflmTr82bpkQoZuTu3hkkxuuhovTH86dZqh1UsJidyckuvlreObSbp/FYDsjGRuHAmhbqcRSsfkZZWf8OfmKN8BKavcouUlnjtOwpE9AKREnEeem0PNFp2KlQmQcP2EYqGpxKgLyPNylOaiz819+j4Tk28rvfZxciReri1LfQ8Xb/7Mo8fXAXiQmML565tpFjCE3BwZeVkmSn8zCZt+4MmV/PeXkZRMwnfrcRs9ssRyU44d5/Hu/JjTz51HyslB6twJ6/vFP89L+1mUpPCx+lgnLS0tlRK8ghhF1x4jlpYGQEp6/oesvb29UsJfGhcXF0xNTYmPj1faHx8fX+qAXFXcvXuXAwcO8Ntvvyntv3XrFitWrODSpUs0adIEgMDAQI4ePcrKlStZvXq12ueUy+V8/vnn7Ny5k+zsbHr27MncuXPx9fXF17f87r9lkoxcdHS0BIitGm3R0dG6rnblEvWy+m2iXopN3zZRJ8Wmj5sq9bJdu3bSpEmTFI/z8vIkb29vaeHCheW+1tfXV1q6dGmpz8+dO1fy8PCQcnJylPZfuHBBAqQrV64o7X/++eel8ePHFysnJCREcnR0LDceSZKk+fPnSyYmJtLzzz8vvfTSS5KVlZU0duzYCr22PEbfwu/l5UV0dDT29vZl3vIVDJ8kSaSmpha79aaPRL2sPoy1XhZ0tSh6+72yDK1cbZatrXKNtU4Khk2dehkcHExQUBBt2rShXbt2LFu2jPT0dMaOHQvkT5/p7e2tGAeQnZ3NlStXFN/HxMRw7tw57OzslPrfy+VyQkJCCAoKwsxMOVVu2LAhAQEBTJgwgSVLluDs7MyOHTvYv38/u3fvVhwXFRVFYmIiUVFR5OXlce7cOQACAgKws7Mr8f1s2rSJb775hgkTJgBw4MAB+vXrx3fffVdq978K08hlgyAIglCtJCcnS4CUnJxcrcvVZtnajFkQjMXXX38t1a5dW7KwsJDatWsnnTp1SvFct27dpKCgIMXjyMjIEu8qdOvWTanMP//8UwKkiIiIEs95/fp1adCgQZKbm5tkY2MjNW/eXNq0aZPSMUFBQSWe6+DBg6W+FwsLCykqKkppn6WlpUbuxskkSQ878QmCIAh6LSUlBUdHR5KTkzXeqm1I5WqzbG3GLAiC/jE1NSUuLg5XV1fFPnt7ey5cuIC/v3+lyjb6Lj2CIAiCIAiCoO8kSWLMmDFKA9czMzP53//+h62trWJf0YHEFSESfkEQBEFllpaWzJ07V+NT5hlaudosW5sxC4Kgf4KCgortGzmy5Jm8VCW69OgBSZKYMGECv/zyC48fP+bs2bO0aNFCq+eUyWRs376dgQMHavU8gqAqPz8/3nnnHd555x29LE8QBEEQDE21WWlXn+3bt48NGzawe/duYmNjSUlJoX///nh5eSGTydixY0eJr7t69SoDBgzA0dERW1tb2rZtS1RUVNUGLwiCIAiCIOg1kfDrgVu3buHp6UnHjh3x8PAgPT2dwMBAVq5cWeZrOnfuTMOGDTl06BAXLlxg9uzZWFkZ/4q8giAIgiAIQsWJhF/HxowZw9tvv01UVBQymQw/Pz9eeOEFPvnkE15++eVSX/fhhx/St29fFi9eTMuWLalbty4DBgzAzc1NrTjmzp2Lp6cnFy5c4IMPPqB9+/bFjgkMDGT+/PlqlS/ojp+fH8uWLVPa16JFCz766CMkSeKjjz6idu3aWFpa4uXlxeTJkxXHZWVl8d577+Ht7Y2trS3t27fn0KFDFTrvhg0bqFGjBrt376ZBgwbY2NjwyiuvkJGRwcaNG/Hz88PJyYnJkyeTl5dXajlRUVG89NJL2NnZ4eDgwNChQ4utrLhr1y7atm2LlZUVLi4uZf7tfPfdd9SoUYPQ0NAKvQ9BEARBMHQi4dexr776ivnz51OrVi1iY2M5c+ZMua+Ry+Xs2bOH+vXr07t3b9zc3Gjfvn2pXX/KIkkSb7/9Nps2beLo0aM0b96cESNG8M8//3Dr1i3FcZcvX+bChQu89tprKp9D0F+//vorS5cuZc2aNdy4cYMdO3bQrFkzxfOTJk3i5MmT/PTTT1y4cIEhQ4bQp08fbty4UaHyMzIyWL58OT/99BP79u3j0KFDvPzyy+zdu5e9e/fy/fffs2bNGn755ZcSXy+Xy3nppZdITEzk8OHD7N+/n9u3bzNs2DDFMXv27OHll1+mb9++nD17ltDQUNq1a1dieYsXL2bGjBn89ddf9OzZU4WflCAIgiAYLjFLj445Ojpib2+PqakpHh4eFXpNQkICaWlpLFq0iE8++YTPPvuMffv2MWjQIA4ePEi3bt0qVE5ubi4jR47k7NmzHDt2DG9vbwCaNGlCYGAgW7ZsYfbs2QBs3ryZ9u3bK61EJxi+qKgoPDw86NWrF+bm5tSuXVuRLEdFRRESEkJUVJRi5cP33nuPffv2ERISwqefflpu+Tk5OaxatYq6desC8Morr/D9998THx+PnZ0djRs3pkePHhw8eFApiS8QGhrKxYsXiYyMxMfHB8hfibBJkyacOXOGtm3bsmDBAoYPH868efMUrwsMDCxW1vTp0/n+++85fPgwTZo0Uf2HJQiCIAgGSrTwGyC5XA7ASy+9xNSpU2nRogUzZszgxRdfZPXq1RUuZ+rUqZw+fZojR44okv0CI0aMYMuWLUD+XYAff/yRESNGaO5NCHphyJAhPHnyhDp16jB+/Hi2b99Obm4uABcvXiQvL4/69etjZ2en2A4fPqx096csNjY2imQfwN3dHT8/P6Vlxd3d3UlISCjx9VevXsXHx0eR7AM0btyYGjVqcPXqVQDOnTtXbmv9F198wbfffsuxY8dEsq8Bmp7cLTY2VrHcvaZlZGSQnZ2tlbIFQRAMhUj4DZCLiwtmZmY0btxYaX+jRo1UmqXnueeeIyYmhj///LPYc6+++ioRERGEh4dz4sQJoqOjS2yBFfSfiYlJsQQtJycHAB8fHyIiIvjmm2+wtrbmrbfeomvXruTk5JCWloapqSlhYWGcO3dOsV29epWvvvqqQuc2NzdXeiyTyUrcV3ARqw5ra+tyj+nSpQt5eXls27ZN7fNUd+np6aSmppKSkoJMJtNYuTExMTRr1oxZs2bx77//aqxcgEuXLjF06FBOnTpFVlaWxsq9d+8e27Zt47fffuPixYsaK7cixEzagiCoQ3TpMUAWFha0bduWiIgIpf3Xr1/H19e3wuUMGDCA/v3789prr2Fqasrw4cMVz9WqVYtu3bqxefNmnjx5wnPPPaf2gGBBt1xdXYmNjVU8TklJITIyUvHY2tqa/v37079/fyZOnEjDhg25ePEiLVu2JC8vj4SEBLp06aKL0GnUqBHR0dFER0crWvmvXLlCUlKS4oK3efPmhIaGMnbs2FLLadeuHZMmTaJPnz6YmZnx3nvvVUn8xuLKlStMnTqVBw8eEB8fz+LFixkxYgSSJFU6+b9x4wbJyckkJyfz9ddfM2XKFFq1agVQqfIvX75Mly5dGDZsGP7+/hpbvOrixYv0798fV1dXoqOjadeuHUuXLlW6k6UJ169fZ926dSQkJNCiRQv69u1LvXr1kMlkGvm5C4JQvYiEXw+lpaVx8+ZNxePIyEjOnTtHzZo1qV27NgDvv/8+w4YNo2vXrvTo0YN9+/axa9euCs+gUuDll1/m+++/Z9SoUZiZmfHKK68onhsxYgRz584lOzubpUuXauS9CVXv2WefZcOGDfTv358aNWowZ84cTE1NgfyZdPLy8mjfvj02Njb88MMPWFtb4+vri7OzMyNGjGD06NF88cUXtGzZkgcPHhAaGkrz5s3p16+f1mPv1asXzZo1Y8SIESxbtozc3FzeeustunXrRps2bYD8GaZ69uxJ3bp1GT58OLm5uezdu5fp06crldWxY0f27t3LCy+8gJmZmViIq4KuXLlC165dGT16NG3atCEsLIyxY8fSpEkTjSwQ2Lx5c/r27Uu/fv1Ys2YNX375JTNnzqRJkyZqJ7bp6ekEBwfz6quv8s033wBw7do1MjMzlT5HVXX37l1eeOEFRo0axaxZszhy5Aivv/46jx490mjCf+XKFTp27EiHDh2wtbVl7ty57N69m2HDhvHGG2+IpF8QBNVJgs4tXbpU8vX1VTw+ePCgBBTbgoKClF63bt06KSAgQLKyspICAwOlHTt2VPicgLR9+3bF461bt0pWVlbSr7/+qtj3+PFjydLSUrKxsZFSU1PVfXuCjiUnJ0vDhg2THBwcJB8fH2nDhg1SYGCgNHfuXGn79u1S+/btJQcHB8nW1lZ65plnpAMHDihem52dLc2ZM0fy8/OTzM3NJU9PT+nll1+WLly4UO55Q0JCJEdHR6V9c+fOlQIDA5X2BQUFSS+99JLisa+vr7R06VLF47t370oDBgyQbG1tJXt7e2nIkCFSXFycUhm//vqr1KJFC8nCwkJycXGRBg0aVGp5hw8flmxtbaXly5eX+x6qu0ePHknPP/+8NHnyZKX93bt3l95++21JkiRJ/v/t3X9M1PUfB/Dn5xAk4Yzkjl9KHCwoAi6uwMRpOlGJppXR5FfMaOGGOgYIulYYa5BJtMXWlhB5UdAwNjVAIAlRy2byQyzAKbAJcyUuoCUkAXfv/mh3Xy/0KwcYcDwf2/3xuY+f1/t9nzHvyZv35/3W6yddf2xsTNy4cUP4+PiIa9euiSNHjojg4GCRkJAgVq5cKSIiIiZVd3h4WKxatUo0NzeLsbExERYWJoKDg4VcLhcrVqwQhYWFk6qbn58v1q5da/KZn3vuOZGfny+KiorEyZMnJ1X3dn/99Zd45ZVXREJCgvG9jo4OERkZKVasWCHy8vKm3AYRzT8M/EREdEfXr18Xy5cvF2fOnBFCCKHT6YQQQsTHx4vY2Ngp1zcE59jYWFFTUyOEEOL48eNCoVAIuVwutFrtpPutVCrFiRMnREpKiggLCxMXL14U1dXVIj09Xbi4uIiysjKz6x48eFB4eXmJ5uZmIYQQWVlZQpIksX79ehEcHCycnJwm3efbbdiwQWzfvl0I8b971N3dLV599VWxevVqUV5ePuU2iGh+4UO7RER0R87OziguLjY+w2HYIG3p0qWQyUy/PgYHB82ub5iSYmVlZZyOeOTIEeh0Ori7u+O7777D+fPnza7r5OSE0NBQlJeXo6OjAykpKVCr1Xj22WeRlJSE9evXo66uDjqdzqyHYDdu3AgXFxds3boVL7/8MjIyMnD06FGcOHEClZWViIqKQlFREfr6+ib1cK1Op8Po6CiWLVuG/v5+44PGer0eDz/8MDIyMjA2NoaSkhKzaxPR/MbAb4FKSkpMllG8/cUlCWm6hIeH3/XnbCJr9NPc4O3tDeCf0GlYYUkIYbKU6v79+1FQUGBc0nWiDKF43bp1WLhwIXbs2IGqqio0NTUhKysLp0+fhlarxfDwsFl1JUnC7t27odVqcfz4cZNlOZctWwZnZ2e0t7dDJpOZNQ/e09MTxcXFyM7Ohr+/PyIiIvDCCy9AkiQ4OTnBzc0NAwMDsLOzM6uu4RcpKysrWFtbY9u2bTh69Cjy8/MhSRJkMhl0Oh28vLywf/9+lJWVoa2tbeI3hIjmPT60a4Gef/55PP3003c89+8lEYkmq7CwELdu3brjuSVLlvzHvaH7zbC8qyHIGkb49+3bh6ysLFy4cAELFpj3lWKo5enpifj4eDg7O6OyshKenp7w9PSEJEl44oknYGtra3Z/g4KCUF1djTVr1qCgoABeXl7GAY/R0VH4+PhgbGzM7P8TDX0rLCxEY2MjRkZGYGNjAwDo7e2FSqUyBviJuHLlCioqKhATEwNXV1cAwJo1a3DgwAGkpKRg0aJFeP31140P2svlcjz66KOws7Mzq99ENL8x8FsguVwOuVw+090gC/fvzdrI8hkC/4IFC+Du7o7c3Fzk5OSgsbHxjrsbT1RISAgKCwsRFBQEtVptbOfFF1+cUn9Xr16NU6dOITo6Gq+99hoCAgIwMjKC8vJyfP/991MaAFm5ciXS0tKQl5cHFxcXtLa2QqvV4syZMxMO452dnQgJCcHAwAD6+vqQmpoKhUIBAEhMTMTQ0BC2b9+O7u5uvPTSS/Dw8EBZWRlGR0cZ+InILJKYzERDIiKat7Kzs5GRkYHFixfj22+/NS6ROhV6vX7ccwHT5fLlyyguLsa5c+fg7e2NHTt2wN/ff8p16+vrkZCQAJlMhqVLlyIvLw9qtXpC1w4NDSEpKQl6vR7BwcHYtWsX0tLSkJ6eDqVSCeCfe1JcXIy9e/fCysoKcrkcf/zxByoqKox7FRARTQQDPxERmaWxsRHLly9Ha2vruB2/ZzPDjs7T+YtFf38/RkdHsXDhQjg4OEz4ulu3bkGr1cLR0RGRkZH46quvEBUVNS70A8DVq1fR09ODP//8EwEBAfzrGhGZjYGfiIjMNjQ0xGklU/Tve3j48GFER0dj9+7d2Lt3LxQKBcbGxvDLL79MerMwIiKAc/iJiGgSGPanznAPdTodZDIZIiMjIYRATEwMJElCcnIycnNz0d3djc8//xyLFi3i7rpENCkc4SciIpph4p+NMCGTyXD48GHExcXBy8sLXV1daGhoQGBg4Ex3kYjmMAZ+IiKiWcDwdSxJEkJDQ9HS0oJTp04hICBghntGRHMdp/QQERHNApIkQafTIT09HfX19WhpaWHYJ6JpwZ12iYiIZhE/Pz80NzdPeIlPIqJ74ZQeIiKiWeT2HY2JiKYDR/iJiIhmEYZ9IppuDPxERERERBaMgZ+IiIiIyIIx8BMRERERWTAGfiIiIiIiC8bAT0RERERkwRj4iYiIiIgsGAM/ERFZjMzMTAQGBs50N4yuXr0KSZLQ0tIy010honmMgZ+IiOYkSZJw7Nixme4GEdGsx8BPRETz2sjIyEx3gYjovmLgJyKiGbN27VokJSVhz549WLJkCVxcXJCZmXnP61QqFQBgy5YtkCTJeGzwxRdfQKVS4cEHH0RUVBRu3rxp0uauXbuQnJwMhUKBsLAwAEBrayvCw8Nhb28PZ2dnxMXF4bfffjNeV1NTg1WrVsHBwQGOjo7YtGkTurq6TNo9f/48NBoNbG1tERQUhAsXLpicHxgYQGxsLJRKJR544AF4e3tDq9WacceIiMzHwE9ERDOqqKgIdnZ2+PHHH5GTk4N33nkHtbW1//eahoYGAIBWq8Wvv/5qPAaArq4uHDt2DJWVlaisrMTp06fx3nvvjWvTxsYGZ8+excGDB/H7779j3bp10Gg0aGxsRE1NDXp7e7F161bjNUNDQ0hNTUVjYyPq6uogk8mwZcsW6PV6AMDg4CA2bdqExx9/HE1NTcjMzERaWppJuxkZGWhvb0d1dTUuXbqEjz/+GAqFYkr3j4joXhbMdAeIiGh+U6vVePvttwEA3t7e+Oijj1BXV4cNGzbc9RqlUgkAcHBwgIuLi8k5vV6Pzz77DHK5HAAQFxeHuro6ZGdnG/+Nt7c3cnJyjMdZWVnQaDR49913je8dOnQI7u7uuHLlCnx8fBAREWHSzqFDh6BUKtHe3g5/f398+eWX0Ov1+PTTT2Fraws/Pz9cu3YNiYmJxmt6enqg0WgQFBQEAOP+MkFEdD9whJ+IiGaUWq02OXZ1dcWNGzcmXU+lUhnD/t3qPfXUUybHFy9eRH19Pezt7Y2vxx57DACM03Y6OjoQHR0NLy8vLF682BjWe3p6AACXLl2CWq2Gra2tsW5ISIhJO4mJiSgtLUVgYCD27NmDH374YdKfk4hoojjCT0REM8ra2trkWJIk4zSZ+1XPzs7O5HhwcBCbN2/GgQMHxtVzdXUFAGzevBkeHh745JNP4ObmBr1eD39/f7Me+g0PD0d3dzeqqqpQW1uL0NBQ7Ny5E7m5uROuQURkLo7wExHRnGRtbQ2dTjcttZ588km0tbVBpVLhkUceMXnZ2dmhr68Ply9fxltvvYXQ0FD4+vpiYGDApIavry9++uknDA8PG987d+7cuLaUSiW2bduG4uJifPjhhygoKJiWz0BEdDcM/ERENCepVCrU1dXh+vXr48K3uXbu3In+/n5ER0ejoaEBXV1d+OabbxAfHw+dToeHHnoIjo6OKCgoQGdnJ06ePInU1FSTGjExMZAkCQkJCWhvb0dVVdW4kft9+/bh66+/RmdnJ9ra2lBZWQlfX98p9Z2I6F4Y+ImIaE764IMPUFtbC3d3d2g0minVcnNzw9mzZ6HT6bBx40YEBAQgOTkZDg4OkMlkkMlkKC0tRVNTE/z9/ZGSkoL333/fpIa9vT0qKirw888/Q6PR4M033xw3RcjGxgZvvPEG1Go1nnnmGVhZWaG0tHRKfSciuhdJCCFmuhNERERERHR/cISfiIiIiMiCMfATEdGsU1JSYrJE5u0vPz+/me4eEdGcwik9REQ069y8eRO9vb13PGdtbQ0PD4//uEdERHMXAz8RERERkQXjlB4iIiIiIgvGwE9EREREZMEY+ImIiIiILBgDPxERERGRBWPgJyIiIiKyYAz8REREREQWjIGfiIiIiMiCMfATEREREVmwvwGf3Ai0X4dDOAAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T14:53:41.602937Z",
     "start_time": "2024-08-11T14:53:41.579636Z"
    }
   },
   "source": [
    "res"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          fun: 0.1650871311624845\n",
       "            x: [False, True, 1, 98]\n",
       "    func_vals: [ 1.881e-01  1.917e-01 ...  1.687e-01  1.675e-01]\n",
       "      x_iters: [[False, False, 11, 26], [True, True, 9, 43], [True, False, 2, 49], [True, True, 5, 4], [False, False, 5, 0], [False, True, 4, 55], [True, False, 5, 4], [True, False, 12, 92], [False, False, 3, 89], [True, True, 11, 37], [True, False, 10, 79], [False, True, 8, 55], [False, True, 12, 53], [True, False, 11, 98], [False, True, 1, 2], [True, True, 12, 60], [False, True, 12, 67], [False, False, 4, 18], [True, True, 1, 72], [False, True, 12, 76], [True, True, 12, 27], [True, True, 12, 22], [False, True, 1, 98], [False, True, 1, 93], [True, True, 12, 96], [True, True, 11, 100], [False, True, 2, 30], [True, True, 1, 87], [False, False, 12, 64], [True, True, 2, 67], [False, True, 12, 87], [True, True, 1, 31], [False, True, 12, 96], [True, True, 1, 97], [True, True, 12, 76], [False, False, 12, 39], [False, True, 1, 26], [False, True, 1, 100], [False, True, 1, 95], [True, True, 12, 98], [False, True, 1, 15], [True, False, 2, 35], [True, True, 12, 25], [False, True, 12, 72], [False, False, 12, 41], [True, True, 2, 98], [False, True, 1, 91], [True, True, 7, 93], [True, True, 1, 74], [True, True, 12, 28], [False, False, 12, 56], [False, False, 12, 10], [False, True, 11, 98], [True, False, 3, 71], [False, False, 12, 74], [True, False, 1, 84], [False, True, 1, 63], [True, False, 2, 65], [True, True, 1, 50], [False, True, 11, 36], [False, False, 12, 45], [True, True, 2, 100], [False, True, 6, 100], [True, False, 6, 99], [True, True, 12, 100], [False, False, 1, 2], [False, False, 6, 100], [False, True, 6, 0], [False, False, 1, 100], [True, True, 1, 92], [False, False, 7, 1], [True, False, 8, 100], [False, False, 1, 1], [True, False, 1, 100], [False, True, 8, 0], [False, True, 4, 0], [True, False, 4, 100], [False, False, 1, 0], [True, True, 4, 100], [False, False, 8, 0], [False, False, 4, 0], [True, False, 1, 99], [True, True, 8, 99], [False, False, 1, 99], [False, True, 2, 0], [False, False, 2, 0], [False, True, 1, 99], [True, False, 1, 98], [False, True, 4, 100], [True, False, 9, 0], [True, True, 5, 100], [False, True, 9, 0], [True, False, 11, 0], [False, True, 2, 100], [False, True, 1, 97], [False, True, 1, 85], [True, False, 10, 0], [True, False, 12, 0], [False, False, 11, 57], [False, True, 1, 92]]\n",
       "       models: [GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=711886581)]\n",
       "        space: Space([Categorical(categories=(True, False), prior=None),\n",
       "                      Categorical(categories=(True, False), prior=None),\n",
       "                      Integer(low=1, high=12, prior='uniform', transform='normalize'),\n",
       "                      Integer(low=0, high=100, prior='uniform', transform='normalize')])\n",
       " random_state: RandomState(MT19937)\n",
       "        specs:     args:                    func: <function objective at 0x7a6a63582a20>\n",
       "                                      dimensions: Space([Categorical(categories=(True, False), prior=None),\n",
       "                                                         Categorical(categories=(True, False), prior=None),\n",
       "                                                         Integer(low=1, high=12, prior='uniform', transform='normalize'),\n",
       "                                                         Integer(low=0, high=100, prior='uniform', transform='normalize')])\n",
       "                                  base_estimator: GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1], nu=2.5),\n",
       "                                                                           n_restarts_optimizer=2, noise='gaussian',\n",
       "                                                                           normalize_y=True, random_state=711886581)\n",
       "                                         n_calls: 100\n",
       "                                 n_random_starts: None\n",
       "                                n_initial_points: 10\n",
       "                         initial_point_generator: random\n",
       "                                        acq_func: gp_hedge\n",
       "                                   acq_optimizer: auto\n",
       "                                              x0: None\n",
       "                                              y0: None\n",
       "                                    random_state: RandomState(MT19937)\n",
       "                                         verbose: False\n",
       "                                        callback: None\n",
       "                                        n_points: 10000\n",
       "                            n_restarts_optimizer: 5\n",
       "                                              xi: 0.01\n",
       "                                           kappa: 1.96\n",
       "                                          n_jobs: 1\n",
       "                                model_queue_size: None\n",
       "                                space_constraint: None\n",
       "               function: base_minimize"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T14:53:41.605154Z",
     "start_time": "2024-08-11T14:53:41.603665Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
